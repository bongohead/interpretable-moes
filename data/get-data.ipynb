{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673ba5acf5b94db5bc570a62295fb1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/25.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993de81170a34ca5b3eca396070caf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a06bb8436494c08a80920786cc625f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 validation samples to ./val_shard.json\n",
      "Wrote shard ./train_shard_0.json with 50000 samples (total so far: 50000).\n",
      "Wrote shard ./train_shard_1.json with 50000 samples (total so far: 100000).\n",
      "Wrote shard ./train_shard_2.json with 50000 samples (total so far: 150000).\n",
      "Wrote shard ./train_shard_3.json with 50000 samples (total so far: 200000).\n",
      "Wrote shard ./train_shard_4.json with 50000 samples (total so far: 250000).\n",
      "Wrote shard ./train_shard_5.json with 50000 samples (total so far: 300000).\n",
      "Wrote shard ./train_shard_6.json with 50000 samples (total so far: 350000).\n",
      "Wrote shard ./train_shard_7.json with 50000 samples (total so far: 400000).\n",
      "Wrote shard ./train_shard_8.json with 50000 samples (total so far: 450000).\n",
      "Wrote shard ./train_shard_9.json with 50000 samples (total so far: 500000).\n",
      "Wrote shard ./train_shard_10.json with 50000 samples (total so far: 550000).\n",
      "Wrote shard ./train_shard_11.json with 50000 samples (total so far: 600000).\n",
      "Wrote shard ./train_shard_12.json with 50000 samples (total so far: 650000).\n",
      "Wrote shard ./train_shard_13.json with 50000 samples (total so far: 700000).\n",
      "Wrote shard ./train_shard_14.json with 50000 samples (total so far: 750000).\n",
      "Wrote shard ./train_shard_15.json with 50000 samples (total so far: 800000).\n",
      "Wrote shard ./train_shard_16.json with 50000 samples (total so far: 850000).\n",
      "Wrote shard ./train_shard_17.json with 50000 samples (total so far: 900000).\n",
      "Wrote shard ./train_shard_18.json with 50000 samples (total so far: 950000).\n",
      "Wrote shard ./train_shard_19.json with 50000 samples (total so far: 1000000).\n",
      "Wrote shard ./train_shard_20.json with 50000 samples (total so far: 1050000).\n",
      "Wrote shard ./train_shard_21.json with 50000 samples (total so far: 1100000).\n",
      "Wrote shard ./train_shard_22.json with 50000 samples (total so far: 1150000).\n",
      "Wrote shard ./train_shard_23.json with 50000 samples (total so far: 1200000).\n",
      "Wrote shard ./train_shard_24.json with 50000 samples (total so far: 1250000).\n",
      "Wrote shard ./train_shard_25.json with 50000 samples (total so far: 1300000).\n",
      "Wrote shard ./train_shard_26.json with 50000 samples (total so far: 1350000).\n",
      "Wrote shard ./train_shard_27.json with 50000 samples (total so far: 1400000).\n",
      "Wrote shard ./train_shard_28.json with 50000 samples (total so far: 1450000).\n",
      "Wrote shard ./train_shard_29.json with 50000 samples (total so far: 1500000).\n",
      "Wrote shard ./train_shard_30.json with 50000 samples (total so far: 1550000).\n",
      "Wrote shard ./train_shard_31.json with 50000 samples (total so far: 1600000).\n",
      "Wrote shard ./train_shard_32.json with 50000 samples (total so far: 1650000).\n",
      "Wrote shard ./train_shard_33.json with 50000 samples (total so far: 1700000).\n",
      "Wrote shard ./train_shard_34.json with 50000 samples (total so far: 1750000).\n",
      "Wrote shard ./train_shard_35.json with 50000 samples (total so far: 1800000).\n",
      "Wrote shard ./train_shard_36.json with 50000 samples (total so far: 1850000).\n",
      "Wrote shard ./train_shard_37.json with 50000 samples (total so far: 1900000).\n",
      "Wrote shard ./train_shard_38.json with 50000 samples (total so far: 1950000).\n",
      "Wrote shard ./train_shard_39.json with 50000 samples (total so far: 2000000).\n",
      "Wrote shard ./train_shard_40.json with 50000 samples (total so far: 2050000).\n",
      "Wrote shard ./train_shard_41.json with 50000 samples (total so far: 2100000).\n",
      "Wrote shard ./train_shard_42.json with 50000 samples (total so far: 2150000).\n",
      "Wrote shard ./train_shard_43.json with 50000 samples (total so far: 2200000).\n",
      "Wrote shard ./train_shard_44.json with 50000 samples (total so far: 2250000).\n",
      "Wrote shard ./train_shard_45.json with 50000 samples (total so far: 2300000).\n",
      "Wrote shard ./train_shard_46.json with 50000 samples (total so far: 2350000).\n",
      "Wrote shard ./train_shard_47.json with 50000 samples (total so far: 2400000).\n",
      "Wrote shard ./train_shard_48.json with 50000 samples (total so far: 2450000).\n",
      "Wrote shard ./train_shard_49.json with 50000 samples (total so far: 2500000).\n",
      "Wrote shard ./train_shard_50.json with 50000 samples (total so far: 2550000).\n",
      "Wrote shard ./train_shard_51.json with 50000 samples (total so far: 2600000).\n",
      "Wrote shard ./train_shard_52.json with 50000 samples (total so far: 2650000).\n",
      "Wrote shard ./train_shard_53.json with 50000 samples (total so far: 2700000).\n",
      "Wrote shard ./train_shard_54.json with 50000 samples (total so far: 2750000).\n",
      "Wrote shard ./train_shard_55.json with 50000 samples (total so far: 2800000).\n",
      "Wrote shard ./train_shard_56.json with 50000 samples (total so far: 2850000).\n",
      "Wrote shard ./train_shard_57.json with 50000 samples (total so far: 2900000).\n",
      "Wrote shard ./train_shard_58.json with 50000 samples (total so far: 2950000).\n",
      "Wrote shard ./train_shard_59.json with 50000 samples (total so far: 3000000).\n",
      "Wrote shard ./train_shard_60.json with 50000 samples (total so far: 3050000).\n",
      "Wrote shard ./train_shard_61.json with 50000 samples (total so far: 3100000).\n",
      "Wrote shard ./train_shard_62.json with 50000 samples (total so far: 3150000).\n",
      "Wrote shard ./train_shard_63.json with 50000 samples (total so far: 3200000).\n",
      "Wrote shard ./train_shard_64.json with 50000 samples (total so far: 3250000).\n",
      "Wrote shard ./train_shard_65.json with 50000 samples (total so far: 3300000).\n",
      "Wrote shard ./train_shard_66.json with 50000 samples (total so far: 3350000).\n",
      "Wrote shard ./train_shard_67.json with 50000 samples (total so far: 3400000).\n",
      "Wrote shard ./train_shard_68.json with 50000 samples (total so far: 3450000).\n",
      "Wrote shard ./train_shard_69.json with 50000 samples (total so far: 3500000).\n",
      "Wrote shard ./train_shard_70.json with 50000 samples (total so far: 3550000).\n",
      "Wrote shard ./train_shard_71.json with 50000 samples (total so far: 3600000).\n",
      "Wrote shard ./train_shard_72.json with 50000 samples (total so far: 3650000).\n",
      "Wrote shard ./train_shard_73.json with 50000 samples (total so far: 3700000).\n",
      "Wrote shard ./train_shard_74.json with 50000 samples (total so far: 3750000).\n",
      "Wrote shard ./train_shard_75.json with 50000 samples (total so far: 3800000).\n",
      "Wrote shard ./train_shard_76.json with 50000 samples (total so far: 3850000).\n",
      "Wrote shard ./train_shard_77.json with 50000 samples (total so far: 3900000).\n",
      "Wrote shard ./train_shard_78.json with 50000 samples (total so far: 3950000).\n",
      "Wrote shard ./train_shard_79.json with 50000 samples (total so far: 4000000).\n",
      "Wrote shard ./train_shard_80.json with 50000 samples (total so far: 4050000).\n",
      "Wrote shard ./train_shard_81.json with 50000 samples (total so far: 4100000).\n",
      "Wrote shard ./train_shard_82.json with 50000 samples (total so far: 4150000).\n",
      "Wrote shard ./train_shard_83.json with 50000 samples (total so far: 4200000).\n",
      "Wrote shard ./train_shard_84.json with 50000 samples (total so far: 4250000).\n",
      "Wrote shard ./train_shard_85.json with 50000 samples (total so far: 4300000).\n",
      "Wrote shard ./train_shard_86.json with 50000 samples (total so far: 4350000).\n",
      "Wrote shard ./train_shard_87.json with 50000 samples (total so far: 4400000).\n",
      "Wrote shard ./train_shard_88.json with 50000 samples (total so far: 4450000).\n",
      "Wrote shard ./train_shard_89.json with 50000 samples (total so far: 4500000).\n",
      "Wrote shard ./train_shard_90.json with 50000 samples (total so far: 4550000).\n",
      "Wrote shard ./train_shard_91.json with 50000 samples (total so far: 4600000).\n",
      "Wrote shard ./train_shard_92.json with 50000 samples (total so far: 4650000).\n",
      "Wrote shard ./train_shard_93.json with 50000 samples (total so far: 4700000).\n",
      "Wrote shard ./train_shard_94.json with 50000 samples (total so far: 4750000).\n",
      "Wrote shard ./train_shard_95.json with 50000 samples (total so far: 4800000).\n",
      "Wrote shard ./train_shard_96.json with 50000 samples (total so far: 4850000).\n",
      "Wrote shard ./train_shard_97.json with 50000 samples (total so far: 4900000).\n",
      "Wrote shard ./train_shard_98.json with 50000 samples (total so far: 4950000).\n",
      "Wrote shard ./train_shard_99.json with 50000 samples (total so far: 5000000).\n",
      "Wrote shard ./train_shard_100.json with 50000 samples (total so far: 5050000).\n",
      "Wrote shard ./train_shard_101.json with 50000 samples (total so far: 5100000).\n",
      "Wrote shard ./train_shard_102.json with 50000 samples (total so far: 5150000).\n",
      "Wrote shard ./train_shard_103.json with 50000 samples (total so far: 5200000).\n",
      "Wrote shard ./train_shard_104.json with 50000 samples (total so far: 5250000).\n",
      "Wrote shard ./train_shard_105.json with 50000 samples (total so far: 5300000).\n",
      "Wrote shard ./train_shard_106.json with 50000 samples (total so far: 5350000).\n",
      "Wrote shard ./train_shard_107.json with 50000 samples (total so far: 5400000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1cdddff9-3ba3-4d6d-aa58-5cc19481dde3)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-20/train-00006-of-00014.parquet\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote shard ./train_shard_108.json with 50000 samples (total so far: 5450000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 62774ec6-d85c-41db-8789-f7b59f14f203)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-20/train-00006-of-00014.parquet\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote shard ./train_shard_109.json with 50000 samples (total so far: 5500000).\n",
      "Wrote shard ./train_shard_110.json with 50000 samples (total so far: 5550000).\n",
      "Wrote shard ./train_shard_111.json with 50000 samples (total so far: 5600000).\n",
      "Wrote shard ./train_shard_112.json with 50000 samples (total so far: 5650000).\n",
      "Wrote shard ./train_shard_113.json with 50000 samples (total so far: 5700000).\n",
      "Wrote shard ./train_shard_114.json with 50000 samples (total so far: 5750000).\n",
      "Wrote shard ./train_shard_115.json with 50000 samples (total so far: 5800000).\n",
      "Wrote shard ./train_shard_116.json with 50000 samples (total so far: 5850000).\n",
      "Wrote shard ./train_shard_117.json with 50000 samples (total so far: 5900000).\n",
      "Wrote shard ./train_shard_118.json with 50000 samples (total so far: 5950000).\n",
      "Wrote shard ./train_shard_119.json with 50000 samples (total so far: 6000000).\n",
      "Wrote shard ./train_shard_120.json with 50000 samples (total so far: 6050000).\n",
      "Wrote shard ./train_shard_121.json with 50000 samples (total so far: 6100000).\n",
      "Wrote shard ./train_shard_122.json with 50000 samples (total so far: 6150000).\n",
      "Wrote shard ./train_shard_123.json with 50000 samples (total so far: 6200000).\n",
      "Wrote shard ./train_shard_124.json with 50000 samples (total so far: 6250000).\n",
      "Wrote shard ./train_shard_125.json with 50000 samples (total so far: 6300000).\n",
      "Wrote shard ./train_shard_126.json with 50000 samples (total so far: 6350000).\n",
      "Wrote shard ./train_shard_127.json with 50000 samples (total so far: 6400000).\n",
      "Wrote shard ./train_shard_128.json with 50000 samples (total so far: 6450000).\n",
      "Wrote shard ./train_shard_129.json with 50000 samples (total so far: 6500000).\n",
      "Wrote shard ./train_shard_130.json with 50000 samples (total so far: 6550000).\n",
      "Wrote shard ./train_shard_131.json with 50000 samples (total so far: 6600000).\n",
      "Wrote shard ./train_shard_132.json with 50000 samples (total so far: 6650000).\n",
      "Wrote shard ./train_shard_133.json with 50000 samples (total so far: 6700000).\n",
      "Wrote shard ./train_shard_134.json with 50000 samples (total so far: 6750000).\n",
      "Wrote shard ./train_shard_135.json with 50000 samples (total so far: 6800000).\n",
      "Wrote shard ./train_shard_136.json with 50000 samples (total so far: 6850000).\n",
      "Wrote shard ./train_shard_137.json with 50000 samples (total so far: 6900000).\n",
      "Wrote shard ./train_shard_138.json with 50000 samples (total so far: 6950000).\n",
      "Wrote shard ./train_shard_139.json with 50000 samples (total so far: 7000000).\n",
      "Wrote shard ./train_shard_140.json with 50000 samples (total so far: 7050000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 97224f4d-a96e-40bc-b210-41d5c05675bd)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-20/train-00009-of-00014.parquet\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote shard ./train_shard_141.json with 50000 samples (total so far: 7100000).\n",
      "Wrote shard ./train_shard_142.json with 50000 samples (total so far: 7150000).\n",
      "Wrote shard ./train_shard_143.json with 50000 samples (total so far: 7200000).\n",
      "Wrote shard ./train_shard_144.json with 50000 samples (total so far: 7250000).\n",
      "Wrote shard ./train_shard_145.json with 50000 samples (total so far: 7300000).\n",
      "Wrote shard ./train_shard_146.json with 50000 samples (total so far: 7350000).\n",
      "Wrote shard ./train_shard_147.json with 50000 samples (total so far: 7400000).\n",
      "Wrote shard ./train_shard_148.json with 50000 samples (total so far: 7450000).\n",
      "Wrote shard ./train_shard_149.json with 50000 samples (total so far: 7500000).\n",
      "Wrote shard ./train_shard_150.json with 50000 samples (total so far: 7550000).\n",
      "Wrote shard ./train_shard_151.json with 50000 samples (total so far: 7600000).\n",
      "Wrote shard ./train_shard_152.json with 50000 samples (total so far: 7650000).\n",
      "Wrote shard ./train_shard_153.json with 50000 samples (total so far: 7700000).\n",
      "Wrote shard ./train_shard_154.json with 50000 samples (total so far: 7750000).\n",
      "Wrote shard ./train_shard_155.json with 50000 samples (total so far: 7800000).\n",
      "Wrote shard ./train_shard_156.json with 50000 samples (total so far: 7850000).\n",
      "Wrote shard ./train_shard_157.json with 50000 samples (total so far: 7900000).\n",
      "Wrote shard ./train_shard_158.json with 50000 samples (total so far: 7950000).\n",
      "Wrote shard ./train_shard_159.json with 50000 samples (total so far: 8000000).\n",
      "Wrote shard ./train_shard_160.json with 50000 samples (total so far: 8050000).\n",
      "Wrote shard ./train_shard_161.json with 50000 samples (total so far: 8100000).\n",
      "Wrote shard ./train_shard_162.json with 50000 samples (total so far: 8150000).\n",
      "Wrote shard ./train_shard_163.json with 50000 samples (total so far: 8200000).\n",
      "Wrote shard ./train_shard_164.json with 50000 samples (total so far: 8250000).\n",
      "Wrote shard ./train_shard_165.json with 50000 samples (total so far: 8300000).\n",
      "Wrote shard ./train_shard_166.json with 50000 samples (total so far: 8350000).\n",
      "Wrote shard ./train_shard_167.json with 50000 samples (total so far: 8400000).\n",
      "Wrote shard ./train_shard_168.json with 50000 samples (total so far: 8450000).\n",
      "Wrote shard ./train_shard_169.json with 50000 samples (total so far: 8500000).\n",
      "Wrote shard ./train_shard_170.json with 50000 samples (total so far: 8550000).\n",
      "Wrote shard ./train_shard_171.json with 50000 samples (total so far: 8600000).\n",
      "Wrote shard ./train_shard_172.json with 50000 samples (total so far: 8650000).\n",
      "Wrote shard ./train_shard_173.json with 50000 samples (total so far: 8700000).\n",
      "Wrote shard ./train_shard_174.json with 50000 samples (total so far: 8750000).\n",
      "Wrote shard ./train_shard_175.json with 50000 samples (total so far: 8800000).\n",
      "Wrote shard ./train_shard_176.json with 50000 samples (total so far: 8850000).\n",
      "Wrote shard ./train_shard_177.json with 50000 samples (total so far: 8900000).\n",
      "Wrote shard ./train_shard_178.json with 50000 samples (total so far: 8950000).\n",
      "Wrote shard ./train_shard_179.json with 50000 samples (total so far: 9000000).\n",
      "Wrote shard ./train_shard_180.json with 50000 samples (total so far: 9050000).\n",
      "Wrote shard ./train_shard_181.json with 50000 samples (total so far: 9100000).\n",
      "Wrote shard ./train_shard_182.json with 50000 samples (total so far: 9150000).\n",
      "Wrote shard ./train_shard_183.json with 50000 samples (total so far: 9200000).\n",
      "Wrote shard ./train_shard_184.json with 50000 samples (total so far: 9250000).\n",
      "Wrote shard ./train_shard_185.json with 50000 samples (total so far: 9300000).\n",
      "Wrote shard ./train_shard_186.json with 50000 samples (total so far: 9350000).\n",
      "Wrote shard ./train_shard_187.json with 50000 samples (total so far: 9400000).\n",
      "Wrote shard ./train_shard_188.json with 50000 samples (total so far: 9450000).\n",
      "Wrote shard ./train_shard_189.json with 50000 samples (total so far: 9500000).\n",
      "Wrote shard ./train_shard_190.json with 50000 samples (total so far: 9550000).\n",
      "Wrote shard ./train_shard_191.json with 50000 samples (total so far: 9600000).\n",
      "Wrote shard ./train_shard_192.json with 50000 samples (total so far: 9650000).\n",
      "Wrote shard ./train_shard_193.json with 50000 samples (total so far: 9700000).\n",
      "Wrote shard ./train_shard_194.json with 50000 samples (total so far: 9750000).\n",
      "Wrote shard ./train_shard_195.json with 50000 samples (total so far: 9800000).\n",
      "Wrote shard ./train_shard_196.json with 50000 samples (total so far: 9850000).\n",
      "Wrote shard ./train_shard_197.json with 50000 samples (total so far: 9900000).\n",
      "Wrote shard ./train_shard_198.json with 50000 samples (total so far: 9950000).\n",
      "Wrote shard ./train_shard_199.json with 50000 samples (total so far: 10000000).\n",
      "Wrote shard ./train_shard_200.json with 50000 samples (total so far: 10050000).\n",
      "Wrote shard ./train_shard_201.json with 50000 samples (total so far: 10100000).\n",
      "Wrote shard ./train_shard_202.json with 50000 samples (total so far: 10150000).\n",
      "Wrote shard ./train_shard_203.json with 50000 samples (total so far: 10200000).\n",
      "Wrote shard ./train_shard_204.json with 50000 samples (total so far: 10250000).\n",
      "Wrote shard ./train_shard_205.json with 50000 samples (total so far: 10300000).\n",
      "Wrote shard ./train_shard_206.json with 50000 samples (total so far: 10350000).\n",
      "Wrote shard ./train_shard_207.json with 50000 samples (total so far: 10400000).\n",
      "Wrote shard ./train_shard_208.json with 50000 samples (total so far: 10450000).\n",
      "Wrote shard ./train_shard_209.json with 50000 samples (total so far: 10500000).\n",
      "Wrote shard ./train_shard_210.json with 50000 samples (total so far: 10550000).\n",
      "Wrote shard ./train_shard_211.json with 50000 samples (total so far: 10600000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a43a0749-66a2-4186-a2a5-3611dedfac8a)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-20/train-00013-of-00014.parquet\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote shard ./train_shard_212.json with 50000 samples (total so far: 10650000).\n",
      "Wrote shard ./train_shard_213.json with 50000 samples (total so far: 10700000).\n",
      "Wrote shard ./train_shard_214.json with 50000 samples (total so far: 10750000).\n",
      "Wrote shard ./train_shard_215.json with 50000 samples (total so far: 10800000).\n",
      "Wrote shard ./train_shard_216.json with 50000 samples (total so far: 10850000).\n",
      "Wrote shard ./train_shard_217.json with 50000 samples (total so far: 10900000).\n",
      "Wrote shard ./train_shard_218.json with 50000 samples (total so far: 10950000).\n",
      "Wrote shard ./train_shard_219.json with 50000 samples (total so far: 11000000).\n",
      "Wrote shard ./train_shard_220.json with 50000 samples (total so far: 11050000).\n",
      "Wrote shard ./train_shard_221.json with 50000 samples (total so far: 11100000).\n",
      "Wrote shard ./train_shard_222.json with 50000 samples (total so far: 11150000).\n",
      "Wrote shard ./train_shard_223.json with 50000 samples (total so far: 11200000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2c91a873-dc81-4690-b7d7-d183fb17a512)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-48/train-00000-of-00014.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 24790192-bd73-4bde-ae46-14d60c2ce859)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-48/train-00000-of-00014.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 27929727-e6db-4213-972e-87c4a9fd8c2c)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-48/train-00000-of-00014.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5e8d7945-fd17-4078-b72c-550189997e76)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-48/train-00000-of-00014.parquet\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote shard ./train_shard_224.json with 50000 samples (total so far: 11250000).\n",
      "Wrote shard ./train_shard_225.json with 50000 samples (total so far: 11300000).\n",
      "Wrote shard ./train_shard_226.json with 50000 samples (total so far: 11350000).\n",
      "Wrote shard ./train_shard_227.json with 50000 samples (total so far: 11400000).\n",
      "Wrote shard ./train_shard_228.json with 50000 samples (total so far: 11450000).\n",
      "Wrote shard ./train_shard_229.json with 50000 samples (total so far: 11500000).\n",
      "Wrote shard ./train_shard_230.json with 50000 samples (total so far: 11550000).\n",
      "Wrote shard ./train_shard_231.json with 50000 samples (total so far: 11600000).\n",
      "Wrote shard ./train_shard_232.json with 50000 samples (total so far: 11650000).\n",
      "Wrote shard ./train_shard_233.json with 50000 samples (total so far: 11700000).\n",
      "Wrote shard ./train_shard_234.json with 50000 samples (total so far: 11750000).\n",
      "Wrote shard ./train_shard_235.json with 50000 samples (total so far: 11800000).\n",
      "Wrote shard ./train_shard_236.json with 50000 samples (total so far: 11850000).\n",
      "Wrote shard ./train_shard_237.json with 50000 samples (total so far: 11900000).\n",
      "Wrote shard ./train_shard_238.json with 50000 samples (total so far: 11950000).\n",
      "Wrote shard ./train_shard_239.json with 50000 samples (total so far: 12000000).\n",
      "Wrote shard ./train_shard_240.json with 50000 samples (total so far: 12050000).\n",
      "Wrote shard ./train_shard_241.json with 50000 samples (total so far: 12100000).\n",
      "Wrote shard ./train_shard_242.json with 50000 samples (total so far: 12150000).\n",
      "Wrote shard ./train_shard_243.json with 50000 samples (total so far: 12200000).\n",
      "Wrote shard ./train_shard_244.json with 50000 samples (total so far: 12250000).\n",
      "Wrote shard ./train_shard_245.json with 50000 samples (total so far: 12300000).\n",
      "Wrote shard ./train_shard_246.json with 50000 samples (total so far: 12350000).\n",
      "Wrote shard ./train_shard_247.json with 50000 samples (total so far: 12400000).\n",
      "Wrote shard ./train_shard_248.json with 50000 samples (total so far: 12450000).\n",
      "Wrote shard ./train_shard_249.json with 50000 samples (total so far: 12500000).\n",
      "Wrote shard ./train_shard_250.json with 50000 samples (total so far: 12550000).\n",
      "Wrote shard ./train_shard_251.json with 50000 samples (total so far: 12600000).\n",
      "Wrote shard ./train_shard_252.json with 50000 samples (total so far: 12650000).\n",
      "Wrote shard ./train_shard_253.json with 50000 samples (total so far: 12700000).\n",
      "Wrote shard ./train_shard_254.json with 50000 samples (total so far: 12750000).\n",
      "Wrote shard ./train_shard_255.json with 50000 samples (total so far: 12800000).\n",
      "Wrote shard ./train_shard_256.json with 50000 samples (total so far: 12850000).\n",
      "Wrote shard ./train_shard_257.json with 50000 samples (total so far: 12900000).\n",
      "Wrote shard ./train_shard_258.json with 50000 samples (total so far: 12950000).\n",
      "Wrote shard ./train_shard_259.json with 50000 samples (total so far: 13000000).\n",
      "Wrote shard ./train_shard_260.json with 50000 samples (total so far: 13050000).\n",
      "Wrote shard ./train_shard_261.json with 50000 samples (total so far: 13100000).\n",
      "Wrote shard ./train_shard_262.json with 50000 samples (total so far: 13150000).\n",
      "Wrote shard ./train_shard_263.json with 50000 samples (total so far: 13200000).\n",
      "Wrote shard ./train_shard_264.json with 50000 samples (total so far: 13250000).\n",
      "Wrote shard ./train_shard_265.json with 50000 samples (total so far: 13300000).\n",
      "Wrote shard ./train_shard_266.json with 50000 samples (total so far: 13350000).\n",
      "Wrote shard ./train_shard_267.json with 50000 samples (total so far: 13400000).\n",
      "Wrote shard ./train_shard_268.json with 50000 samples (total so far: 13450000).\n",
      "Wrote shard ./train_shard_269.json with 50000 samples (total so far: 13500000).\n",
      "Wrote shard ./train_shard_270.json with 50000 samples (total so far: 13550000).\n",
      "Wrote shard ./train_shard_271.json with 50000 samples (total so far: 13600000).\n",
      "Wrote shard ./train_shard_272.json with 50000 samples (total so far: 13650000).\n",
      "Wrote shard ./train_shard_273.json with 50000 samples (total so far: 13700000).\n",
      "Wrote shard ./train_shard_274.json with 50000 samples (total so far: 13750000).\n",
      "Wrote shard ./train_shard_275.json with 50000 samples (total so far: 13800000).\n",
      "Wrote shard ./train_shard_276.json with 50000 samples (total so far: 13850000).\n",
      "Wrote shard ./train_shard_277.json with 50000 samples (total so far: 13900000).\n",
      "Wrote shard ./train_shard_278.json with 50000 samples (total so far: 13950000).\n",
      "Wrote shard ./train_shard_279.json with 50000 samples (total so far: 14000000).\n",
      "Wrote shard ./train_shard_280.json with 50000 samples (total so far: 14050000).\n",
      "Wrote shard ./train_shard_281.json with 50000 samples (total so far: 14100000).\n",
      "Wrote shard ./train_shard_282.json with 50000 samples (total so far: 14150000).\n",
      "Wrote shard ./train_shard_283.json with 50000 samples (total so far: 14200000).\n",
      "Wrote shard ./train_shard_284.json with 50000 samples (total so far: 14250000).\n",
      "Wrote shard ./train_shard_285.json with 50000 samples (total so far: 14300000).\n",
      "Wrote shard ./train_shard_286.json with 50000 samples (total so far: 14350000).\n",
      "Wrote shard ./train_shard_287.json with 50000 samples (total so far: 14400000).\n",
      "Wrote shard ./train_shard_288.json with 50000 samples (total so far: 14450000).\n",
      "Wrote shard ./train_shard_289.json with 50000 samples (total so far: 14500000).\n",
      "Wrote shard ./train_shard_290.json with 50000 samples (total so far: 14550000).\n",
      "Wrote shard ./train_shard_291.json with 50000 samples (total so far: 14600000).\n",
      "Wrote shard ./train_shard_292.json with 50000 samples (total so far: 14650000).\n",
      "Wrote shard ./train_shard_293.json with 50000 samples (total so far: 14700000).\n",
      "Wrote shard ./train_shard_294.json with 50000 samples (total so far: 14750000).\n",
      "Wrote shard ./train_shard_295.json with 50000 samples (total so far: 14800000).\n",
      "Wrote shard ./train_shard_296.json with 50000 samples (total so far: 14850000).\n",
      "Wrote shard ./train_shard_297.json with 50000 samples (total so far: 14900000).\n",
      "Wrote shard ./train_shard_298.json with 50000 samples (total so far: 14950000).\n",
      "Wrote shard ./train_shard_299.json with 50000 samples (total so far: 15000000).\n",
      "Wrote shard ./train_shard_300.json with 50000 samples (total so far: 15050000).\n",
      "Wrote shard ./train_shard_301.json with 50000 samples (total so far: 15100000).\n",
      "Wrote shard ./train_shard_302.json with 50000 samples (total so far: 15150000).\n",
      "Wrote shard ./train_shard_303.json with 50000 samples (total so far: 15200000).\n",
      "Wrote shard ./train_shard_304.json with 50000 samples (total so far: 15250000).\n",
      "Wrote shard ./train_shard_305.json with 50000 samples (total so far: 15300000).\n",
      "Wrote shard ./train_shard_306.json with 50000 samples (total so far: 15350000).\n",
      "Wrote shard ./train_shard_307.json with 50000 samples (total so far: 15400000).\n",
      "Wrote shard ./train_shard_308.json with 50000 samples (total so far: 15450000).\n",
      "Wrote shard ./train_shard_309.json with 50000 samples (total so far: 15500000).\n",
      "Wrote shard ./train_shard_310.json with 50000 samples (total so far: 15550000).\n",
      "Wrote shard ./train_shard_311.json with 50000 samples (total so far: 15600000).\n",
      "Wrote shard ./train_shard_312.json with 50000 samples (total so far: 15650000).\n",
      "Wrote shard ./train_shard_313.json with 50000 samples (total so far: 15700000).\n",
      "Wrote shard ./train_shard_314.json with 50000 samples (total so far: 15750000).\n",
      "Wrote shard ./train_shard_315.json with 50000 samples (total so far: 15800000).\n",
      "Wrote shard ./train_shard_316.json with 50000 samples (total so far: 15850000).\n",
      "Wrote shard ./train_shard_317.json with 50000 samples (total so far: 15900000).\n",
      "Wrote shard ./train_shard_318.json with 50000 samples (total so far: 15950000).\n",
      "Wrote shard ./train_shard_319.json with 50000 samples (total so far: 16000000).\n",
      "Wrote shard ./train_shard_320.json with 50000 samples (total so far: 16050000).\n",
      "Wrote shard ./train_shard_321.json with 50000 samples (total so far: 16100000).\n",
      "Wrote shard ./train_shard_322.json with 50000 samples (total so far: 16150000).\n",
      "Wrote shard ./train_shard_323.json with 50000 samples (total so far: 16200000).\n",
      "Wrote shard ./train_shard_324.json with 50000 samples (total so far: 16250000).\n",
      "Wrote shard ./train_shard_325.json with 50000 samples (total so far: 16300000).\n",
      "Wrote shard ./train_shard_326.json with 50000 samples (total so far: 16350000).\n",
      "Wrote shard ./train_shard_327.json with 50000 samples (total so far: 16400000).\n",
      "Wrote shard ./train_shard_328.json with 50000 samples (total so far: 16450000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 64b0746a-185d-4ba9-9bcb-eb78d8b7b2f5)')' thrown while requesting GET https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/4863ab07d7520451e6f73e2912ad8bfee7d97c11/data/CC-MAIN-2013-48/train-00007-of-00014.parquet\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote shard ./train_shard_329.json with 50000 samples (total so far: 16500000).\n",
      "Wrote shard ./train_shard_330.json with 50000 samples (total so far: 16550000).\n",
      "Wrote shard ./train_shard_331.json with 50000 samples (total so far: 16600000).\n",
      "Wrote shard ./train_shard_332.json with 50000 samples (total so far: 16650000).\n",
      "Wrote shard ./train_shard_333.json with 50000 samples (total so far: 16700000).\n",
      "Wrote shard ./train_shard_334.json with 50000 samples (total so far: 16750000).\n",
      "Wrote shard ./train_shard_335.json with 50000 samples (total so far: 16800000).\n",
      "Wrote shard ./train_shard_336.json with 50000 samples (total so far: 16850000).\n",
      "Wrote shard ./train_shard_337.json with 50000 samples (total so far: 16900000).\n",
      "Wrote shard ./train_shard_338.json with 50000 samples (total so far: 16950000).\n",
      "Wrote shard ./train_shard_339.json with 50000 samples (total so far: 17000000).\n",
      "Wrote shard ./train_shard_340.json with 50000 samples (total so far: 17050000).\n",
      "Wrote shard ./train_shard_341.json with 50000 samples (total so far: 17100000).\n",
      "Wrote shard ./train_shard_342.json with 50000 samples (total so far: 17150000).\n",
      "Wrote shard ./train_shard_343.json with 50000 samples (total so far: 17200000).\n",
      "Wrote shard ./train_shard_344.json with 50000 samples (total so far: 17250000).\n",
      "Wrote shard ./train_shard_345.json with 50000 samples (total so far: 17300000).\n",
      "Wrote shard ./train_shard_346.json with 50000 samples (total so far: 17350000).\n",
      "Wrote shard ./train_shard_347.json with 50000 samples (total so far: 17400000).\n",
      "Wrote shard ./train_shard_348.json with 50000 samples (total so far: 17450000).\n",
      "Wrote shard ./train_shard_349.json with 50000 samples (total so far: 17500000).\n",
      "Wrote shard ./train_shard_350.json with 50000 samples (total so far: 17550000).\n",
      "Wrote shard ./train_shard_351.json with 50000 samples (total so far: 17600000).\n",
      "Wrote shard ./train_shard_352.json with 50000 samples (total so far: 17650000).\n",
      "Wrote shard ./train_shard_353.json with 50000 samples (total so far: 17700000).\n",
      "Wrote shard ./train_shard_354.json with 50000 samples (total so far: 17750000).\n",
      "Wrote shard ./train_shard_355.json with 50000 samples (total so far: 17800000).\n",
      "Wrote shard ./train_shard_356.json with 50000 samples (total so far: 17850000).\n",
      "Wrote shard ./train_shard_357.json with 50000 samples (total so far: 17900000).\n",
      "Wrote shard ./train_shard_358.json with 50000 samples (total so far: 17950000).\n",
      "Wrote shard ./train_shard_359.json with 50000 samples (total so far: 18000000).\n",
      "Wrote shard ./train_shard_360.json with 50000 samples (total so far: 18050000).\n",
      "Wrote shard ./train_shard_361.json with 50000 samples (total so far: 18100000).\n",
      "Wrote shard ./train_shard_362.json with 50000 samples (total so far: 18150000).\n",
      "Wrote shard ./train_shard_363.json with 50000 samples (total so far: 18200000).\n",
      "Wrote shard ./train_shard_364.json with 50000 samples (total so far: 18250000).\n",
      "Wrote shard ./train_shard_365.json with 50000 samples (total so far: 18300000).\n",
      "Wrote shard ./train_shard_366.json with 50000 samples (total so far: 18350000).\n",
      "Wrote shard ./train_shard_367.json with 50000 samples (total so far: 18400000).\n",
      "Wrote shard ./train_shard_368.json with 50000 samples (total so far: 18450000).\n",
      "Wrote shard ./train_shard_369.json with 50000 samples (total so far: 18500000).\n",
      "Wrote shard ./train_shard_370.json with 50000 samples (total so far: 18550000).\n",
      "Wrote shard ./train_shard_371.json with 50000 samples (total so far: 18600000).\n",
      "Wrote shard ./train_shard_372.json with 50000 samples (total so far: 18650000).\n",
      "Wrote shard ./train_shard_373.json with 50000 samples (total so far: 18700000).\n",
      "Wrote shard ./train_shard_374.json with 50000 samples (total so far: 18750000).\n",
      "Wrote shard ./train_shard_375.json with 50000 samples (total so far: 18800000).\n",
      "Wrote shard ./train_shard_376.json with 50000 samples (total so far: 18850000).\n",
      "Wrote shard ./train_shard_377.json with 50000 samples (total so far: 18900000).\n",
      "Wrote shard ./train_shard_378.json with 50000 samples (total so far: 18950000).\n",
      "Wrote shard ./train_shard_379.json with 50000 samples (total so far: 19000000).\n",
      "Wrote shard ./train_shard_380.json with 50000 samples (total so far: 19050000).\n",
      "Wrote shard ./train_shard_381.json with 50000 samples (total so far: 19100000).\n",
      "Wrote shard ./train_shard_382.json with 50000 samples (total so far: 19150000).\n",
      "Wrote shard ./train_shard_383.json with 50000 samples (total so far: 19200000).\n",
      "Wrote shard ./train_shard_384.json with 50000 samples (total so far: 19250000).\n",
      "Wrote shard ./train_shard_385.json with 50000 samples (total so far: 19300000).\n",
      "Wrote shard ./train_shard_386.json with 50000 samples (total so far: 19350000).\n",
      "Wrote shard ./train_shard_387.json with 50000 samples (total so far: 19400000).\n",
      "Wrote shard ./train_shard_388.json with 50000 samples (total so far: 19450000).\n",
      "Wrote shard ./train_shard_389.json with 50000 samples (total so far: 19500000).\n",
      "Wrote shard ./train_shard_390.json with 50000 samples (total so far: 19550000).\n",
      "Wrote shard ./train_shard_391.json with 50000 samples (total so far: 19600000).\n",
      "Wrote shard ./train_shard_392.json with 50000 samples (total so far: 19650000).\n",
      "Wrote shard ./train_shard_393.json with 50000 samples (total so far: 19700000).\n",
      "Wrote shard ./train_shard_394.json with 50000 samples (total so far: 19750000).\n",
      "Wrote shard ./train_shard_395.json with 50000 samples (total so far: 19800000).\n",
      "Wrote shard ./train_shard_396.json with 50000 samples (total so far: 19850000).\n",
      "Wrote shard ./train_shard_397.json with 50000 samples (total so far: 19900000).\n",
      "Wrote shard ./train_shard_398.json with 50000 samples (total so far: 19950000).\n",
      "Wrote shard ./train_shard_399.json with 50000 samples (total so far: 20000000).\n",
      "Done generating shards.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "def get_fineweb_edu_data_sharded(\n",
    "    shard_size = 50000,\n",
    "    max_samples = 20000000,\n",
    "    out_prefix = \"./train_shard\",\n",
    "    val_filename = \"./val_shard.json\",\n",
    "    val_size = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream the FineWeb-Edu dataset and write out training samples in shards. Also create a validation shard of 'val_size' samples at the beginning.\n",
    "    These are stored as raw samples.\n",
    "\n",
    "    Params:\n",
    "        @shard_size: Number of samples per training shard.\n",
    "        @max_samples: Total samples for training. If `None`, read until dataset ends.\n",
    "        @out_prefix: Filename prefix for train shards.\n",
    "        @val_filename: Filename for the validation shard.\n",
    "        @val_size: Number of samples in the validation set.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"default\", split=\"train\", streaming=True)\n",
    "    ds = ds.filter(lambda x: x.get(\"language\") == \"en\") #and x.get(\"score\") >= 4\n",
    "    ds_iter = iter(ds)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Collect validation samples\n",
    "    # ------------------------------------------------\n",
    "    val_data = []\n",
    "    for _ in range(val_size):\n",
    "        sample = next(ds_iter, None)\n",
    "        if sample is None:\n",
    "            break\n",
    "        val_data.append(sample[\"text\"])\n",
    "\n",
    "    with open(val_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(val_data, f, ensure_ascii=False)\n",
    "    print(f\"Saved {len(val_data)} validation samples to {val_filename}\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Collect training shards in a single pass\n",
    "    # ------------------------------------------------\n",
    "    total_written = 0\n",
    "    shard_idx = 0\n",
    "\n",
    "    while True:\n",
    "        # If we have a max_samples limit and we've reached it, stop\n",
    "        if max_samples is not None and total_written >= max_samples:\n",
    "            break\n",
    "\n",
    "        # Gather up to shard_size items\n",
    "        chunk = []\n",
    "        for _ in range(shard_size):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                # No more data in the stream\n",
    "                break\n",
    "            chunk.append(sample)\n",
    "\n",
    "        if not chunk:\n",
    "            break  # We reached EOF on the stream\n",
    "\n",
    "        # Extract text from each sample\n",
    "        texts = [x[\"text\"] for x in chunk]\n",
    "\n",
    "        # Write shard\n",
    "        shard_path = f\"{out_prefix}_{shard_idx}.json\"\n",
    "        with open(shard_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(texts, f, ensure_ascii=False)\n",
    "\n",
    "        shard_idx += 1\n",
    "        total_written += len(chunk)\n",
    "        print(f\"Wrote shard {shard_path} with {len(chunk)} samples (total so far: {total_written}).\")\n",
    "\n",
    "    print(\"Done generating shards.\")\n",
    "\n",
    "get_fineweb_edu_data_sharded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old, non-sharded\n",
    "# def get_fineweb_edu_data(n_samples: int = 1000):\n",
    "#     dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", name = \"default\", split = 'train', streaming = True)\n",
    "#     dataset = dataset.filter(lambda x: x.get('language') == 'en' and x.get('score') >= 4)\n",
    "#     dataset_pulled = list(islice(dataset, n_samples))  # Convert to a list of the first 1,000 samples\n",
    "#     dataset_pulled = [x['text'] for x in dataset_pulled]    \n",
    "#     return dataset_pulled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
