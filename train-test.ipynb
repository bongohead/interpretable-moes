{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "import math\n",
    "from accelerate import Accelerator\n",
    "from helpers.memory import check_memory\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "main_device = 'cuda:0'\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/OLMoE-1B-7B-0924',\n",
    "    device_map = main_device, \n",
    "    torch_dtype = torch.bfloat16,\n",
    "    trust_remote_code = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MoeConf:\n",
    "    vocab_size: int = 50304 # Base OlMoE: 50304 (vocab size)\n",
    "    D: int = 2048 # Base OlMoE: 2048 (hidden state dimension)\n",
    "    H = 16 # Base OlMoE: 16 (number of attention heads)\n",
    "    router_aux_loss_coef: float = 0.01  # Base OlMoE: 0.01\n",
    "    n_experts: int = 64             # Base OlMoE: 64\n",
    "    top_k: int = 8                  # Base OlMoE: 8 \n",
    "    padding_idx: int = 1            # Base OlMoE: 1 (index where padding gets mapped to)\n",
    "    n_layers: int = 16              # Base OlMoE: 16 (transformer layers)\n",
    "    rms_norm_eps: float = 1e-05     # Base OlMoE: 1e-05\n",
    "    rope_theta = 10000.0 # Base OlMoe: 10000.0 (this is something needed for ROPE)\n",
    "    max_position_embeddings: 4096 # Base OlMoE: 4096 (this is something needed for ROPE)\n",
    "\n",
    "conf = MoeConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from transformers.modeling_flash_attention_utils import _flash_attention_forward # Flash attention forward\n",
    "from transformers.activations import silu\n",
    "\n",
    "class OlmoeRMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    RMS norm, copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L137-L154\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "    \n",
    "\n",
    "class OlmoeAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf, attn_method: str):\n",
    "        super().__init__()\n",
    "        self.attn_method = attn_method\n",
    "        self.D = conf.D # Hidden state dim\n",
    "        self.H = conf.H # Num of attention heads\n",
    "        self.Dh = int(conf.D/conf.H) # Dimensions per head\n",
    "        \n",
    "        # Initialize attention layers - no biases following OlMoE architecture https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L318-L325\n",
    "        self.q_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.k_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.v_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.o_proj = nn.Linear(self.D, self.D, bias = False)\n",
    "        self.q_norm = OlmoeRMSNorm(self.D, eps = conf.rms_norm_eps)\n",
    "        self.k_norm = OlmoeRMSNorm((self.D // self.H) * self.num_key_value_heads, eps = conf.rms_norm_eps)\n",
    "\n",
    "    # See https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L223-L255\n",
    "    def apply_rotary_pos_emb(self, q, k, cos, sin, unsqueeze_dim = 1):\n",
    "        def rotate_half(x):\n",
    "            \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "            x1 = x[..., : x.shape[-1] // 2]\n",
    "            x2 = x[..., x.shape[-1] // 2 :]\n",
    "            return torch.cat((-x2, x1), dim=-1)\n",
    "        \n",
    "        cos = cos.unsqueeze(unsqueeze_dim)\n",
    "        sin = sin.unsqueeze(unsqueeze_dim)\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_embeddings: tuple[torch.Tensor, torch.Tensor]):\n",
    "        \n",
    "        B, N , D = hidden_state.shape\n",
    "\n",
    "        query_state = self.self_attn.q_norm(self.self_attn.q_proj(hidden_state)).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "        key_state = self.self_attn.k_norm(self.self_attn.k_proj(hidden_state)).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "        value_state = self.self_attn.v_proj(hidden_state).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_state, key_state = self.apply_rotary_pos_emb(query_state, key_state, cos, sin)\n",
    "        \n",
    "        if self.attn_method == 'normal':\n",
    "            attn_weights = torch.matmul(query_state, key_state.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "            attn_weights = attn_weights + attention_mask # Attention mask is upper triangular of negative infinity\n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(query_state.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_state) # B x H x N x D/H\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "            attn_output = attn_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "        elif self.attn_method == 'sdpa':\n",
    "            attn_output = torch.nn.functional.scaled_dot_product_attention(query_state, key_state, value_state, dropout_p = 0.0, is_causal = True)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            attn_output = attn_output.view(B, N, D)\n",
    "        elif self.attn_method == 'fa2':\n",
    "            query_state = query_state.transpose(1, 2)\n",
    "            key_state = key_state.transpose(1, 2)\n",
    "            value_state = value_state.transpose(1, 2)\n",
    "            attn_output = _flash_attention_forward(\n",
    "                query_state, key_state, value_state,\n",
    "                attention_mask, N, dropout = 0.0, use_top_left_mask = False, is_causal = True\n",
    "            )\n",
    "            attn_output = attn_output.reshape(B, N, D).contiguous()\n",
    "    \n",
    "        return attn_output\n",
    "    \n",
    "class OlmoeMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "    \n",
    "class OlmoeMoe(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.num_experts\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "        self.norm_topk_prob = config.norm_topk_prob\n",
    "        self.gate = nn.Linear(config.hidden_size, self.num_experts, bias=False)\n",
    "        self.experts = nn.ModuleList([OlmoeMLP(config) for _ in range(self.num_experts)])\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        # router_logits: (batch * sequence_length, n_experts)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        if self.norm_topk_prob:\n",
    "            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        # we cast back to the input dtype\n",
    "        routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask\n",
    "        # this will be used to easily index which expert is going to be selected\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            # Index the correct hidden states and compute the expert hidden state for\n",
    "            # the current expert. We need to make sure to multiply the output hidden\n",
    "            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
    "            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use\n",
    "            # the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        return final_hidden_states, router_logits\n",
    "    \n",
    "class OlmoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer layer\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.D = conf.D\n",
    "        self.self_attn = OlmoeAttention(conf, attn_method = 'fa2')\n",
    "        self.mlp = OlmoeSparseMoeBlock(conf)\n",
    "        self.input_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "        self.post_attention_layernorm = OlmoeRMSNorm(conf.hidden_size, eps = conf.rms_norm_eps)\n",
    "        \n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings: tuple[torch.Tensor, torch.Tensor]):\n",
    "            \n",
    "        ### Pre-SA Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.input_layernorm(hidden_state)\n",
    "        \n",
    "        ### Self-attention ###\n",
    "        H = layer.self_attn.num_heads # Number of attention heads\n",
    "        Dh = int(D/H) # Dimensions per head\n",
    "        \n",
    "        query_state = layer.self_attn.q_norm(layer.self_attn.q_proj(hidden_state)).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "        key_state = layer.self_attn.k_norm(layer.self_attn.k_proj(hidden_state)).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "        value_state = layer.self_attn.v_proj(hidden_state).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_state, key_state = apply_rotary_pos_emb(query_state, key_state, cos, sin)\n",
    "        \n",
    "        if attention_method == 'normal':\n",
    "            attn_weights = torch.matmul(query_state, key_state.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "            attn_weights = attn_weights + causal_mask # Attention mask is upper triangular of negative infinity\n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(query_state.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_state) # B x H x N x D/H\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "            attn_output = attn_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "        elif attention_method == 'sdpa':\n",
    "            attn_output = torch.nn.functional.scaled_dot_product_attention(query_state, key_state, value_state, dropout_p = 0.0, is_causal = True)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            attn_output = attn_output.view(B, N, D)\n",
    "        elif attention_method == 'fa2':\n",
    "            query_state = query_state.transpose(1, 2)\n",
    "            key_state = key_state.transpose(1, 2)\n",
    "            value_state = value_state.transpose(1, 2)\n",
    "            attn_output = _flash_attention_forward(\n",
    "                query_state,\n",
    "                key_state,\n",
    "                value_state,\n",
    "                causal_mask,\n",
    "                N,\n",
    "                dropout = 0.0,\n",
    "                use_top_left_mask = False,\n",
    "                is_causal = True\n",
    "            )\n",
    "            attn_output = attn_output.reshape(B, N, D).contiguous()\n",
    "\n",
    "        ### Post-SA linear layer + Sum to Residual Stream ###\n",
    "        attn_output = layer.self_attn.o_proj(attn_output)\n",
    "        hidden_state = residual + attn_output\n",
    "\n",
    "        ### Pre-MLP Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        \n",
    "        ### MLP ###\n",
    "        TOP_K = layer.mlp.top_k # 8\n",
    "        N_EXPERTS = layer.mlp.num_experts # 64\n",
    "\n",
    "        hidden_state = hidden_state.view(B * N, D) # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        router_logits = layer.mlp.gate(hidden_state) # Output BN x N_EXPERTS (routing probability for each token)\n",
    "        routing_weights = F.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "\n",
    "        # Below both routing_weights and selected_experts are of size BN x TOP_K (for each token, the selected TOP_K experts and corresponding weights)\n",
    "        # Weights do NOT sum to 1 since we only top_k'd after the softmax\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, TOP_K, dim = -1) \n",
    "        routing_weights = routing_weights.to(hidden_state.dtype)\n",
    "\n",
    "        mlp_output = torch.zeros((B * N, D), dtype = hidden_state.dtype, device = hidden_state.device) # Initialize MLP output - later iterate through experts and sum onto this object\n",
    "        # One hot encode - for each expert, which topk x token is active - e.g. expert_assignment_mask[0, :] will be 0s if the first expert is never chosen\n",
    "        expert_assignment_mask = torch.nn.functional.one_hot(selected_experts, num_classes = N_EXPERTS).permute(2, 1, 0) # Creates (N_EXPERTS, TOP_K, BN)\n",
    "        \n",
    "        # Iterate through all the experts, apply each expert to the tokens where the expert are relevant, multiple output by the weights for the topk/token for that expert, then sum onto the mlp_output obj\n",
    "        for expert_ix, expert_wrapper in enumerate(layer.mlp.experts):\n",
    "            expert_device = expert_wrapper.target_device\n",
    "            expert = expert_wrapper.expert\n",
    "\n",
    "            # For this expert, gives the (topk, token) coordinates which uses the expert\n",
    "            topk_slot, token_indices = torch.where(expert_assignment_mask[expert_ix, :])\n",
    "            # Get hidden states for tokens that use this expert - shape of num_assigned_tokens x D\n",
    "            tokens_for_expert = hidden_state[token_indices, :]\n",
    "\n",
    "            # Get expert output, multiply by routing weights\n",
    "            gate_output = expert.gate_proj(tokens_for_expert.to(expert_device))\n",
    "            expert_output = expert.act_fn(expert.gate_proj(tokens_for_expert.to(expert_device))) * expert.up_proj(tokens_for_expert.to(expert_device)) # Gate * up_proj\n",
    "            expert_output = expert.down_proj(expert_output) # Down project it -> Shape = num_assigned_tokens x D\n",
    "            expert_output = expert_output.to(main_device) * routing_weights[token_indices, topk_slot].unsqueeze(1) # For each num_assigned_tokens, multiples it by the corresponding weight in topk_slot fort that token_index\n",
    "\n",
    "            mlp_output.index_add_(0, token_indices, expert_output.to(hidden_state.dtype))\n",
    "\n",
    "        mlp_output = mlp_output.reshape(B, N, D) # Convert back from BN x D -> B x N x D\n",
    "\n",
    "        ### Post-MLP Sum to Residual Stream ###\n",
    "        hidden_state = residual + mlp_output\n",
    "        \n",
    "        all_router_logits += (router_logits, ) # Also save router logits from this layer\n",
    "\n",
    "\n",
    "class OlmoeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The top level model object. Also handles weight initialization and loss calculations.\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf):\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "\n",
    "        ### Layers ###\n",
    "        self.embed_tokens = nn.Embedding(self.vocab_size, self.D, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [OlmoeBlock(self.conf, layer_idx) for layer_idx in range(self.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = OlmoeRMSNorm(config.hidden_size, eps = self.conf.rms_norm_eps)\n",
    "        self.rotary_emb = OlmoeRotaryEmbedding(config = self.conf)\n",
    "        self.lm_head = nn.Linear(self.D, self.vocab_size, bias = False)\n",
    "\n",
    "        ### Init ###\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # OlMoE weight initiation - see https://github.com/huggingface/transformers/blob/8f1509a96c96747c893051ac947795cfb0750357/src/transformers/modeling_utils.py#L2500-L2515\n",
    "    # Normal distribution for linear layers + embeddings\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "            # In the vocab -> embedding layer, set all embeddings to 0 for the padding token (tokenizer.pad_token_id)\n",
    "            self.embed_tokens.weight.data[self.padding_idx].zero_()\n",
    "    \n",
    "    # Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "    def load_balancing_loss_func(gate_logits, num_experts, top_k, attention_mask):\n",
    "        compute_device = gate_logits[0].device\n",
    "        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim = 0)\n",
    "        routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "        _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "        if attention_mask is None:\n",
    "            # Compute the percentage of tokens routed to each experts\n",
    "            tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "            # Compute the average probability of routing to these experts\n",
    "            router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "        else:\n",
    "            batch_size, sequence_length = attention_mask.shape\n",
    "            num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "            # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "            expert_attention_mask = (attention_mask[None, :, :, None, None].expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts)).reshape(-1, top_k, num_experts).to(compute_device))\n",
    "            # Compute the percentage of tokens routed to each experts\n",
    "            tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(expert_attention_mask, dim=0)\n",
    "            # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "            router_per_expert_attention_mask = (attention_mask[None, :, :, None].expand((num_hidden_layers, batch_size, sequence_length, num_experts)).reshape(-1, num_experts).to(compute_device))\n",
    "            # Compute the average probability of routing to these experts\n",
    "            router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(router_per_expert_attention_mask, dim=0)\n",
    "        overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "        return overall_loss * num_experts\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor = None, attention_mask: torch.Tensor = None, position_ids: torch.LongTensor = None, labels: torch.LongTensor = None):\n",
    "        \n",
    "        embeds_output = self.embed_tokens(input_ids)\n",
    "        B, N, D = embeds_output.shape\n",
    "\n",
    "        cache_position = torch.arange(0, N, device = embeds_output.device)\n",
    "        position_ids = cache_position.unsqueeze(0)\n",
    "        # Flash attention mask\n",
    "        causal_mask  = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "        position_embeddings = self.rotary_emb(embeds_output, position_ids) # Position embeddings to be shared across the decoder layers\n",
    "        \n",
    "        hidden_state = embeds_output\n",
    "\n",
    "        # Now iterate through the layers\n",
    "        all_router_logits = () # Save router logits from each layer into this; will be needed for balancing loss\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            hidden_state, router_logits = layer(hidden_state)\n",
    "            all_router_logits += (router_logits, ) # Also save router logits from this layer\n",
    "\n",
    "        hidden_state = self.norm(hidden_state)\n",
    "        output_logits = self.lm_head(hidden_state)\n",
    "\n",
    "        ##### Calculate Loss #####\n",
    "        # The labels object should be a tensor of token IDs or -100 (for attention mask, since don't want to calculate loss for those)\n",
    "        label_ids = torch.where(input_ids == self.padding_idx, torch.tensor(-100), input_ids)\n",
    "        # Get regular loss\n",
    "        base_loss = ForCausalLMLoss(output_logits, label_ids, self.conf.vocab_size)\n",
    "        # Get load balancing loss\n",
    "        aux_loss = self.load_balancing_loss_func(gate_logits = all_router_logits, num_experts = self.conf.n_experts, top_k = self.conf.top_k, attention_mask = attention_mask)\n",
    "        # Get total loss = regular loss + .01 * load bal loss\n",
    "        loss = base_loss + self.router_aux_loss_coef * aux_loss \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "\n",
    "ACT2FN['silu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.loss.loss_utils import ForCausalLMLoss\n",
    "\n",
    "ForCausalLMLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
