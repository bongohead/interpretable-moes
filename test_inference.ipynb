{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "import math\n",
    "from accelerate import Accelerator\n",
    "from helpers.memory import check_memory\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "main_device = 'cuda:0'\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi-GPU Access\n",
    "\n",
    "# Verify Pytorch can communicate with each GPU\n",
    "def check_communication():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device = torch.device(f\"cuda:{i}\")\n",
    "        try:\n",
    "            x = torch.tensor([1.0, 2.0, 3.0], device = device)\n",
    "            print(f\"GPU {i}: Computation successful.\")\n",
    "        except Exception as e:\n",
    "            print(f\"GPU {i}: Computation failed. Error: {e}\")\n",
    "\n",
    "check_communication()\n",
    "\n",
    "# Initialize accelerator - will later use for training\n",
    "accelerator = Accelerator(\n",
    "    device_placement = True,\n",
    "    mixed_precision = 'bf16',\n",
    "    gradient_accumulation_steps = 1, # Temp\n",
    "    split_batches = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference with Base HF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False, padding_side = 'right')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/OLMoE-1B-7B-0924',\n",
    "    device_map = main_device, \n",
    "    torch_dtype = torch.bfloat16,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up experts but keep everything else on the main GPU \n",
    "# (to avoid having to constantly switch devices for other computations)\n",
    "class DistributedExpertWrapper(torch.nn.Module):\n",
    "    def __init__(self, expert, target_device):\n",
    "        super().__init__()\n",
    "        self.expert = expert.to(target_device)\n",
    "        self.target_device = target_device\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move input to expert's device, compute, and return to original device\n",
    "        orig_device = x.device\n",
    "        out = self.expert(x.to(self.target_device))\n",
    "        return out.to(orig_device)  # Now explicitly returning the output\n",
    "\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "for layer_idx, layer in enumerate(model.model.layers):\n",
    "    experts = layer.mlp.experts\n",
    "    num_experts = len(experts)\n",
    "    experts_per_gpu = num_experts // num_gpus\n",
    "    \n",
    "    for expert_idx in range(num_experts):\n",
    "        target_gpu = (expert_idx // experts_per_gpu) % num_gpus\n",
    "        # if target_gpu != 0:  # Only move & add a wrapper if not GPU 0\n",
    "        target_device = f\"cuda:{target_gpu}\"\n",
    "        layer.mlp.experts[expert_idx] = DistributedExpertWrapper(\n",
    "            experts[expert_idx], \n",
    "            target_device\n",
    "        )\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with .pipeline()\n",
    "@torch.no_grad()\n",
    "def eval_model_v1(model, tokenizer, prompt, max_new_tokens):\n",
    "    tokens = tokenizer(prompt, return_tensors = \"pt\").to(main_device)\n",
    "    res = model.generate(\n",
    "        **tokens,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        do_sample = False,\n",
    "        eos_token_id = [tokenizer.eos_token_id]\n",
    "        )\n",
    "    print(res)\n",
    "    return tokenizer.batch_decode(res)[0]\n",
    "\n",
    "print(eval_model_v1(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    'I am a dog and I like to eat. My favorite food is',\n",
    "    1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with token-by-token generation\n",
    "@torch.no_grad()\n",
    "def eval_model_v2(model, tokenizer, prompt, max_new_tokens):\n",
    "    tokens = tokenizer(prompt, return_tensors = 'pt').to(main_device)['input_ids']\n",
    "    i = 1\n",
    "    while i <= max_new_tokens:\n",
    "        output = model(tokens)\n",
    "        logits = output['logits']\n",
    "        output_token = torch.argmax(F.softmax(logits.squeeze(), dim = 1), dim = 1)[-1]\n",
    "        print(output_token)\n",
    "        tokens = torch.cat((tokens, output_token.view(1, 1)), dim = 1)\n",
    "                \n",
    "        if output_token in [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|end|>\")]:\n",
    "            break\n",
    "\n",
    "        i = i + 1\n",
    "    \n",
    "    print(tokens)\n",
    "    return tokenizer.batch_decode(tokens)[0]\n",
    "\n",
    "print(eval_model_v2(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    'I am a dog and I like to eat. My favorite food is',\n",
    "    1\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Engineer the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'I am a dog and I like to eat. My favorite food is' # Correct next token output is 'steak'\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to(main_device)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split up LM head + rest of model\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Everything before the LM head\n",
    "    decoder_output = model.model(\n",
    "        input_ids,\n",
    "        attention_mask\n",
    "    )['last_hidden_state']\n",
    "    \n",
    "    # The LM head\n",
    "    output_logits = model.lm_head(decoder_output)\n",
    "\n",
    "output_ids = torch.argmax(output_logits[0, :, :], dim = 1)\n",
    "print(tokenizer.decode(output_ids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split out embeddings layer + seperate decoder layers\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Embedding layer\n",
    "    embeds_output = model.model.embed_tokens(input_ids)\n",
    "    B, N, D = embeds_output.shape\n",
    "\n",
    "    cache_position = torch.arange(0, N, device = embeds_output.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, embeds_output, cache_position, None, False)\n",
    "\n",
    "    position_embeddings = model.model.rotary_emb(embeds_output, position_ids) # Position embeddings to be shared across the decoder layers\n",
    "\n",
    "    hidden_state = embeds_output\n",
    "\n",
    "    # Now iterate through the layers\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        \n",
    "        # We can ignore all the arguments related to caching/intermediate outputs\n",
    "        layer_output = layer(\n",
    "            hidden_state,\n",
    "            causal_mask,\n",
    "            position_ids,\n",
    "            position_embeddings = position_embeddings\n",
    "        )\n",
    "\n",
    "        hidden_state = layer_output[0]\n",
    "        \n",
    "    # RMS Norm\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "    # The LM head\n",
    "    output_logits = model.lm_head(hidden_state)\n",
    "\n",
    "\n",
    "output_ids = torch.argmax(output_logits[0, :, :], dim = 1)\n",
    "print(tokenizer.decode(output_ids[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Further split out the transformer layers into SA module + MLP module\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Embedding layer\n",
    "    embeds_output = model.model.embed_tokens(input_ids)\n",
    "    B, N, D = embeds_output.shape\n",
    "\n",
    "    cache_position = torch.arange(0, N, device = embeds_output.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, embeds_output, cache_position, None, False)\n",
    "\n",
    "    position_embeddings = model.model.rotary_emb(embeds_output, position_ids) # Position embeddings to be shared across the decoder layers\n",
    "\n",
    "    hidden_state = embeds_output\n",
    "\n",
    "    # Now iterate through the layers\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        \n",
    "        residual = hidden_state\n",
    "\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        ### SA ###\n",
    "        hidden_state, _, _ = layer.self_attn(\n",
    "            hidden_state,\n",
    "            attention_mask = causal_mask,\n",
    "            position_ids = position_ids,\n",
    "            position_embeddings = position_embeddings\n",
    "        )\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        ### MLP ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        hidden_state, router_logits = layer.mlp(hidden_state)\n",
    "        hidden_state = residual + hidden_state\n",
    "                \n",
    "    # RMS Norm\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "    # The LM head\n",
    "    output_logits = model.lm_head(hidden_state)\n",
    "\n",
    "\n",
    "output_ids = torch.argmax(output_logits[0, :, :], dim = 1)\n",
    "print(tokenizer.decode(output_ids[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Further split out the SA module into raw max components (or SDPA sub-module), and split out the MLP layer into router + individual expert operations\n",
    "\n",
    "from helpers.olmoe import apply_rotary_pos_emb\n",
    "\n",
    "# The model supports 3 attention implementations\n",
    "# - normal = calculate attention normally with matrix operations\n",
    "# - sdpa = use pytorch sdpa attention implementation (same result as normal but faster with fused kernel operations)\n",
    "# - flash attention 2 = will get different results, so don't implement this\n",
    "attention_method = ['normal', 'sdpa'][1] \n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Embedding layer\n",
    "    embeds_output = model.model.embed_tokens(input_ids)\n",
    "    B, N, D = embeds_output.shape\n",
    "\n",
    "    cache_position = torch.arange(0, N, device = embeds_output.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    # This is the upper-trangular matrix of infinities to mask future tokens in the attention softmax; only needed when attention_method = 'normal'\n",
    "    causal_mask = model.model._prepare_4d_causal_attention_mask_with_cache_position(attention_mask, N, N, embeds_output.dtype, embeds_output.device, cache_position, B)\n",
    "    position_embeddings = model.model.rotary_emb(embeds_output, position_ids) # Position embeddings to be shared across the decoder layers\n",
    "\n",
    "    hidden_state = embeds_output\n",
    "\n",
    "    # Now iterate through the layers\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "\n",
    "        ### Pre-SA Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        \n",
    "        ### Self-attention ###\n",
    "        H = layer.self_attn.num_heads # Number of attention heads\n",
    "        Dh = int(D/H) # Dimensions per head\n",
    "        \n",
    "        query_state = layer.self_attn.q_norm(layer.self_attn.q_proj(hidden_state)).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "        key_state = layer.self_attn.k_norm(layer.self_attn.k_proj(hidden_state)).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "        value_state = layer.self_attn.v_proj(hidden_state).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_state, key_state = apply_rotary_pos_emb(query_state, key_state, cos, sin)\n",
    "        \n",
    "        if attention_method == 'normal':\n",
    "            # See OlMoeAttention class https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "            attn_weights = torch.matmul(query_state, key_state.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "            attn_weights = attn_weights + causal_mask # Attention mask is upper triangular of negative infinity\n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(query_state.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_state) # B x H x N x D/H\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "            attn_output = attn_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "\n",
    "        elif attention_method == 'sdpa':\n",
    "            # See OlmoeSdpaAttention class\n",
    "            # Don't pass causal mask at all, let it create it itself\n",
    "            attn_output = torch.nn.functional.scaled_dot_product_attention(query_state, key_state, value_state, dropout_p = 0.0, is_causal = True)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            attn_output = attn_output.view(B, N, D)\n",
    "        \n",
    "        else:\n",
    "            raise Exception('No')\n",
    "\n",
    "        ### Post-SA linear layer + Sum to Residual Stream ###\n",
    "        attn_output = layer.self_attn.o_proj(attn_output)\n",
    "        hidden_state = residual + attn_output\n",
    "\n",
    "        ### Pre-MLP Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        \n",
    "        ### MLP ###\n",
    "        TOP_K = layer.mlp.top_k # 8\n",
    "        N_EXPERTS = layer.mlp.num_experts # 64\n",
    "\n",
    "        hidden_state = hidden_state.view(B * N, D) # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        router_logits = layer.mlp.gate(hidden_state) # Output BN x N_EXPERTS (routing probability for each token)\n",
    "        routing_weights = F.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "\n",
    "        # Below both routing_weights and selected_experts are of size BN x TOP_K (for each token, the selected TOP_K experts and corresponding weights)\n",
    "        # Weights do NOT sum to 1 since we only top_k'd after the softmax\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, TOP_K, dim = -1) \n",
    "        routing_weights = routing_weights.to(hidden_state.dtype)\n",
    "        \n",
    "        mlp_output = torch.zeros((B * N, D), dtype = hidden_state.dtype, device = hidden_state.device) # Initialize MLP output - later iterate through experts and sum onto this object\n",
    "        # One hot encode - for each expert, which topk x token is active - e.g. expert_assignment_mask[0, :] will be 0s if the first expert is never chosen\n",
    "        expert_assignment_mask = torch.nn.functional.one_hot(selected_experts, num_classes = N_EXPERTS).permute(2, 1, 0) # Creates (N_EXPERTS, TOP_K, BN)\n",
    "\n",
    "        # Iterate through all the experts, apply each expert to the tokens where the expert are relevant, multiple output by the weights for the topk/token for that expert, then sum onto the mlp_output obj\n",
    "        for expert_ix, expert_wrapper in enumerate(layer.mlp.experts):\n",
    "            expert_device = expert_wrapper.target_device\n",
    "            expert = expert_wrapper.expert\n",
    "\n",
    "            # For this expert, gives the (topk, token) coordinates which uses the expert\n",
    "            topk_slot, token_indices = torch.where(expert_assignment_mask[expert_ix, :])\n",
    "            # Get hidden states for tokens that use this expert - shape of num_assigned_tokens x D\n",
    "            tokens_for_expert = hidden_state[token_indices, :]\n",
    "\n",
    "            # Get expert output, multiply by routing weights\n",
    "            expert_output = expert.down_proj(expert.act_fn(expert.gate_proj(tokens_for_expert.to(expert_device))) * expert.up_proj(tokens_for_expert.to(expert_device))) # Shape = num_assigned_tokens x D\n",
    "            expert_output = expert_output.to(main_device) * routing_weights[token_indices, topk_slot].unsqueeze(1) # For each num_assigned_tokens, multiples it by the corresponding weight in topk_slot fort that token_index\n",
    "\n",
    "            mlp_output.index_add_(0, token_indices, expert_output.to(hidden_state.dtype))\n",
    "\n",
    "        mlp_output = mlp_output.reshape(B, N, D) # Convert back from BN x D -> B x N x D\n",
    "\n",
    "        ### Post-MLP Sum to Residual Stream ###\n",
    "        hidden_state = residual + mlp_output\n",
    "                \n",
    "    # RMS Norm\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "    # LM head\n",
    "    output_logits = model.lm_head(hidden_state)\n",
    "\n",
    "\n",
    "output_ids = torch.argmax(output_logits[0, :, :], dim = 1)\n",
    "print(tokenizer.decode(output_ids[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add back in the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_method = 'normal'\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Embedding layer\n",
    "    embeds_output = model.model.embed_tokens(input_ids)\n",
    "    B, N, D = embeds_output.shape\n",
    "\n",
    "    cache_position = torch.arange(0, N, device = embeds_output.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    # This is the upper-trangular matrix of infinities to mask future tokens in the attention softmax; only needed when attention_method = 'normal'\n",
    "    causal_mask = model.model._prepare_4d_causal_attention_mask_with_cache_position(attention_mask, N, N, embeds_output.dtype, embeds_output.device, cache_position, B)\n",
    "    position_embeddings = model.model.rotary_emb(embeds_output, position_ids) # Position embeddings to be shared across the decoder layers\n",
    "\n",
    "    hidden_state = embeds_output\n",
    "\n",
    "    # Now iterate through the layers\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "\n",
    "        ### Pre-SA Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        \n",
    "        ### Self-attention ###\n",
    "        H = layer.self_attn.num_heads # Number of attention heads\n",
    "        Dh = int(D/H) # Dimensions per head\n",
    "        \n",
    "        query_state = layer.self_attn.q_norm(layer.self_attn.q_proj(hidden_state)).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "        key_state = layer.self_attn.k_norm(layer.self_attn.k_proj(hidden_state)).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "        value_state = layer.self_attn.v_proj(hidden_state).view(B, N, H, Dh).transpose(1, 2) # B x N x 2048\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_state, key_state = apply_rotary_pos_emb(query_state, key_state, cos, sin)\n",
    "        \n",
    "        if attention_method == 'normal':\n",
    "            attn_weights = torch.matmul(query_state, key_state.transpose(2, 3))/math.sqrt(Dh)  # Should be shape B x H x N x N\n",
    "            attn_weights = attn_weights + causal_mask # Attention mask is upper triangular of negative infinity\n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(query_state.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_state) # B x H x N x D/H\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "            attn_output = attn_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "        elif attention_method == 'sdpa':\n",
    "            attn_output = torch.nn.functional.scaled_dot_product_attention(query_state, key_state, value_state, dropout_p = 0.0, is_causal = True)\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            attn_output = attn_output.view(B, N, D)\n",
    "        else:\n",
    "            raise Exception('No')\n",
    "\n",
    "        ### Post-SA linear layer + Sum to Residual Stream ###\n",
    "        attn_output = layer.self_attn.o_proj(attn_output)\n",
    "        hidden_state = residual + attn_output\n",
    "\n",
    "        ### Pre-MLP Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        \n",
    "        ### MLP ###\n",
    "        TOP_K = layer.mlp.top_k # 8\n",
    "        N_EXPERTS = layer.mlp.num_experts # 64\n",
    "\n",
    "        hidden_state = hidden_state.view(B * N, D) # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        router_logits = layer.mlp.gate(hidden_state) # Output BN x N_EXPERTS (routing probability for each token)\n",
    "        routing_weights = F.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "\n",
    "        # Below both routing_weights and selected_experts are of size BN x TOP_K (for each token, the selected TOP_K experts and corresponding weights)\n",
    "        # Weights do NOT sum to 1 since we only top_k'd after the softmax\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, TOP_K, dim = -1) \n",
    "        routing_weights = routing_weights.to(hidden_state.dtype)\n",
    "\n",
    "        mlp_output = torch.zeros((B * N, D), dtype = hidden_state.dtype, device = hidden_state.device) # Initialize MLP output - later iterate through experts and sum onto this object\n",
    "        # One hot encode - for each expert, which topk x token is active - e.g. expert_assignment_mask[0, :] will be 0s if the first expert is never chosen\n",
    "        expert_assignment_mask = torch.nn.functional.one_hot(selected_experts, num_classes = N_EXPERTS).permute(2, 1, 0) # Creates (N_EXPERTS, TOP_K, BN)\n",
    "        \n",
    "        # Iterate through all the experts, apply each expert to the tokens where the expert are relevant, multiple output by the weights for the topk/token for that expert, then sum onto the mlp_output obj\n",
    "        for expert_ix, expert_wrapper in enumerate(layer.mlp.experts):\n",
    "            expert_device = expert_wrapper.target_device\n",
    "            expert = expert_wrapper.expert\n",
    "\n",
    "            # For this expert, gives the (topk, token) coordinates which uses the expert\n",
    "            topk_slot, token_indices = torch.where(expert_assignment_mask[expert_ix, :])\n",
    "            # Get hidden states for tokens that use this expert - shape of num_assigned_tokens x D\n",
    "            tokens_for_expert = hidden_state[token_indices, :]\n",
    "\n",
    "            # Get expert output, multiply by routing weights\n",
    "            expert_output = expert.down_proj(expert.act_fn(expert.gate_proj(tokens_for_expert.to(expert_device))) * expert.up_proj(tokens_for_expert.to(expert_device))) # Shape = num_assigned_tokens x D\n",
    "            expert_output = expert_output.to(main_device) * routing_weights[token_indices, topk_slot].unsqueeze(1) # For each num_assigned_tokens, multiples it by the corresponding weight in topk_slot fort that token_index\n",
    "\n",
    "            mlp_output.index_add_(0, token_indices, expert_output.to(hidden_state.dtype))\n",
    "\n",
    "        mlp_output = mlp_output.reshape(B, N, D) # Convert back from BN x D -> B x N x D\n",
    "\n",
    "        ### Post-MLP Sum to Residual Stream ###\n",
    "        hidden_state = residual + mlp_output\n",
    "                \n",
    "    # RMS Norm\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    \n",
    "    # LM head\n",
    "    output_logits = model.lm_head(hidden_state)\n",
    "\n",
    "\n",
    "output_ids = torch.argmax(output_logits[0, :, :], dim = 1)\n",
    "print(tokenizer.decode(output_ids[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
