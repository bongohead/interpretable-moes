{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 1: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 2: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 3: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "import math\n",
    "from helpers.memory import check_memory, profile_memory\n",
    "from helpers.logging import get_gradient_stats\n",
    "from helpers.moe_utils import check_cosine_similarity\n",
    "from helpers.dataset import load_shard_as_dataloader\n",
    "from dataclasses import dataclass, asdict\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob \n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from config import ModelConf, TrainConf\n",
    "from moe import OlmoeModel\n",
    "from train import train\n",
    "\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.5339e+00, -1.3411e-07,  0.0000e+00],\n",
      "        [-1.3411e-07,  1.4727e+00,  2.3842e-07],\n",
      "        [ 0.0000e+00,  2.3842e-07,  3.6907e+00]])\n",
      "tensor([[ 3.5339,  1.4842, -0.4625],\n",
      "        [ 1.4842,  1.4727,  1.3688],\n",
      "        [-0.4625,  1.3688,  3.6907]])\n",
      "tensor([[-1.8224,  0.3410, -0.0952, -0.2958],\n",
      "        [ 0.1713,  0.2342, -1.0960, -0.4328],\n",
      "        [ 0.3380,  0.6748,  0.8158, -1.5670]])\n",
      "tensor([[-1.8224,  0.3410, -0.0952, -0.2958],\n",
      "        [-0.5618,  0.5124, -0.0708, -0.9432],\n",
      "        [ 0.5737,  0.6246,  0.8216, -1.5154]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from helpers.moe_utils import differentiable_gram_schmidt\n",
    "# write a test\n",
    "n_experts = 3\n",
    "D = 4\n",
    "x = torch.randn(n_experts, D) \n",
    "\n",
    "ortho_x = differentiable_gram_schmidt(x,keep_magnitude=True,use_random_order=True)\n",
    "\n",
    "# the non-diagonal elements should be close to 0\n",
    "gram_matrix = ortho_x @ ortho_x.T\n",
    "\n",
    "gram_matrix_x = x @ x.T\n",
    "\n",
    "print(gram_matrix)\n",
    "print(gram_matrix_x)\n",
    "\n",
    "print(ortho_x)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = ModelConf(\n",
    "    D = 768, \n",
    "    H = 8,\n",
    "    I = 512,\n",
    "    n_experts = 30,\n",
    "    n_shared_experts = 2,\n",
    "    top_k = 4,\n",
    "    norm_topk_prob = False,\n",
    "    n_layers = 10,\n",
    "    max_position_embeddings = 2048,\n",
    "    gate_orthogonal = False,\n",
    "    is_freeze_weights = False,\n",
    "    main_device = 'cuda:0',\n",
    "    keep_magnitude =True,\n",
    "    use_random_order = True\n",
    ")\n",
    "\n",
    "train_conf = TrainConf(\n",
    "    router_cos_loss_coef = 0.01,\n",
    ")\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 478,609,152\n",
      "Device 0: NVIDIA H200\n",
      "  Allocated: 0.89 GB\n",
      "  Reserved: 1.14 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 1: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 2: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 3: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Let's load the model\n",
    "- Set the default_device to specify where all the non-expert layers live (the experts are moved on model init)\n",
    "- Set the default_dtype to specify the model dtype, all params will be in this dtype except for this explicitly specified differently in class definition\n",
    "  - In the default OlMoE, RMSNorm is required to be f32 whereas all other params are bf16. \n",
    "\"\"\"\n",
    "# torch.set_default_device(conf.main_device) # This is buggy, don't use\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "torch.set_float32_matmul_precision('medium') # See https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html \n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = OlmoeModel(\n",
    "    model_conf,\n",
    "    primary_device = model_conf.main_device, # Where to store dense layers and shared experts\n",
    "    expert_device_map = [model_conf.main_device] * model_conf.n_experts #=, here let's test them with all of them on cuda:0\n",
    ")\n",
    "model = torch.compile(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some data\n",
    "Default setting: \n",
    "\n",
    "\n",
    "    1.02 vs 0.0027\n",
    "    D 768-> 768*4 : 0.530 vs 0.001\n",
    "    n_experts 30-> 120 : 4.305 vs 0.010\n",
    "\n",
    "    D_768 -> 768/4 : 2.166 vs 0.0054\n",
    "    n_experts 30-> 8: 0.261 vs 0.00089\n",
    "\n",
    "    D 768-> 768*4 and n_experts 30-> 120: 2.151 vs 0.0052\n",
    "    D 768-> 768/4 and n_experts 30-> 8:  0.476 vs 0.0018\n",
    "\n",
    "linear dependency on # of experts\n",
    "$O(1/\\sqrt{d})$ dependency on dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the lm loss is ~ 11, and the aux loss is 2~4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuanbo096\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/interpretable-moes/experiments_cli/base_olmoe_ortho_mapping/wandb/run-20250223_175851-sv8xkq7v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuanbo096/interpretable-moes/runs/sv8xkq7v' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/yuanbo096/interpretable-moes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yuanbo096/interpretable-moes' target=\"_blank\">https://wandb.ai/yuanbo096/interpretable-moes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yuanbo096/interpretable-moes/runs/sv8xkq7v' target=\"_blank\">https://wandb.ai/yuanbo096/interpretable-moes/runs/sv8xkq7v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup a Wandb run for logging. Choose a run name and notes for the run!\n",
    "\"\"\"\n",
    "RUN_NAME = 'test'\n",
    "RUN_NOTES = 'None'\n",
    "\n",
    "load_dotenv('./../../secrets.env')\n",
    "wandb.login(key = os.getenv('WANDB_API_KEY'))\n",
    "run = wandb.init(\n",
    "    project = 'interpretable-moes', \n",
    "    name = RUN_NAME,\n",
    "    notes = RUN_NOTES,\n",
    "    config = {**asdict(model_conf), **asdict(train_conf)}\n",
    ")\n",
    "\n",
    "# (Optional) Also log various info as a wandb media object.\n",
    "additional_log_notes = {\n",
    "    'run_name': RUN_NAME,\n",
    "    'notes': RUN_NOTES,\n",
    "    'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_model_params': sum(p.numel() for p in model.parameters()),\n",
    "    'available_cuda_gpus': [torch.cuda.get_device_properties(i).name for i in range(torch.cuda.device_count())],\n",
    "    'model_conf': asdict(model_conf),\n",
    "    'train_conf': asdict(train_conf)\n",
    "}\n",
    "\n",
    "wandb.log({'conf': wandb.Html(f\"<pre style='font-size:12px;'>{json.dumps(additional_log_notes, indent = 2)}</pre>\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = load_shard_as_dataloader(\n",
    "    './../../data/val_shard.json',\n",
    "    tokenizer,\n",
    "    batch_size = 32,\n",
    "    seq_len = 2048,\n",
    "    eos_seperator_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1946 shards.\n",
      "\n",
      "=== Loading shard ./../../data/train_shard_0.json (index 0) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/workspace/interpretable-moes/experiments_cli/base_olmoe_ortho_mapping/train.py:92\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, tokenizer, train_conf, model_conf, val_dl, seed, save_dir)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_idx, shard_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(shard_files):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Loading shard \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m     shard_dl \u001b[38;5;241m=\u001b[39m \u001b[43mload_shard_as_dataloader_mp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_seperator_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(shard_dl):\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m         \u001b[38;5;66;03m# ====================== SPLIT BATCH INTO MICRO-BATCHES ======================\u001b[39;00m\n\u001b[1;32m     97\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(model_conf\u001b[38;5;241m.\u001b[39mmain_device)\n",
      "File \u001b[0;32m/workspace/interpretable-moes/helpers/dataset.py:144\u001b[0m, in \u001b[0;36mload_shard_as_dataloader_mp\u001b[0;34m(shard_path, tokenizer, batch_size, seq_len, eos_seperator_id, shuffle)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Create a pool of processes\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes \u001b[38;5;241m=\u001b[39m num_processes) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 144\u001b[0m     tokenized_lines \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_tokenize_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_encode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_encode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_seperator_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# 3) Concatenate everything into one big token buffer\u001b[39;00m\n\u001b[1;32m    147\u001b[0m big_token_buffer \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, tokenizer, train_conf, model_conf, val_dl, seed, save_dir = 'test')\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
