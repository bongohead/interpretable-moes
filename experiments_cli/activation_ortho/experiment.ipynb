{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 1: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 2: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 3: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "import math\n",
    "from helpers.memory import check_memory, profile_memory\n",
    "from helpers.logging import get_gradient_stats\n",
    "from helpers.moe_utils import check_cosine_similarity\n",
    "from helpers.dataset import load_shard_as_dataloader_mp\n",
    "from dataclasses import dataclass, asdict\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob \n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from config import ModelConf, TrainConf, OrthoMappingConf\n",
    "from moe import OlmoeModel\n",
    "from train import train\n",
    "\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = 'test'\n",
    "RUN_NOTES = 'Baseline test without cosine loss'\n",
    "save_dir = 'test'\n",
    "\n",
    "model_conf = ModelConf(\n",
    "    D = 768, \n",
    "    H = 12,\n",
    "    I = 768*4,\n",
    "    n_experts = 16,\n",
    "    n_shared_experts = 0,\n",
    "    top_k = 2,\n",
    "    norm_topk_prob = False,\n",
    "    n_layers = 12,\n",
    "    max_position_embeddings = 1024,\n",
    "    main_device = 'cuda:0'\n",
    ")\n",
    "\n",
    "train_conf = TrainConf(\n",
    "    micro_batch_size = 8,\n",
    "    accumulation_steps = 2,\n",
    "    seq_len = 1024, \n",
    "    use_lflb = False\n",
    ")\n",
    "\n",
    "or_conf = OrthoMappingConf(\n",
    "    is_gate_orthogonal_init = False,\n",
    "    is_freeze_gate_weights = False,\n",
    "    router_cos_loss_coef = 0,\n",
    "    expert_cos_loss_coef = 0.01\n",
    "\n",
    ")\n",
    "\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd12c8ec949b444492dad957a7be059c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9511b8f072954086b7066c2e31b7eb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c511637fe24f578dd710ec107bceab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,464,718,080\n",
      "Device 0: NVIDIA H200\n",
      "  Allocated: 2.77 GB\n",
      "  Reserved: 2.98 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 1: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 2: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n",
      "Device 3: NVIDIA H200\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 139.83 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Let's load the model\n",
    "- Set the default_device to specify where all the non-expert layers live (the experts are moved on model init)\n",
    "- Set the default_dtype to specify the model dtype, all params will be in this dtype except for this explicitly specified differently in class definition\n",
    "  - In the default OlMoE, RMSNorm is required to be f32 whereas all other params are bf16. \n",
    "\"\"\"\n",
    "# torch.set_default_device(conf.main_device) # This is buggy, don't use\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "torch.set_float32_matmul_precision('medium') # See https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html \n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = OlmoeModel(\n",
    "    model_conf,\n",
    "    or_conf,\n",
    "    primary_device = model_conf.main_device, # Where to store dense layers and shared experts\n",
    "    expert_device_map = [model_conf.main_device] * model_conf.n_experts #=, here let's test them with all of them on cuda:0\n",
    ")\n",
    "model = torch.compile(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0] or:\n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0] to include these operations in the captured graph.\n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0] Graph break: from user code at:\n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0]   File \"/workspace/interpretable-moes/experiments_cli/current/moe.py\", line 689, in forward\n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0]     causal_mask  = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0] \n",
      "W0226 14:14:48.270000 143420 torch/_dynamo/variables/tensor.py:869] [0/0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 16, 768])\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_143420/2449217069.py\", line 12, in <module>\n",
      "    output = model(inputs['input_ids'], inputs['attention_mask'], moe_method = 'forward_slow', use_lflb = True, use_checkpointing = False)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/interpretable-moes/experiments_cli/current/moe.py\", line 696, in forward\n",
      "    hidden_state, router_logits, topk_experts, expert_outputs = layer(\n",
      "                                                                ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/interpretable-moes/experiments_cli/current/moe.py\", line 531, in forward\n",
      "    def custom_forward(hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings: tuple[torch.Tensor, torch.Tensor], moe_method: str, use_lflb: bool = False):\n",
      "  File \"/workspace/interpretable-moes/experiments_cli/current/moe.py\", line 531, in torch_dynamo_resume_in_forward_at_531\n",
      "    def custom_forward(hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings: tuple[torch.Tensor, torch.Tensor], moe_method: str, use_lflb: bool = False):\n",
      "  File \"/workspace/interpretable-moes/experiments_cli/current/moe.py\", line 531, in custom_forward\n",
      "    def custom_forward(hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings: tuple[torch.Tensor, torch.Tensor], moe_method: str, use_lflb: bool = False):\n",
      "  File \"/workspace/interpretable-moes/experiments_cli/current/moe.py\", line 538, in torch_dynamo_resume_in_custom_forward_at_538\n",
      "    attn_output = self.self_attn(\n",
      "  File \"/workspace/interpretable-moes/experiments_cli/current/moe.py\", line 551, in torch_dynamo_resume_in_custom_forward_at_551\n",
      "    mlp_output, router_logits, topk_experts = self.moe(hidden_state, moe_method = moe_method, use_lflb = use_lflb)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: too many values to unpack (expected 3)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2170, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1182, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/stack_data/core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/stack_data/utils.py\", line 77, in collapse_repeated\n",
      "    for is_highlighted, group in itertools.groupby(\n",
      "                                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/stack_data/utils.py\", line 45, in highlight_unique\n",
      "    counts = Counter(lst)\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/collections/__init__.py\", line 607, in __init__\n",
      "    self.update(iterable, **kwds)\n",
      "  File \"/usr/lib/python3.12/collections/__init__.py\", line 699, in update\n",
      "    _count_elements(self, iterable)\n",
      "TypeError: unhashable type: 'dict'\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Let's load a forward pass with a batch size of 2, to make sure the model is able to run\n",
    "- If you have multiple working forward methods, this is a good chance to test them for equality\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False)\n",
    "prompt = ['I am a dog and I like to eat. My favorite food is', 'My cat is']\n",
    "inputs = tokenizer(prompt, truncation = True, max_length = 128, padding = 'max_length', return_tensors = 'pt').to(model_conf.main_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(inputs['input_ids'], inputs['attention_mask'], moe_method = 'forward_slow', use_lflb = True, use_checkpointing = False)\n",
    "    # output = model(inputs['input_ids'], inputs['attention_mask'], moe_method = 'forward_fast', use_lflb = True, use_checkpointing = False)\n",
    "    # output = model(inputs['input_ids'], inputs['attention_mask'], moe_method = 'forward_async', use_lflb = True, use_checkpointing = False)\n",
    "\n",
    "output_ids = torch.argmax(output['logits'][:, :, :], dim = 2)\n",
    "for i in range(output_ids.size(0)):\n",
    "    idx = inputs[\"attention_mask\"].sum(dim = -1)[i].item() - 1 # get length of attention mask to find the last non-mask output token ix\n",
    "    print(tokenizer.decode(output_ids[i, idx], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload expert_cos_loss_func\n",
    "import importlib\n",
    "from helpers import moe_utils\n",
    "importlib.reload(moe_utils)\n",
    "from helpers.moe_utils import expert_cos_loss_func\n",
    "start_time = time.time()\n",
    "mean_loss, layer_losses = expert_cos_loss_func(model, model_conf)\n",
    "compute_time = time.time() - start_time\n",
    "\n",
    "# validate results\n",
    "print(f\"running time: {compute_time:.4f} seconds\")\n",
    "print(f\"average loss: {mean_loss.item():.4f}\")\n",
    "print(\"layer losses:\", [f\"{l.item():.4f}\" for l in layer_losses])\n",
    "\n",
    "# basic assertions\n",
    "assert isinstance(mean_loss, torch.Tensor)\n",
    "assert len(layer_losses) == model_conf.n_layers\n",
    "assert all(isinstance(l, torch.Tensor) for l in layer_losses)\n",
    "assert mean_loss == torch.mean(torch.stack(layer_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001)\n",
      "tensor(18.4207)\n",
      "tensor(0.7719)\n",
      "-9.999999889225291e-09\n"
     ]
    }
   ],
   "source": [
    "from helpers import moe_utils\n",
    "import importlib\n",
    "importlib.reload(moe_utils)\n",
    "from helpers.moe_utils import gap_loss_func\n",
    "\n",
    "def test_gap_loss_func():\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 测试基本情况（无attention_mask）\n",
    "    gate_logits = [\n",
    "        torch.tensor([[10., 0., 0.], [0., 10., 0.]]),  # 层1：明确选择专家0和1\n",
    "        torch.tensor([[0., 10., 0.], [0., 0., 10.]])   # 层2：明确选择专家1和2\n",
    "    ]\n",
    "    loss = gap_loss_func(gate_logits, top_k=1, attention_mask=None)\n",
    "    print(loss)\n",
    "    # assert torch.allclose(loss, torch.tensor(0.0), atol=1e-4)\n",
    "\n",
    "    # 测试有attention_mask的情况\n",
    "    mask = torch.tensor([[1, 0], [1, 1]])  # 第二个样本的第一个token被mask\n",
    "    gate_logits = [\n",
    "        torch.tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]),  # 模拟batch_size=2, seq_len=2\n",
    "    ]\n",
    "    loss_masked = gap_loss_func(gate_logits, top_k=1, attention_mask=mask)\n",
    "    \n",
    "    # 计算期望值：有效token数为3（mask掉1个）\n",
    "    routing_weights = torch.softmax(torch.cat(gate_logits), -1)\n",
    "    top2_vals, _ = torch.topk(routing_weights, k=2, dim=-1)\n",
    "    expected_loss = -torch.log(top2_vals[:,0] - top2_vals[:,1] + 1e-8)[[0,2,3]].mean()\n",
    "    assert torch.allclose(loss_masked, expected_loss), \"掩码处理不正确\"\n",
    "\n",
    "    # 测试均匀分布时的损失值\n",
    "    uniform_logits = [torch.zeros(4, 3)]  # 均匀分布logits\n",
    "    uniform_loss = gap_loss_func(uniform_logits, top_k=1, attention_mask=None)\n",
    "    print(uniform_loss)\n",
    "\n",
    "    # 测试不同top_k值\n",
    "    specific_logits = [torch.tensor([[4.,3.]])]  # top_k=2时差距应为1.0\n",
    "    loss_k2 = gap_loss_func(specific_logits, top_k=1, attention_mask=None)\n",
    "    expected = -math.log(1.0 + 1e-8)\n",
    "    print(loss_k2)\n",
    "    print(expected)\n",
    "\n",
    "\n",
    "\n",
    "test_gap_loss_func()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uniform_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muniform_loss\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uniform_loss' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some data\n",
    "Default setting: \n",
    "\n",
    "\n",
    "    1.02 vs 0.0027\n",
    "    D 768-> 768*4 : 0.530 vs 0.001\n",
    "    n_experts 30-> 120 : 4.305 vs 0.010\n",
    "\n",
    "    D_768 -> 768/4 : 2.166 vs 0.0054\n",
    "    n_experts 30-> 8: 0.261 vs 0.00089\n",
    "\n",
    "    D 768-> 768*4 and n_experts 30-> 120: 2.151 vs 0.0052\n",
    "    D 768-> 768/4 and n_experts 30-> 8:  0.476 vs 0.0018\n",
    "\n",
    "linear dependency on # of experts\n",
    "$O(1/\\sqrt{d})$ dependency on dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the lm loss is ~ 11, and the aux loss is 2~4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = load_shard_as_dataloader_mp(\n",
    "    './../../data/val_shard.json',\n",
    "    tokenizer,\n",
    "    batch_size = 32,\n",
    "    seq_len = 2048,\n",
    "    eos_seperator_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('./../../secrets.env')\n",
    "wandb.login(key = os.getenv('WANDB_API_KEY'))\n",
    "run = wandb.init(\n",
    "    project = 'interpretable-moes', \n",
    "    name = RUN_NAME,\n",
    "    notes = RUN_NOTES,\n",
    "    config = {**asdict(model_conf), **asdict(train_conf), **asdict(or_conf)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "importlib.reload(train)\n",
    "\n",
    "train.train(model, tokenizer, train_conf, model_conf, or_conf, val_dl, seed, save_dir = 'test')\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
