{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from helpers.memory import check_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from helpers.expert_specialization import get_context_labeled_data, get_js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'moonshotai/Moonlight-16B-A3B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda()\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# Hooks needed: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/blob/main/modeling_deepseek.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "def attach_moe_gate_hooks(model):\n",
    "    \"\"\"\n",
    "    Registers forward-hooks on each MoE gating module in 'model'\n",
    "    so that after a forward pass, we can retrieve the BN x topk 'topk_idx'\n",
    "    from each layer that uses MoE.\n",
    "\n",
    "    Returns:\n",
    "        all_expert_ids: A list that will be appended to at runtime\n",
    "                        with the topk_idx Tensor from each MoEGate.\n",
    "        handles:        A dict {layer_index: hook_handle}, so you can remove them if desired.\n",
    "    \"\"\"\n",
    "    all_expert_ids = []\n",
    "    handles = {}\n",
    "\n",
    "    def gate_forward_hook(module, inputs, output):\n",
    "        \"\"\"\n",
    "        This hook is triggered after MoEGate.forward(...).\n",
    "        'output' should be the tuple: (topk_idx, topk_weight).\n",
    "        We only need topk_idx here.\n",
    "        \"\"\"\n",
    "        (topk_idx, topk_weight) = output\n",
    "        all_expert_ids.append(topk_idx.detach())  # shape [B*N, top_k]\n",
    "\n",
    "    # The DeepseekV3ForCausalLM has `self.model` -> DeepseekV3Model -> .layers\n",
    "    # Each layer has `self.mlp`, which might be DeepseekV3MoE or DeepseekV3MLP\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        # Check if this layer is indeed MoE (DeepseekV3MoE)\n",
    "        if hasattr(layer.mlp, 'gate'):\n",
    "            # layer.mlp.gate is the actual MoEGate object\n",
    "            gate_module = layer.mlp.gate\n",
    "            gate_module._layer_id = layer_idx  # optional attribute for debugging\n",
    "            hook_handle = gate_module.register_forward_hook(gate_forward_hook)\n",
    "            handles[layer_idx] = hook_handle\n",
    "\n",
    "    return all_expert_ids, handles\n",
    "\n",
    "\n",
    "all_expert_ids, hook_handles = attach_moe_gate_hooks(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "for topk_idx_tensor in all_expert_ids:\n",
    "    print(f\"topk_idx shape = {topk_idx_tensor.shape}\")\n",
    "\n",
    "for layer_idx, h in hook_handles.items():\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# No need for hooks! https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_topk_experts, hook_handles = attach_moe_gate_hooks(model)\n",
    "    outputs = model(**inputs)\n",
    "    for layer_idx, h in hook_handles.items():\n",
    "        h.remove()\n",
    "\n",
    "all_topk_experts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_ics(model, test_token_data_list, use_topk1 = False):\n",
    "    \"\"\"\n",
    "    Modified from `helpers.expert_specialization.get_ics` for this model.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    n_layers = model.config.num_hidden_layers - 1 # first layer has no experts\n",
    "    n_experts = model.config.n_routed_experts\n",
    "\n",
    "    for test_item in test_token_data_list:\n",
    "        test_token = test_item[\"test_token\"]\n",
    "        test_token_id = test_item[\"test_token_id\"]\n",
    "        test_meanings = test_item[\"test_meanings\"]\n",
    "        dl = test_item[\"dl\"]\n",
    "\n",
    "        meaning_counts_per_layer = [\n",
    "            [torch.zeros(n_experts, dtype = torch.long, device = 'cpu') for _ in range(len(test_meanings))]\n",
    "            for _ in range(n_layers)\n",
    "        ] # meaning_counts_per_layer[l][meaning_idx]\"\" torch.LongTensor of shape (n_experts,) expert-length count\n",
    "        # total_counts_per_layer[l]: same shape (n_experts,), expert-length count for baseline usage\n",
    "        total_counts_per_layer = [torch.zeros(n_experts, dtype = torch.long, device = 'cpu') for _ in range(n_layers)]\n",
    "\n",
    "        # Map each meaning string to an index\n",
    "        meaning_to_idx = {m: i for i, m in enumerate(test_meanings)}\n",
    "\n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            batch_meanings = batch[\"test_meanings\"]\n",
    "            B, N = input_ids.shape\n",
    "\n",
    "            all_topk_experts, hook_handles = attach_moe_gate_hooks(model)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            for layer_idx, h in hook_handles.items():\n",
    "                h.remove()\n",
    "\n",
    "            if use_topk1 == True:\n",
    "                all_topk_experts = tuple(x[..., :1] for x in all_topk_experts)\n",
    "\n",
    "            flat_ids = input_ids.view(-1)  # shape (B*N, ) # Flatten the input IDs for indexing alignment with all_topk_experts\n",
    "            \n",
    "            # Convert each example's meaning label to an integer index shape (B,). e.g. meaning_id_array[b] = meaning_idx of that example\n",
    "            meaning_id_array = []\n",
    "            for b_idx in range(B):\n",
    "                m_str = batch_meanings[b_idx]\n",
    "                meaning_id_array.append(meaning_to_idx[m_str])\n",
    "            meaning_id_array = torch.tensor(meaning_id_array, device = model.device)  # (B,)\n",
    "            meaning_id_array = meaning_id_array.unsqueeze(1).repeat(1, N).view(-1)  # Expand to shape (B, N), so each token in that example has the same meaning\n",
    "\n",
    "\n",
    "            for l in range(n_layers):\n",
    "                layer_exps = all_topk_experts[l]  # shape (BN, top_k)\n",
    "\n",
    "                # A) Baseline distribution for the token - we want all rows where (flat_ids == test_token_id)\n",
    "                base_mask = (flat_ids == test_token_id)\n",
    "                base_idx = base_mask.nonzero(as_tuple=True)[0]\n",
    "                if len(base_idx) > 0:\n",
    "                    base_exps = layer_exps[base_idx, :] # gather => shape (#rows, top_k)\n",
    "                    base_exps = base_exps.view(-1) # flatten => (#rows * top_k,)\n",
    "                    hist_base = torch.bincount(base_exps, minlength = n_experts) # bincount => shape (n_experts,)\n",
    "                    hist_base = hist_base.cpu()\n",
    "                    total_counts_per_layer[l] += hist_base\n",
    "\n",
    "                # B) For each meaning m, gather usage mask: (flat_ids == test_token_id) & (meaning_id_array == m)\n",
    "                for m_idx in range(len(test_meanings)):\n",
    "                    meaning_mask = (flat_ids == test_token_id) & (meaning_id_array == m_idx)\n",
    "                    mm_idx = meaning_mask.nonzero(as_tuple=True)[0]\n",
    "                    if len(mm_idx) > 0:\n",
    "                        m_exps = layer_exps[mm_idx, :]\n",
    "                        m_exps = m_exps.view(-1)\n",
    "                        hist_m = torch.bincount(m_exps, minlength = n_experts)\n",
    "                        hist_m = hist_m.cpu()\n",
    "                        meaning_counts_per_layer[l][m_idx] += hist_m\n",
    "\n",
    "        # Now compute the average JS distance for each layer (comparing each meaning vs. the overall distribution) then averaging\n",
    "        layer_js_distances = []\n",
    "        for l in range(n_layers):\n",
    "            meaning_dists = []\n",
    "            \n",
    "            # Convert total_counts -> python dict \n",
    "            base_array = total_counts_per_layer[l].numpy()\n",
    "            dict_base = {}\n",
    "            for ex_id, c_val in enumerate(base_array):\n",
    "                if c_val > 0:\n",
    "                    dict_base[ex_id] = int(c_val)\n",
    "            \n",
    "            # Each meaning\n",
    "            for m_idx in range(len(test_meanings)):\n",
    "                sense_arr = meaning_counts_per_layer[l][m_idx].numpy()\n",
    "                dict_sense = {}\n",
    "                for ex_id, c_val in enumerate(sense_arr):\n",
    "                    if c_val > 0:\n",
    "                        dict_sense[ex_id] = int(c_val)\n",
    "                d_js = get_js_distance(dict_sense, dict_base)\n",
    "                meaning_dists.append(d_js)\n",
    "\n",
    "            avg_js = sum(meaning_dists) / len(meaning_dists)\n",
    "            layer_js_distances.append(avg_js)\n",
    "\n",
    "        results[test_token] = {layer_idx: val for layer_idx, val in enumerate(layer_js_distances)}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_tis(model, test_token_data_list, pad_token_id, use_topk1 = False):\n",
    "    \"\"\"\n",
    "    Modified from `helpers.expert_specialization.get_ics` for this model.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    n_layers = model.config.num_hidden_layers - 1 # first layer has no experts\n",
    "    n_experts = model.config.n_routed_experts\n",
    "\n",
    "    for token_item in test_token_data_list:\n",
    "        token_str = token_item[\"test_token\"]\n",
    "        token_id = token_item[\"test_token_id\"]\n",
    "        dl = token_item[\"dl\"]\n",
    "\n",
    "        # For each layer, we'll track:\n",
    "        global_counts_per_layer = [torch.zeros(n_experts, dtype = torch.long) for _ in range(n_layers)]\n",
    "        token_counts_per_layer = [torch.zeros(n_experts, dtype = torch.long) for _ in range(n_layers)]        \n",
    "\n",
    "        for i, batch in enumerate(dl):\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "            all_topk_experts, hook_handles = attach_moe_gate_hooks(model)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            for layer_idx, h in hook_handles.items():\n",
    "                h.remove()\n",
    "\n",
    "            if use_topk1 == True:\n",
    "                all_topk_experts = tuple(x[..., :1] for x in all_topk_experts)\n",
    "\n",
    "\n",
    "            flat_ids = input_ids.view(-1)  # shape B*N\n",
    "\n",
    "            nonpad_mask = (flat_ids != pad_token_id) # (A) Non-pad\n",
    "            token_mask = (flat_ids == token_id) # (B) This specific token\n",
    "\n",
    "            # For each layer, accumulate counts via bincount\n",
    "            for l in range(n_layers):\n",
    "                layer_exps = all_topk_experts[l]  # (B*N, topk)\n",
    "                \n",
    "                # A) Global usage (non-pad) gather only the rows where nonpad_mask is True\n",
    "                nonpad_indices = nonpad_mask.nonzero(as_tuple = True)[0]\n",
    "                if len(nonpad_indices) > 0:\n",
    "                    nonpad_rows = layer_exps[nonpad_indices, :] # shape (#nonpad, top_k)\n",
    "                    nonpad_rows = nonpad_rows.view(-1) # flatten => shape (#nonpad*top_k,)\n",
    "                    hist_global = torch.bincount(nonpad_rows, minlength = n_experts)\n",
    "                    global_counts_per_layer[l] += hist_global.cpu()\n",
    "\n",
    "                # B) Token usage (token_mask)\n",
    "                token_indices = token_mask.nonzero(as_tuple = True)[0]\n",
    "                if len(token_indices) > 0:\n",
    "                    token_rows = layer_exps[token_indices, :] # shape (#token, top_k)\n",
    "                    token_rows = token_rows.view(-1)\n",
    "                    hist_token = torch.bincount(token_rows, minlength = n_experts)\n",
    "                    token_counts_per_layer[l] += hist_token.cpu()\n",
    "                    \n",
    "        # Now compute the JS distance for each layer, comparing token_expert_counts vs. global_expert_counts\n",
    "        layer_js_list = []\n",
    "        for l in range(n_layers):\n",
    "            dict_global = {}\n",
    "            dict_token = {}\n",
    "            global_arr = global_counts_per_layer[l].numpy()\n",
    "            token_arr = token_counts_per_layer[l].numpy()\n",
    "            # Build dictionaries for get_js_distance\n",
    "            for ex_id, count_val in enumerate(global_arr):\n",
    "                dict_global[ex_id] = int(count_val)\n",
    "            for ex_id, count_val in enumerate(token_arr):\n",
    "                dict_token[ex_id] = int(count_val)\n",
    "            d_js = get_js_distance(dict_token, dict_global)\n",
    "            layer_js_list.append(d_js)\n",
    "\n",
    "        results[token_str] = {i: val for i, val in enumerate(layer_js_list)}\n",
    "        \n",
    "    return results\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_ec(model, test_token_data_list, use_topk1 = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_layers = model.config.num_hidden_layers - 1 # first layer has no experts\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for token_item in test_token_data_list:\n",
    "        token_str = token_item[\"test_token\"]\n",
    "        token_id = token_item[\"test_token_id\"]\n",
    "        dl = token_item[\"dl\"]\n",
    "\n",
    "        # overlap_count[l] = number of occurrences that keep at least one expert from layer l->l+1\n",
    "        # total_count[l]   = total number of token occurrences we see for layer l\n",
    "        overlap_count = [0 for _ in range(n_layers-1)]\n",
    "        total_count   = [0 for _ in range(n_layers-1)]\n",
    "\n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)       # (B,N)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "            all_topk_experts, hook_handles = attach_moe_gate_hooks(model)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            for layer_idx, h in hook_handles.items():\n",
    "                h.remove()\n",
    "\n",
    "            if use_topk1 == True:\n",
    "                all_topk_experts = tuple(x[..., :1] for x in all_topk_experts)\n",
    "\n",
    "\n",
    "            flat_ids = input_ids.view(-1)\n",
    "\n",
    "            valid_mask = (flat_ids == token_id)\n",
    "            valid_indices = valid_mask.nonzero(as_tuple = True)[0]  # 1D array of row indices\n",
    "\n",
    "            # 3) For each layer up to n_layers - 2, check overlap with layer + 1\n",
    "            # We'll gather the topk experts for those valid_indices in layer l and l+1\n",
    "            for l in range(n_layers - 1):\n",
    "                exps_l     = all_topk_experts[l]    # (BN, top_k)\n",
    "                exps_next  = all_topk_experts[l+1]  # (BN, top_k)\n",
    "\n",
    "                # Gather relevant rows => shape (#valid, top_k)\n",
    "                exps_l_valid    = exps_l[valid_indices, :]\n",
    "                exps_next_valid = exps_next[valid_indices, :]\n",
    "\n",
    "                total_count[l] += exps_l_valid.size(0) # total_count[l] += #valid\n",
    "\n",
    "                # Now check overlap row by row in a vectorized manner.\n",
    "                # If top_k = 1, simpler check: just eq\n",
    "                if exps_l_valid.size(1) == 1:\n",
    "                    same = (exps_l_valid[:, 0] == exps_next_valid[:, 0]) # shape (#valid,)\n",
    "                    overlap_count[l] += same.sum().item()\n",
    "                else:\n",
    "                    # exps_l_valid, exps_next_valid: both shape (#valid, top_k)\n",
    "                    # We'll do a broadcast eq => shape (#valid, top_k, top_k).\n",
    "                    # If ANY of [top_k x top_k] is True => there's intersection.\n",
    "                    # Then we reduce \"any\" across dims 1 & 2 => shape (#valid,).\n",
    "                    # We'll sum up how many are True => that many have overlap.\n",
    "                    exps_l_3d = exps_l_valid.unsqueeze(2) # shape (#valid, top_k, 1)\n",
    "                    exps_next_3d = exps_next_valid.unsqueeze(1) # shape (#valid, 1, top_k)\n",
    "                    eq_matrix = (exps_l_3d == exps_next_3d) # eq => shape (#valid, top_k, top_k)\n",
    "                    overlap_bool = eq_matrix.any(dim=(1,2))  # overlap => shape (#valid,)\n",
    "                    overlap_count[l] += overlap_bool.sum().item()\n",
    "\n",
    "        continuity_dict = {}\n",
    "        for l in range(n_layers - 1):\n",
    "            continuity_dict[l] = overlap_count[l] / total_count[l]\n",
    "            \n",
    "        continuity_dict[n_layers-1] = 0.0 # define last layer's continuity=0.0\n",
    "\n",
    "        results[token_str] = continuity_dict\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_aware_test_dataset = get_context_labeled_data(\"./../../data/contextual-tokens/samples_*.yaml\", tokenizer, 512, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ics = get_ics(model, context_aware_test_dataset[0:20])\n",
    "tis = get_tis(model, context_aware_test_dataset[0:20], tokenizer.pad_token_id)\n",
    "ec = get_ec(model, context_aware_test_dataset[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ics1 = get_ics(model, context_aware_test_dataset[0:20], use_topk1 = True)\n",
    "tis1 = get_tis(model, context_aware_test_dataset[0:20], tokenizer.pad_token_id, use_topk1 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import plotly.express as px \n",
    "\n",
    "def plot_token_layer_metrics(input_data, title, yaxis_title, color, show_token_lines):\n",
    "    \n",
    "    df_all = pd.DataFrame([\n",
    "        {\"token\": token, \"layer\": layer_idx, \"avg_value\": val} \n",
    "        for token, layer_dict in input_data.items()\n",
    "        for layer_idx, val in layer_dict.items() \n",
    "    ])\n",
    "\n",
    "    df_avg = df_all.groupby(\"layer\", as_index = False)[\"avg_value\"].mean().assign(token  = \"__AVG__\")\n",
    "    if show_token_lines == True:\n",
    "        df_plot = pd.concat([df_all, df_avg], ignore_index = True)\n",
    "    else:\n",
    "        df_plot = df_avg\n",
    "\n",
    "    fig = px.line(df_plot, x = \"layer\", y = \"avg_value\", color = \"token\", title = title, range_y = (0,1), markers = True)\n",
    "\n",
    "    for trace in fig.data:\n",
    "        if trace.name == \"__AVG__\":\n",
    "            trace.update(line = dict(color = color, width = 2), opacity = 1.0, marker = dict(size = 6), name = \"Average\")\n",
    "        else:\n",
    "            trace.update(line = dict(color = color, width = 1), opacity = 0.2, marker = dict(size = 1))\n",
    "    \n",
    "    fig.add_shape(type = \"line\", x0 = 0, x1 = 1, xref = \"paper\", y0 = 0.5, y1 = 0.5, yref = \"y\", line = dict(color = \"gray\", dash = \"dash\"))\n",
    "    fig.update_layout(width = 600, height = 400, xaxis_title = \"Layer\", yaxis_title = yaxis_title, showlegend = False)\n",
    "    fig.show()\n",
    "\n",
    "plot_token_layer_metrics(ics, 'ICS by layer', 'ICS(l)', color = 'firebrick', show_token_lines = True)\n",
    "plot_token_layer_metrics(tis, 'TIS by layer', 'TIS(l)', color = 'cornflowerblue', show_token_lines = True)\n",
    "plot_token_layer_metrics(ec, 'EC by layer', 'EC(l)', color = 'forestgreen', show_token_lines = True)\n",
    "plot_token_layer_metrics(ics1, 'ICS by layer for Topk = 1', 'ICS(l)', color = 'firebrick', show_token_lines = True)\n",
    "plot_token_layer_metrics(tis1, 'TIS by layer for Topk = 1', 'TIS(l)', color = 'cornflowerblue', show_token_lines = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
