{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from helpers.memory import check_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "from helpers.expert_specialization import get_context_aware_test_data, get_js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'deepseek-ai/DeepSeek-V2-Lite'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16,trust_remote_code = True).cuda()\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# No need for hooks! https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "def attach_moe_gate_hooks(model):\n",
    "    \"\"\"\n",
    "    Registers forward-hooks on each MoE gating module in 'model' so that after a forward pass,\n",
    "    we can retrieve the BN x topk 'topk_idx' from each layer.\n",
    "    \n",
    "    Returns:\n",
    "        all_expert_ids: A list that will be filled at runtime with\n",
    "                        tuples of (layer_index, topk_idx_tensor).\n",
    "        handles: A dictionary of {layer_index: hook_handle}, so you can remove them if desired.\n",
    "    \"\"\"\n",
    "    all_expert_ids = []\n",
    "    handles = {}\n",
    "\n",
    "    def gate_forward_hook(module, input, output):\n",
    "        \"\"\"\n",
    "        This hook is triggered after MoEGate.forward(...).\n",
    "        'output' should be the tuple: (topk_idx, topk_weight, aux_loss).\n",
    "        We only need topk_idx here.\n",
    "        \"\"\"\n",
    "        topk_idx, topk_weight, aux_loss = output\n",
    "        # You might want to store 'topk_idx' along with which layer it came from\n",
    "        layer_id = getattr(module, \"_layer_id\", None)\n",
    "        all_expert_ids.append((layer_id, topk_idx.detach().cpu()))\n",
    "\n",
    "        # Optionally store them in a more structured way:\n",
    "        # e.g. all_expert_ids[layer_id] = topk_idx.detach().cpu()\n",
    "\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        # Layer 0 is not moe\n",
    "        if layer_idx > 0:\n",
    "            # attach an attribute so we know which layer this gating belongs to\n",
    "            layer.mlp.gate._layer_id = layer_idx\n",
    "            hook_handle = layer.mlp.gate.register_forward_hook(gate_forward_hook)\n",
    "            handles[layer_idx] = hook_handle\n",
    "\n",
    "    return all_expert_ids, handles\n",
    "\n",
    "all_expert_ids, hook_handles = attach_moe_gate_hooks(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "for (layer_idx, topk_idx_tensor) in all_expert_ids:\n",
    "    print(f\"Layer {layer_idx}: topk_idx shape = {topk_idx_tensor.shape}\")\n",
    "    # e.g. shape is [B*N, top_k]\n",
    "\n",
    "# 4) (Optional) remove hooks if you no longer need them\n",
    "for layer_idx, h in hook_handles.items():\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[2].mlp.last_topk_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_context_awareness_metric(model, test_token_data_list):\n",
    "    \"\"\"\n",
    "    Compute the CA metric for each test token x layer, using JS distance to compare each meaning-specific expert distribution vs. \n",
    "     the overall distribution for that token.\n",
    "    \n",
    "    Params:\n",
    "        @model: The model, must return `all_topk_experts` which is a tuple of length equal to # layers, where each element of\n",
    "          the tuple is a BN x topk tensor of selected expert IDs.\n",
    "        @test_token_data_list: A list of dictionaries of the exact format returned by `get_context_aware_test_data`.\n",
    "\n",
    "    Returns:\n",
    "        A dict of format:\n",
    "            {\n",
    "                test_token1: {0: .52, 1: .34, ...},\n",
    "                test_token2: {0: .55, 1: .62, ...},\n",
    "                ...\n",
    "            },\n",
    "          where the keys represent layer indices and the values represent context-awareness scores between 0 - 1\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    model.eval()\n",
    "\n",
    "    # Number of layers from the model config\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "\n",
    "    for test_item in test_token_data_list:\n",
    "        test_token = test_item[\"test_token\"]\n",
    "        test_token_id = test_item[\"test_token_id\"]\n",
    "        test_meanings = test_item[\"test_meanings\"]\n",
    "        dl = test_item[\"dl\"]\n",
    "\n",
    "        meaning_counts_per_layer = [\n",
    "            [defaultdict(int) for _ in range(len(test_meanings))]  # one dict per meaning\n",
    "            for _ in range(n_layers)\n",
    "        ] # meaning_counts_per_layer[l][meaning_idx][expert_id] -> count per meaning\n",
    "        total_counts_per_layer = [defaultdict(int) for _ in range(n_layers)] # total_counts_per_layer[l][expert_id]: count for the all meanings baseline\n",
    "\n",
    "        # Map each meaning string to an index\n",
    "        meaning_to_idx = {m: i for i, m in enumerate(test_meanings)}\n",
    "\n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            batch_meanings = batch[\"test_meanings\"]\n",
    "            B, N = input_ids.shape\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, output_router_logits = True)\n",
    "            all_topk_experts = ()\n",
    "            for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "                # layer_router_logits is shape [B*N, num_experts]\n",
    "                gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "                _, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1)\n",
    "                all_topk_experts += (topk_experts,) \n",
    "\n",
    "            # Flatten the input IDs for indexing alignment with all_topk_experts\n",
    "            flat_input_ids = input_ids.view(-1)  # shape (B*N, )\n",
    "            \n",
    "            for l in range(n_layers):\n",
    "                layer_experts = all_topk_experts[l]  # shape (B*N, topk)\n",
    "\n",
    "                for token_index in range(B * N):\n",
    "                    if flat_input_ids[token_index].item() == test_token_id:\n",
    "                        b_idx = token_index // N # Figure out which example in the batch we belong to\n",
    "                        meaning_label = batch_meanings[b_idx]\n",
    "                        meaning_idx = meaning_to_idx[meaning_label]\n",
    "\n",
    "                        # Gather all top-k experts\n",
    "                        topk_exs = layer_experts[token_index]  # shape (topk,)\n",
    "                        for ex_val in topk_exs:\n",
    "                            ex_id = int(ex_val.item())\n",
    "                            meaning_counts_per_layer[l][meaning_idx][ex_id] += 1\n",
    "                            total_counts_per_layer[l][ex_id] += 1\n",
    "\n",
    "\n",
    "        # Now compute the average JS distance for each layer (comparing each meaning vs. the overall distribution) then averaging\n",
    "        layer_js_distances = []\n",
    "\n",
    "        for l in range(n_layers):\n",
    "            layer_sense_dists = []\n",
    "            for s_idx in range(len(test_meanings)):\n",
    "                d_js = get_js_distance(meaning_counts_per_layer[l][s_idx], total_counts_per_layer[l])\n",
    "                layer_sense_dists.append(d_js)\n",
    "\n",
    "            if len(layer_sense_dists) > 0:\n",
    "                avg_js = sum(layer_sense_dists) / len(layer_sense_dists)\n",
    "            else:\n",
    "                avg_js = 0.0\n",
    "\n",
    "            layer_js_distances.append(avg_js)\n",
    "\n",
    "        results[test_token] = {i: val for i, val in enumerate(layer_js_distances)}\n",
    "\n",
    "    return results\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_token_specialization_metric(model, test_token_data_list, pad_token_id):\n",
    "    \"\"\"\n",
    "    Computes a token specialization metric for each test token x layer, using the JS distance b/t: (a) the distribution of \n",
    "      experts used for that token and (b) the distribution of experts used for *all* tokens (excluding padding).\n",
    "    \n",
    "    Params:\n",
    "        @model: The model, must return `all_topk_experts` which is a tuple of length equal to # layers, where each element of\n",
    "          the tuple is a BN x topk tensor of selected expert IDs.\n",
    "        @test_token_data_list: A list of dictionaries of the exact format returned by `get_context_aware_test_data`.\n",
    "        @pad_token_id: The ID used for padding, which we should exclude from the \"global usage\" distribution.\n",
    "\n",
    "    Returns:\n",
    "        A dict of format:\n",
    "            {\n",
    "                test_token1: {0: .52, 1: .34, ...},\n",
    "                test_token2: {0: .55, 1: .62, ...},\n",
    "                ...\n",
    "            },\n",
    "          where the keys represent layer indices and the values represent token-specialization scores between 0 - 1\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for token_item in test_token_data_list:\n",
    "        token_str = token_item[\"test_token\"]\n",
    "        token_id = token_item[\"test_token_id\"]\n",
    "        dl = token_item[\"dl\"]\n",
    "\n",
    "        # For each layer, we'll track:\n",
    "        token_expert_counts = [defaultdict(int) for _ in range(n_layers)] # token_expert_counts[l][expert_id] = # of times `token_id` is assigned to expert_id\n",
    "        global_expert_counts = [defaultdict(int) for _ in range(n_layers)] # global_expert_counts[l][expert_id] = # of times ANY non-pad token is assigned to expert_id\n",
    " \n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            B, N = input_ids.shape\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, output_router_logits = True)\n",
    "            all_topk_experts = ()\n",
    "            for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "                # layer_router_logits is shape [B*N, num_experts]\n",
    "                gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "                _, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1)\n",
    "                all_topk_experts += (topk_experts,) \n",
    "\n",
    "            flat_ids = input_ids.view(-1)  # shape B*N\n",
    "            # for each layer, shape (B*N, topk)\n",
    "            for l in range(n_layers):\n",
    "                layer_experts = all_topk_experts[l]  # (B*N, topk)\n",
    "                for idx in range(B*N):\n",
    "                    # skip if it's padded\n",
    "                    if flat_ids[idx].item() == pad_token_id:\n",
    "                        continue\n",
    "\n",
    "                    # Add global usage counts\n",
    "                    topk_exs = layer_experts[idx]\n",
    "                    for ex_id_val in topk_exs:\n",
    "                        ex_id = int(ex_id_val.item())\n",
    "                        global_expert_counts[l][ex_id] += 1\n",
    "\n",
    "                    # If it's our target token, also track token_expert_counts\n",
    "                    if flat_ids[idx].item() == token_id:\n",
    "                        for ex_id_val in topk_exs:\n",
    "                            ex_id = int(ex_id_val.item())\n",
    "                            token_expert_counts[l][ex_id] += 1\n",
    "\n",
    "        # Now compute the JS distance for each layer, comparing token_expert_counts vs. global_expert_counts\n",
    "        layer_js_list = []\n",
    "        for l in range(n_layers):\n",
    "            d_js = get_js_distance(token_expert_counts[l], global_expert_counts[l])\n",
    "            layer_js_list.append(d_js)\n",
    "\n",
    "        results[token_str] = {i: val for i, val in enumerate(layer_js_list)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_aware_test_dataset = get_context_aware_test_data(\"./../../data/contextual-tokens/samples_*.yaml\", tokenizer, 512, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_awareness = get_context_awareness_metric(model, context_aware_test_dataset[0:9])\n",
    "token_specialization = get_token_specialization_metric(model, context_aware_test_dataset[0:9], tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import plotly.express as px \n",
    "\n",
    "def plot_ca(ca_data):\n",
    "    # 1) For each layer, average CA across all tokens\n",
    "    n_layers = len(next(iter(ca_data.values())))  # e.g. 24\n",
    "    layer_vals = []\n",
    "    \n",
    "    # Create a list of (layer, avg_ca) pairs\n",
    "    for layer_idx in range(n_layers):\n",
    "        sum_val = 0.0\n",
    "        count = 0\n",
    "        for _, layer_dict in ca_data.items():\n",
    "            sum_val += layer_dict[layer_idx]\n",
    "            count += 1\n",
    "        avg_ca = sum_val / count\n",
    "        layer_vals.append((layer_idx, avg_ca))\n",
    "        \n",
    "    df = pd.DataFrame(layer_vals, columns=[\"layer\", \"avg_ca\"])\n",
    "    \n",
    "    fig = px.line(df, x = 'layer', y = 'avg_ca', title = \"Average Context-Awareness by Layer\", range_y = (0, 1), markers = True)\n",
    "    \n",
    "    fig.update_layout(xaxis_title = 'Layer', yaxis_title = 'Average CA(l)', yaxis = dict(tickformat = '.2f'), width = 600, height = 400)\n",
    "    fig.add_hline(y = 0.5, line_dash = 'dash', line_color = 'red')\n",
    "    fig.show()\n",
    "\n",
    "def plot_ts(ts_data):\n",
    "    # 1) For each layer, average CA across all tokens\n",
    "    n_layers = len(next(iter(ts_data.values())))  # e.g. 24\n",
    "    layer_vals = []\n",
    "    \n",
    "    # Create a list of (layer, avg_ts) pairs\n",
    "    for layer_idx in range(n_layers):\n",
    "        sum_val = 0.0\n",
    "        count = 0\n",
    "        for _, layer_dict in ts_data.items():\n",
    "            sum_val += layer_dict[layer_idx]\n",
    "            count += 1\n",
    "        avg_ts = sum_val / count\n",
    "        layer_vals.append((layer_idx, avg_ts))\n",
    "        \n",
    "    df = pd.DataFrame(layer_vals, columns=[\"layer\", \"avg_ts\"])\n",
    "    \n",
    "    fig = px.line(df, x = 'layer', y = 'avg_ts', title = \"Average Token Specialization by Layer\", range_y = (0, 1), markers = True)\n",
    "    \n",
    "    fig.update_layout(xaxis_title = 'Layer', yaxis_title = 'Average TS(l)', yaxis = dict(tickformat = '.2f'), width = 600, height = 400)\n",
    "    fig.add_hline(y = 0.5, line_dash = 'dash', line_color = 'red')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_ca(context_awareness)\n",
    "plot_ts(token_specialization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
