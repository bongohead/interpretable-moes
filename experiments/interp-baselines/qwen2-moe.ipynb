{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from helpers.memory import check_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from collections import defaultdict\n",
    "from helpers.expert_specialization import get_context_aware_test_data, get_js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'Qwen/Qwen1.5-MoE-A2.7B-Chat'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16,trust_remote_code = True).cuda()\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# For Qwen2 MoE, just use output_router_logits.\n",
    "# No need for hooks! https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_router_logits = True)\n",
    "\n",
    "# Now parse the router logits\n",
    "# Suppose the model has L layers with MoE. outputs.router_logits is a tuple of length L.\n",
    "all_topk_experts = ()\n",
    "for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "    # layer_router_logits is shape [B*N, num_experts]\n",
    "    gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "    _, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1)\n",
    "    all_topk_experts += (topk_experts,) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_context_awareness_metric(model, test_token_data_list):\n",
    "    \"\"\"\n",
    "    Compute the CA metric for each test token x layer, using JS distance to compare each meaning-specific expert distribution vs. \n",
    "     the overall distribution for that token.\n",
    "    \n",
    "    Params:\n",
    "        @model: The model, must return `all_topk_experts` which is a tuple of length equal to # layers, where each element of\n",
    "          the tuple is a BN x topk tensor of selected expert IDs.\n",
    "        @test_token_data_list: A list of dictionaries of the exact format returned by `get_context_aware_test_data`.\n",
    "\n",
    "    Returns:\n",
    "        A dict of format:\n",
    "            {\n",
    "                test_token1: {0: .52, 1: .34, ...},\n",
    "                test_token2: {0: .55, 1: .62, ...},\n",
    "                ...\n",
    "            },\n",
    "          where the keys represent layer indices and the values represent context-awareness scores between 0 - 1\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    model.eval()\n",
    "\n",
    "    # Number of layers from the model config\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "\n",
    "    for test_item in test_token_data_list:\n",
    "        test_token = test_item[\"test_token\"]\n",
    "        test_token_id = test_item[\"test_token_id\"]\n",
    "        test_meanings = test_item[\"test_meanings\"]\n",
    "        dl = test_item[\"dl\"]\n",
    "\n",
    "        meaning_counts_per_layer = [\n",
    "            [defaultdict(int) for _ in range(len(test_meanings))]  # one dict per meaning\n",
    "            for _ in range(n_layers)\n",
    "        ] # meaning_counts_per_layer[l][meaning_idx][expert_id] -> count per meaning\n",
    "        total_counts_per_layer = [defaultdict(int) for _ in range(n_layers)] # total_counts_per_layer[l][expert_id]: count for the all meanings baseline\n",
    "\n",
    "        # Map each meaning string to an index\n",
    "        meaning_to_idx = {m: i for i, m in enumerate(test_meanings)}\n",
    "\n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            batch_meanings = batch[\"test_meanings\"]\n",
    "            B, N = input_ids.shape\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, output_router_logits = True)\n",
    "            all_topk_experts = ()\n",
    "            for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "                # layer_router_logits is shape [B*N, num_experts]\n",
    "                gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "                _, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1)\n",
    "                all_topk_experts += (topk_experts,) \n",
    "\n",
    "            # Flatten the input IDs for indexing alignment with all_topk_experts\n",
    "            flat_input_ids = input_ids.view(-1)  # shape (B*N, )\n",
    "            \n",
    "            for l in range(n_layers):\n",
    "                layer_experts = all_topk_experts[l]  # shape (B*N, topk)\n",
    "\n",
    "                for token_index in range(B * N):\n",
    "                    if flat_input_ids[token_index].item() == test_token_id:\n",
    "                        b_idx = token_index // N # Figure out which example in the batch we belong to\n",
    "                        meaning_label = batch_meanings[b_idx]\n",
    "                        meaning_idx = meaning_to_idx[meaning_label]\n",
    "\n",
    "                        # Gather all top-k experts\n",
    "                        topk_exs = layer_experts[token_index]  # shape (topk,)\n",
    "                        for ex_val in topk_exs:\n",
    "                            ex_id = int(ex_val.item())\n",
    "                            meaning_counts_per_layer[l][meaning_idx][ex_id] += 1\n",
    "                            total_counts_per_layer[l][ex_id] += 1\n",
    "\n",
    "\n",
    "        # Now compute the average JS distance for each layer (comparing each meaning vs. the overall distribution) then averaging\n",
    "        layer_js_distances = []\n",
    "\n",
    "        for l in range(n_layers):\n",
    "            layer_sense_dists = []\n",
    "            for s_idx in range(len(test_meanings)):\n",
    "                d_js = get_js_distance(meaning_counts_per_layer[l][s_idx], total_counts_per_layer[l])\n",
    "                layer_sense_dists.append(d_js)\n",
    "\n",
    "            if len(layer_sense_dists) > 0:\n",
    "                avg_js = sum(layer_sense_dists) / len(layer_sense_dists)\n",
    "            else:\n",
    "                avg_js = 0.0\n",
    "\n",
    "            layer_js_distances.append(avg_js)\n",
    "\n",
    "        results[test_token] = {i: val for i, val in enumerate(layer_js_distances)}\n",
    "\n",
    "    return results\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_token_specialization_metric(model, test_token_data_list, pad_token_id):\n",
    "    \"\"\"\n",
    "    Computes a token specialization metric for each test token x layer, using the JS distance b/t: (a) the distribution of \n",
    "      experts used for that token and (b) the distribution of experts used for *all* tokens (excluding padding).\n",
    "    \n",
    "    Params:\n",
    "        @model: The model, must return `all_topk_experts` which is a tuple of length equal to # layers, where each element of\n",
    "          the tuple is a BN x topk tensor of selected expert IDs.\n",
    "        @test_token_data_list: A list of dictionaries of the exact format returned by `get_context_aware_test_data`.\n",
    "        @pad_token_id: The ID used for padding, which we should exclude from the \"global usage\" distribution.\n",
    "\n",
    "    Returns:\n",
    "        A dict of format:\n",
    "            {\n",
    "                test_token1: {0: .52, 1: .34, ...},\n",
    "                test_token2: {0: .55, 1: .62, ...},\n",
    "                ...\n",
    "            },\n",
    "          where the keys represent layer indices and the values represent token-specialization scores between 0 - 1\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for token_item in test_token_data_list:\n",
    "        token_str = token_item[\"test_token\"]\n",
    "        token_id = token_item[\"test_token_id\"]\n",
    "        dl = token_item[\"dl\"]\n",
    "\n",
    "        # For each layer, we'll track:\n",
    "        token_expert_counts = [defaultdict(int) for _ in range(n_layers)] # token_expert_counts[l][expert_id] = # of times `token_id` is assigned to expert_id\n",
    "        global_expert_counts = [defaultdict(int) for _ in range(n_layers)] # global_expert_counts[l][expert_id] = # of times ANY non-pad token is assigned to expert_id\n",
    " \n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            B, N = input_ids.shape\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, output_router_logits = True)\n",
    "            all_topk_experts = ()\n",
    "            for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "                # layer_router_logits is shape [B*N, num_experts]\n",
    "                gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "                _, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1)\n",
    "                all_topk_experts += (topk_experts,) \n",
    "\n",
    "            flat_ids = input_ids.view(-1)  # shape B*N\n",
    "            # for each layer, shape (B*N, topk)\n",
    "            for l in range(n_layers):\n",
    "                layer_experts = all_topk_experts[l]  # (B*N, topk)\n",
    "                for idx in range(B*N):\n",
    "                    # skip if it's padded\n",
    "                    if flat_ids[idx].item() == pad_token_id:\n",
    "                        continue\n",
    "\n",
    "                    # Add global usage counts\n",
    "                    topk_exs = layer_experts[idx]\n",
    "                    for ex_id_val in topk_exs:\n",
    "                        ex_id = int(ex_id_val.item())\n",
    "                        global_expert_counts[l][ex_id] += 1\n",
    "\n",
    "                    # If it's our target token, also track token_expert_counts\n",
    "                    if flat_ids[idx].item() == token_id:\n",
    "                        for ex_id_val in topk_exs:\n",
    "                            ex_id = int(ex_id_val.item())\n",
    "                            token_expert_counts[l][ex_id] += 1\n",
    "\n",
    "        # Now compute the JS distance for each layer, comparing token_expert_counts vs. global_expert_counts\n",
    "        layer_js_list = []\n",
    "        for l in range(n_layers):\n",
    "            d_js = get_js_distance(token_expert_counts[l], global_expert_counts[l])\n",
    "            layer_js_list.append(d_js)\n",
    "\n",
    "        results[token_str] = {i: val for i, val in enumerate(layer_js_list)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_aware_test_dataset = get_context_aware_test_data(\"./../../data/contextual-tokens/samples_*.yaml\", tokenizer, 512, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_awareness = get_context_awareness_metric(model, context_aware_test_dataset)\n",
    "token_specialization = get_token_specialization_metric(model, context_aware_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
