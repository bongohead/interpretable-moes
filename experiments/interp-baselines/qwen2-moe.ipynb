{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from helpers.memory import check_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from helpers.expert_specialization import get_context_aware_test_data, get_js_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'Qwen/Qwen1.5-MoE-A2.7B-Chat'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16,trust_remote_code = True).cuda()\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# For Qwen2 MoE, just use output_router_logits.\n",
    "# No need for hooks! https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_router_logits = True)\n",
    "\n",
    "all_topk_experts = ()\n",
    "for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "    # layer_router_logits is shape [B*N, num_experts]\n",
    "    gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "    _, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1)\n",
    "    all_topk_experts += (topk_experts,) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_context_awareness_metric(model, test_token_data_list):\n",
    "    \"\"\"\n",
    "    Compute the CA metric for each test token x layer, using JS distance to compare each meaning-specific expert distribution vs. \n",
    "     the overall distribution for that token.\n",
    "    \n",
    "    Params:\n",
    "        @model: The model, must return `all_topk_experts` which is a tuple of length equal to # layers, where each element of\n",
    "          the tuple is a BN x topk tensor of selected expert IDs.\n",
    "        @test_token_data_list: A list of dictionaries of the exact format returned by `get_context_aware_test_data`.\n",
    "\n",
    "    Returns:\n",
    "        A dict of format:\n",
    "            {\n",
    "                test_token1: {0: .52, 1: .34, ...},\n",
    "                test_token2: {0: .55, 1: .62, ...},\n",
    "                ...\n",
    "            },\n",
    "          where the keys represent layer indices and the values represent context-awareness scores between 0 - 1\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    n_experts = model.config.num_experts\n",
    "\n",
    "    for test_item in test_token_data_list:\n",
    "        test_token = test_item[\"test_token\"]\n",
    "        test_token_id = test_item[\"test_token_id\"]\n",
    "        test_meanings = test_item[\"test_meanings\"]\n",
    "        dl = test_item[\"dl\"]\n",
    "\n",
    "        meaning_counts_per_layer = [\n",
    "            [torch.zeros(n_experts, dtype = torch.long, device = 'cpu') for _ in range(len(test_meanings))]\n",
    "            for _ in range(n_layers)\n",
    "        ] # meaning_counts_per_layer[l][meaning_idx]\"\" torch.LongTensor of shape (n_experts,) expert-length count\n",
    "        # total_counts_per_layer[l]: same shape (n_experts,), expert-length count for baseline usage\n",
    "        total_counts_per_layer = [torch.zeros(n_experts, dtype = torch.long, device = 'cpu') for _ in range(n_layers)]\n",
    "\n",
    "        # Map each meaning string to an index\n",
    "        meaning_to_idx = {m: i for i, m in enumerate(test_meanings)}\n",
    "\n",
    "        for batch in dl:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            batch_meanings = batch[\"test_meanings\"]\n",
    "            B, N = input_ids.shape\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, output_router_logits = True)\n",
    "            all_topk_experts = ()\n",
    "            for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "                # layer_router_logits is shape [B*N, num_experts]\n",
    "                _, topk_experts = torch.topk(layer_router_logits, k = model.config.num_experts_per_tok, dim = -1)\n",
    "                all_topk_experts += (topk_experts,) \n",
    "\n",
    "            flat_ids = input_ids.view(-1)  # shape (B*N, ) # Flatten the input IDs for indexing alignment with all_topk_experts\n",
    "            \n",
    "            # Convert each example's meaning label to an integer index shape (B,). e.g. meaning_id_array[b] = meaning_idx of that example\n",
    "            meaning_id_array = []\n",
    "            for b_idx in range(B):\n",
    "                m_str = batch_meanings[b_idx]\n",
    "                meaning_id_array.append(meaning_to_idx[m_str])\n",
    "            meaning_id_array = torch.tensor(meaning_id_array, device = model.device)  # (B,)\n",
    "            meaning_id_array = meaning_id_array.unsqueeze(1).repeat(1, N).view(-1)  # Expand to shape (B, N), so each token in that example has the same meaning\n",
    "\n",
    "\n",
    "            for l in range(n_layers):\n",
    "                layer_exps = all_topk_experts[l]  # shape (BN, top_k)\n",
    "\n",
    "                # A) Baseline distribution for the token - we want all rows where (flat_ids == test_token_id)\n",
    "                base_mask = (flat_ids == test_token_id)\n",
    "                base_idx = base_mask.nonzero(as_tuple=True)[0]\n",
    "                if len(base_idx) > 0:\n",
    "                    base_exps = layer_exps[base_idx, :] # gather => shape (#rows, top_k)\n",
    "                    base_exps = base_exps.view(-1) # flatten => (#rows * top_k,)\n",
    "                    hist_base = torch.bincount(base_exps, minlength = n_experts) # bincount => shape (n_experts,)\n",
    "                    hist_base = hist_base.cpu()\n",
    "                    total_counts_per_layer[l] += hist_base\n",
    "\n",
    "                # B) For each meaning m, gather usage mask: (flat_ids == test_token_id) & (meaning_id_array == m)\n",
    "                for m_idx in range(len(test_meanings)):\n",
    "                    meaning_mask = (flat_ids == test_token_id) & (meaning_id_array == m_idx)\n",
    "                    mm_idx = meaning_mask.nonzero(as_tuple=True)[0]\n",
    "                    if len(mm_idx) > 0:\n",
    "                        m_exps = layer_exps[mm_idx, :]\n",
    "                        m_exps = m_exps.view(-1)\n",
    "                        hist_m = torch.bincount(m_exps, minlength = n_experts)\n",
    "                        hist_m = hist_m.cpu()\n",
    "                        meaning_counts_per_layer[l][m_idx] += hist_m\n",
    "\n",
    "        # Now compute the average JS distance for each layer (comparing each meaning vs. the overall distribution) then averaging\n",
    "        layer_js_distances = []\n",
    "        for l in range(n_layers):\n",
    "            meaning_dists = []\n",
    "            \n",
    "            # Convert total_counts -> python dict \n",
    "            base_array = total_counts_per_layer[l].numpy()\n",
    "            dict_base = {}\n",
    "            for ex_id, c_val in enumerate(base_array):\n",
    "                if c_val > 0:\n",
    "                    dict_base[ex_id] = int(c_val)\n",
    "            \n",
    "            # Each meaning\n",
    "            for m_idx in range(len(test_meanings)):\n",
    "                sense_arr = meaning_counts_per_layer[l][m_idx].numpy()\n",
    "                dict_sense = {}\n",
    "                for ex_id, c_val in enumerate(sense_arr):\n",
    "                    if c_val > 0:\n",
    "                        dict_sense[ex_id] = int(c_val)\n",
    "                d_js = get_js_distance(dict_sense, dict_base)\n",
    "                meaning_dists.append(d_js)\n",
    "\n",
    "            avg_js = sum(meaning_dists) / len(meaning_dists)\n",
    "            layer_js_distances.append(avg_js)\n",
    "\n",
    "        results[test_token] = {layer_idx: val for layer_idx, val in enumerate(layer_js_distances)}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_token_specialization_metric(model, test_token_data_list, pad_token_id):\n",
    "    \"\"\"\n",
    "    Computes a token specialization metric for each test token x layer, using the JS distance b/t: (a) the distribution of \n",
    "      experts used for that token and (b) the distribution of experts used for *all* tokens (excluding padding).\n",
    "    \n",
    "    Params:\n",
    "        @model: The model, must return `all_topk_experts` which is a tuple of length equal to # layers, where each element of\n",
    "          the tuple is a BN x topk tensor of selected expert IDs.\n",
    "        @test_token_data_list: A list of dictionaries of the exact format returned by `get_context_aware_test_data`.\n",
    "        @pad_token_id: The ID used for padding, which we should exclude from the \"global usage\" distribution.\n",
    "\n",
    "    Returns:\n",
    "        A dict of format:\n",
    "            {\n",
    "                test_token1: {0: .52, 1: .34, ...},\n",
    "                test_token2: {0: .55, 1: .62, ...},\n",
    "                ...\n",
    "            },\n",
    "          where the keys represent layer indices and the values represent token-specialization scores between 0 - 1\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    n_experts = model.config.num_experts\n",
    "\n",
    "    for token_item in test_token_data_list:\n",
    "        token_str = token_item[\"test_token\"]\n",
    "        token_id = token_item[\"test_token_id\"]\n",
    "        dl = token_item[\"dl\"]\n",
    "\n",
    "        # For each layer, we'll track:\n",
    "        global_counts_per_layer = [torch.zeros(n_experts, dtype = torch.long) for _ in range(n_layers)]\n",
    "        token_counts_per_layer = [torch.zeros(n_experts, dtype = torch.long) for _ in range(n_layers)]        \n",
    "\n",
    "        for i, batch in enumerate(dl):\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, output_router_logits = True)\n",
    "\n",
    "            all_topk_experts = ()\n",
    "            for l, layer_router_logits in enumerate(outputs.router_logits): # layer_router_logits is shape [B*N, num_experts]\n",
    "                gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "                _, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1)\n",
    "                all_topk_experts += (topk_experts,) \n",
    "\n",
    "            flat_ids = input_ids.view(-1)  # shape B*N\n",
    "\n",
    "            nonpad_mask = (flat_ids != pad_token_id) # (A) Non-pad\n",
    "            token_mask = (flat_ids == token_id) # (B) This specific token\n",
    "\n",
    "            # For each layer, accumulate counts via bincount\n",
    "            for l in range(n_layers):\n",
    "                layer_exps = all_topk_experts[l]  # (B*N, topk)\n",
    "                \n",
    "                # A) Global usage (non-pad) gather only the rows where nonpad_mask is True\n",
    "                nonpad_indices = nonpad_mask.nonzero(as_tuple = True)[0]\n",
    "                if len(nonpad_indices) > 0:\n",
    "                    nonpad_rows = layer_exps[nonpad_indices, :] # shape (#nonpad, top_k)\n",
    "                    nonpad_rows = nonpad_rows.view(-1) # flatten => shape (#nonpad*top_k,)\n",
    "                    hist_global = torch.bincount(nonpad_rows, minlength = n_experts)\n",
    "                    global_counts_per_layer[l] += hist_global.cpu()\n",
    "\n",
    "                # B) Token usage (token_mask)\n",
    "                token_indices = token_mask.nonzero(as_tuple = True)[0]\n",
    "                if len(token_indices) > 0:\n",
    "                    token_rows = layer_exps[token_indices, :] # shape (#token, top_k)\n",
    "                    token_rows = token_rows.view(-1)\n",
    "                    hist_token = torch.bincount(token_rows, minlength = n_experts)\n",
    "                    token_counts_per_layer[l] += hist_token.cpu()\n",
    "                    \n",
    "        # Now compute the JS distance for each layer, comparing token_expert_counts vs. global_expert_counts\n",
    "        layer_js_list = []\n",
    "        for l in range(n_layers):\n",
    "            dict_global = {}\n",
    "            dict_token = {}\n",
    "            global_arr = global_counts_per_layer[l].numpy()\n",
    "            token_arr = token_counts_per_layer[l].numpy()\n",
    "            # Build dictionaries for get_js_distance\n",
    "            for ex_id, count_val in enumerate(global_arr):\n",
    "                dict_global[ex_id] = int(count_val)\n",
    "            for ex_id, count_val in enumerate(token_arr):\n",
    "                dict_token[ex_id] = int(count_val)\n",
    "            d_js = get_js_distance(dict_token, dict_global)\n",
    "            layer_js_list.append(d_js)\n",
    "\n",
    "        results[token_str] = {i: val for i, val in enumerate(layer_js_list)}\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_aware_test_dataset = get_context_aware_test_data(\"./../../data/contextual-tokens/samples_*.yaml\", tokenizer, 512, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_awareness = get_context_awareness_metric(model, context_aware_test_dataset[0:20])\n",
    "token_specialization = get_token_specialization_metric(model, context_aware_test_dataset[0:20], tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import plotly.express as px \n",
    "\n",
    "def plot_ca(ca_data):\n",
    "    n_layers = len(next(iter(ca_data.values()))) \n",
    "    layer_vals = []\n",
    "    \n",
    "    # Create a list of (layer, avg_ca) pairs\n",
    "    for layer_idx in range(n_layers):\n",
    "        sum_val = 0.0\n",
    "        count = 0\n",
    "        for _, layer_dict in ca_data.items():\n",
    "            sum_val += layer_dict[layer_idx]\n",
    "            count += 1\n",
    "        avg_ca = sum_val / count\n",
    "        layer_vals.append((layer_idx, avg_ca))\n",
    "        \n",
    "    df = pd.DataFrame(layer_vals, columns=[\"layer\", \"avg_ca\"])\n",
    "    \n",
    "    fig = px.line(df, x = 'layer', y = 'avg_ca', title = \"Average Context-Awareness by Layer\", range_y = (0, 1), markers = True)\n",
    "    \n",
    "    fig.update_layout(xaxis_title = 'Layer', yaxis_title = 'Average CA(l)', yaxis = dict(tickformat = '.2f'), width = 600, height = 400)\n",
    "    fig.add_hline(y = 0.5, line_dash = 'dash', line_color = 'red')\n",
    "    fig.show()\n",
    "\n",
    "def plot_ts(ts_data):\n",
    "    n_layers = len(next(iter(ts_data.values())))\n",
    "    layer_vals = []\n",
    "    \n",
    "    # Create a list of (layer, avg_ts) pairs\n",
    "    for layer_idx in range(n_layers):\n",
    "        sum_val = 0.0\n",
    "        count = 0\n",
    "        for _, layer_dict in ts_data.items():\n",
    "            sum_val += layer_dict[layer_idx]\n",
    "            count += 1\n",
    "        avg_ts = sum_val / count\n",
    "        layer_vals.append((layer_idx, avg_ts))\n",
    "        \n",
    "    df = pd.DataFrame(layer_vals, columns=[\"layer\", \"avg_ts\"])\n",
    "    \n",
    "    fig = px.line(df, x = 'layer', y = 'avg_ts', title = \"Average Token Specialization by Layer\", range_y = (0, 1), markers = True)\n",
    "    \n",
    "    fig.update_layout(xaxis_title = 'Layer', yaxis_title = 'Average TS(l)', yaxis = dict(tickformat = '.2f'), width = 600, height = 400)\n",
    "    fig.add_hline(y = 0.5, line_dash = 'dash', line_color = 'red')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_ca(context_awareness)\n",
    "plot_ts(token_specialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ca2(ca_data):\n",
    "    \n",
    "    df_all = pd.DataFrame([\n",
    "        {\"token\": token, \"layer\": layer_idx, \"ca_value\": val} \n",
    "        for token, layer_dict in ca_data.items()\n",
    "        for layer_idx, val in layer_dict.items() \n",
    "    ])\n",
    "\n",
    "    df_avg = df_all.groupby(\"layer\", as_index = False)[\"ca_value\"].mean().assign(token  = \"__AVG__\")\n",
    "    df_plot = pd.concat([df_all, df_avg], ignore_index = True)\n",
    "\n",
    "    fig = px.line(df_plot, x = \"layer\", y = \"ca_value\", color = \"token\", title = \"CA Per Layer\", range_y = (0,1), markers = True)\n",
    "\n",
    "    for trace in fig.data:\n",
    "        if trace.name == \"__AVG__\":\n",
    "            trace.update(line = dict(color = \"firebrick\", width = 2), opacity = 1.0, marker = dict(size = 6), name = \"Average CA\")\n",
    "        else:\n",
    "            trace.update(line = dict(color = \"firebrick\", width = 1), opacity = 0.2, marker = dict(size = 1))\n",
    "    \n",
    "    fig.add_shape(type=\"line\", x0=0, x1=1, xref=\"paper\", y0=0.5, y1=0.5, yref=\"y\", line=dict(color=\"gray\", dash=\"dash\"))\n",
    "    fig.update_layout(width=600, height=400, xaxis_title=\"Layer\", yaxis_title=\"CA(l)\")\n",
    "    fig.show()\n",
    "\n",
    "def plot_ts2(ts_data):\n",
    "    \n",
    "    df_all = pd.DataFrame([\n",
    "        {\"token\": token, \"layer\": layer_idx, \"ca_value\": val} \n",
    "        for token, layer_dict in ts_data.items()\n",
    "        for layer_idx, val in layer_dict.items() \n",
    "    ])\n",
    "\n",
    "    df_avg = df_all.groupby(\"layer\", as_index = False)[\"ca_value\"].mean().assign(token  = \"__AVG__\")\n",
    "    df_plot = pd.concat([df_all, df_avg], ignore_index = True)\n",
    "\n",
    "    fig = px.line(df_plot, x = \"layer\", y = \"ca_value\", color = \"token\", title = \"TS Per Layer\", range_y = (0,1), markers = True)\n",
    "\n",
    "    for trace in fig.data:\n",
    "        if trace.name == \"__AVG__\":\n",
    "            trace.update(line = dict(color = \"cornflowerblue\", width = 2), opacity = 1.0, marker = dict(size = 6), name = \"Average TS\")\n",
    "        else:\n",
    "            trace.update(line = dict(color = \"cornflowerblue\", width = 1), opacity = 0.2, marker = dict(size = 1))\n",
    "    \n",
    "    fig.add_shape(type=\"line\", x0=0, x1=1, xref=\"paper\", y0=0.5, y1=0.5, yref=\"y\", line=dict(color=\"gray\", dash=\"dash\"))\n",
    "    fig.update_layout(width=600, height=400, xaxis_title=\"Layer\", yaxis_title=\"TS(l)\")\n",
    "    fig.show()\n",
    "\n",
    "plot_ca2(context_awareness)\n",
    "plot_ts2(token_specialization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
