{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA A100 80GB PCIe\n",
      "  Allocated: 1.45 GB\n",
      "  Reserved: 1.53 GB\n",
      "  Total: 79.25 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "import math\n",
    "from helpers.memory import check_memory, profile_memory\n",
    "from helpers.logging import get_gradient_stats\n",
    "from helpers.tree import enumerate_paths\n",
    "from dataclasses import dataclass, asdict\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob \n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a conf with configurable model settings.\n",
    "- These will be passed into the model class during model initialization, so add new confs needed for whatever architecture is used.\n",
    "- If you use the default conf values with the default model class defined later, it will exactly replicate the OlMoE-7B model,\n",
    "   with 7B total params/1B active/64 experts.\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class ModelConf:\n",
    "    \"\"\"\n",
    "    General config settings for this MoE\n",
    "    \"\"\"\n",
    "    vocab_size: int = 50304 # Base OlMoE: 50304 (vocab size)\n",
    "    D: int = 2048 # Base OlMoE: 2048 (hidden state dimension)\n",
    "    H: int = 16 # Base OlMoE: 16 (number of attention heads)\n",
    "    I: int = 1024 # Base OlMoE: 1024 (expert MLP dimension)\n",
    "\n",
    "    # n_experts: int = 6 # Base OlMoE: 6 (non-shared experts only)\n",
    "    # n_shared_experts: int = 0 # Base OlMoE: 0 (base OlMoE doesn't support shared experts, but may help with inducing expert specialization - see Deepseek paper)\n",
    "    top_k: int = 2 # Base OlMoE: 8 \n",
    "    norm_topk_prob: bool = False # Base OlMoE: false (whether to normalize so that expert weights sum to 1 after topk)\n",
    "    padding_idx: int = 1 # Base OlMoE: 1 (index where padding gets mapped to)\n",
    "    n_layers: int = 16 # Base OlMoE: 16 (transformer layers)\n",
    "    rms_norm_eps: float = 1e-05 # Base OlMoE: 1e-05\n",
    "    rope_theta: float = 10000.0 # Base OlMoe: 10000.0 (this is something needed for ROPE)\n",
    "    max_position_embeddings: int = 4096 # Base OlMoE: 4096 (this is something needed for ROPE)\n",
    "    attn_method: str = 'fa2' # In OlMoE this is chosen automatically, here we explicitly pass it - choose 'normal', 'sdpa', or 'fa2'\n",
    "    \n",
    "    # Hierarchical MoE settings\n",
    "    n_layers_tree: int = 6 # The number of layers in the hierarchical tree, as least 1\n",
    "    n_branches_tree: int = 2 # The number of branches, at least 2 (maybe consider 1 for debugging)\n",
    "\n",
    "conf = ModelConf(\n",
    "    D = 768,\n",
    "    H = 8,\n",
    "    I = int(768 * 4),\n",
    "    n_layers_tree = 4,\n",
    "    n_branches_tree = 3,\n",
    "    max_position_embeddings = 2048,\n",
    ")\n",
    "# n_experts = int(self.n_branches_tree*(self.n_branches_tree ** self.n_layers_tree -1) // (self.n_branches_tree - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers_tree = conf.n_layers_tree\n",
    "n_branches_tree = conf.n_branches_tree\n",
    "node_paths, branch_paths, expert_indices  = enumerate_paths(n_layers_tree, n_branches_tree)\n",
    "node_paths = node_paths.to(main_device)\n",
    "branch_paths = branch_paths.to(main_device)\n",
    "expert_indices = expert_indices.to(main_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "These is a dump of helper functions called by the model layers, needed to make forward/backward passes correctly.\n",
    "- `_prepare_4d_causal_attention_mask_with_cache_position` is used to create the upper-triangular infinity mask for attention (not used by flash attention).\n",
    "- `load_balancing_loss_func` is the usual load balancing function.\n",
    "- Add any new functions here if needed, but most experiments won't need to touch this section.\n",
    "\"\"\"\n",
    "\n",
    "# Create the upper-trangular matrix of infinities to mask future tokens in the attention softmax (needed for SDPA + normal attention)\n",
    "# Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L1099C1-L1152 \n",
    "def _prepare_4d_causal_attention_mask_with_cache_position(attention_mask: torch.Tensor, sequence_length: int, target_length: int, dtype: torch.dtype, device: torch.device, cache_position: torch.Tensor, batch_size: int):\n",
    "    if attention_mask is not None and attention_mask.dim() == 4:\n",
    "        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n",
    "        causal_mask = attention_mask\n",
    "    else:\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        causal_mask = torch.full((sequence_length, target_length), fill_value = min_dtype, dtype=dtype, device=device)\n",
    "        if sequence_length != 1:\n",
    "            causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "            mask_length = attention_mask.shape[-1]\n",
    "            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "            padding_mask = padding_mask == 0\n",
    "            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                padding_mask, min_dtype\n",
    "            )\n",
    "    return causal_mask\n",
    "\n",
    "# Load balancing loss, copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "def load_balancing_loss_func(gate_logits, num_experts, top_k, attention_mask):\n",
    "    compute_device = gate_logits[0].device\n",
    "    concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim = 0)\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "    else:\n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (attention_mask[None, :, :, None, None].expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts)).reshape(-1, top_k, num_experts).to(compute_device))\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(expert_attention_mask, dim=0)\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (attention_mask[None, :, :, None].expand((num_hidden_layers, batch_size, sequence_length, num_experts)).reshape(-1, num_experts).to(compute_device))\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(router_per_expert_attention_mask, dim=0)\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "First let's define the RMSNorm, ROPE, and self-attention layers.\n",
    "- These are basically taken straight from the OlMoE source code, but heavily simplified/cleaned up.\n",
    "- Note that RMSNorm is the ONLY norm type we define (same as OlMoE).\n",
    "- These layers generally do not need to be modified for MoE experiments.\n",
    "\"\"\"\n",
    "from transformers.modeling_flash_attention_utils import _flash_attention_forward # Flash attention forward\n",
    "\n",
    "class OlmoeRMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply RMS Norm\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L137-L154\n",
    "    - This is the only norm used in OlMoE!\n",
    "      - It's used 4 times per layer (attention key norm, attention query norm, layer residual pre-attention norm, post-attention norm)\n",
    "      - Also one additional time before the final LM head \n",
    "    \"\"\"\n",
    "    def __init__(self, D, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(D))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim = True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class OlmoeRotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Get sin/cos ROPE embeddings\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L161-L219\n",
    "    - Code has been simplified heavily since we're not using dynamic ROPE scaling\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf):\n",
    "        super().__init__()\n",
    "        dim = int(conf.D/conf.H)\n",
    "        inv_freq = 1.0 / (conf.rope_theta ** (torch.arange(0, dim, 2, dtype = torch.int64).float()/dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent = False)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type = device_type, enabled = False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim = -1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype = x.dtype), sin.to(dtype = x.dtype)\n",
    "\n",
    "class OlmoeAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention implementation\n",
    "    - Modified from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L288-L391\n",
    "    - Simplfied to handle base attention/sdpa/flash attention within this one class\n",
    "    - Also doesn't support GQA (OlMoE doesn't use anyways)\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf):\n",
    "        super().__init__()\n",
    "        self.attn_method = conf.attn_method\n",
    "        self.D = conf.D # Hidden state dim\n",
    "        self.H = conf.H # Num of attention heads\n",
    "        self.Dh = int(conf.D/conf.H) # Dimensions per head\n",
    "        \n",
    "        # Initialize attention layers - no biases following OlMoE architecture\n",
    "        self.q_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.k_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.v_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.o_proj = nn.Linear(self.D, self.D, bias = False)\n",
    "        self.q_norm = OlmoeRMSNorm(self.D, eps = conf.rms_norm_eps)\n",
    "        self.k_norm = OlmoeRMSNorm(self.D, eps = conf.rms_norm_eps)\n",
    "\n",
    "    # Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L223-L255\n",
    "    def apply_rotary_pos_emb(self, q, k, cos, sin, unsqueeze_dim = 1):\n",
    "        def rotate_half(x):\n",
    "            \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "            x1 = x[..., : x.shape[-1] // 2]\n",
    "            x2 = x[..., x.shape[-1] // 2 :]\n",
    "            return torch.cat((-x2, x1), dim=-1)\n",
    "            \n",
    "        cos = cos.unsqueeze(unsqueeze_dim)\n",
    "        sin = sin.unsqueeze(unsqueeze_dim)\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings: tuple[torch.Tensor, torch.Tensor]):\n",
    "        \n",
    "        B, N , D = hidden_state.shape\n",
    "\n",
    "        query_state = self.q_norm(self.q_proj(hidden_state)).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "        key_state = self.k_norm(self.k_proj(hidden_state)).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "        value_state = self.v_proj(hidden_state).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_state, key_state = self.apply_rotary_pos_emb(query_state, key_state, cos, sin)\n",
    "        \n",
    "        if self.attn_method == 'normal':\n",
    "            attn_weights = torch.matmul(query_state, key_state.transpose(2, 3))/math.sqrt(self.Dh)  # Should be shape B x H x N x N\n",
    "            attn_weights = attn_weights + attention_mask # Attention mask is upper triangular of negative infinity\n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(query_state.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_state) # B x H x N x D/H\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "            attn_output = attn_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "            \n",
    "        elif self.attn_method == 'sdpa':\n",
    "            attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "                query_state, key_state, value_state,\n",
    "                attention_mask, dropout_p = 0.0, is_causal = True\n",
    "            )\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            attn_output = attn_output.view(B, N, D)\n",
    "            \n",
    "        elif self.attn_method == 'fa2':\n",
    "            query_state = query_state.transpose(1, 2)\n",
    "            key_state = key_state.transpose(1, 2)\n",
    "            value_state = value_state.transpose(1, 2)\n",
    "            attn_output = _flash_attention_forward(\n",
    "                query_state, key_state, value_state,\n",
    "                attention_mask, N, dropout = 0.0, use_top_left_mask = False, is_causal = True\n",
    "            )\n",
    "            attn_output = attn_output.reshape(B, N, D).contiguous()\n",
    "            \n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Now let's define the MLP layer and the MoE layer.\n",
    "- The MLP layer is simple; modify as needed.\n",
    "- However, the MoE layer is much more complex, and this layer will probably need to be modified heavily for most experiments.\n",
    "  - By default, I've defined three forward methods here. As currently implemented, they all generate IDENTICAL outputs but become increasingly more efficient yet complex.\n",
    "    - `forward_slow` is the most straightforward implementation (similar to the original OlMoE code). It is also the fastest for single-GPU, limited experts (32 or less) operations.\n",
    "    - `forward_fast` is faster for large # experts, as it places all the relevant states for a single expert to be continguous in memory. For single GPU, it reaches parity w/forward_slow at ~64 experts.\n",
    "    - `forward_async` is faster for large GPU counts + large # of experts, as it batches all experts who belong on one device together, and also runs them all asynchronously.\n",
    "    - For initial testing, it's probably best to modify just `forward_slow`, and only modify the others once you want to run a large-scale training run.\n",
    "  - Each forward method must return a tuple where the first element is the B x N x D MoE layer output, and the second element is the router logits. \n",
    "    - To return more, you'll need to also modify the transformer layer class in the next section.\n",
    "\"\"\"\n",
    "from transformers.activations import silu\n",
    "\n",
    "class OlmoeMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Individual expert MLP\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L258-L272\n",
    "    - Added a reduction factor to reduce the intermediate dimension of the MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf, reduction_factor: int = 1):\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "        self.D = conf.D\n",
    "        self.I = max(1,int(conf.I / reduction_factor))\n",
    "        self.gate_proj = nn.Linear(self.D, self.I, bias = False)\n",
    "        self.up_proj = nn.Linear(self.D, self.I, bias = False)\n",
    "        self.down_proj = nn.Linear(self.I, self.D, bias = False)\n",
    "        self.act_fn = silu\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we consider a complete tree, the indices for parent and children can be computed by the index of the node\n",
    "class OlmoeMoe(nn.Module):\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        super(OlmoeMoe, self).__init__()\n",
    " \n",
    "    \n",
    "        self.n_branches_tree = conf.n_branches_tree\n",
    "        self.n_layers_tree = conf.n_layers_tree\n",
    "        \n",
    "        self.n_routers = int((self.n_branches_tree ** self.n_layers_tree -1) // (self.n_branches_tree - 1))\n",
    "        self.n_experts = int(self.n_branches_tree*(self.n_branches_tree ** self.n_layers_tree -1) // (self.n_branches_tree - 1))\n",
    "\n",
    "        self.experts = nn.ModuleList([OlmoeMLP(conf, reduction_factor = self.get_expert_reduction_factor(expert_idx) ) for expert_idx in range(self.n_experts)])\n",
    "        \n",
    "        self.routers = nn.Linear(conf.D , self.n_branches_tree * self.n_routers, bias=False) # this is a flattened version of all routers\n",
    "        \n",
    "        self.n_all_paths = self.n_branches_tree ** self.n_layers_tree\n",
    "        \n",
    "        self.node_paths = node_paths\n",
    "        self.branch_paths = branch_paths\n",
    "        self.top_k = conf.top_k\n",
    "    \n",
    "        self.shared_expert = OlmoeMLP(conf)\n",
    "        \n",
    "        self.to(main_device)\n",
    "    \n",
    "    def get_expert_reduction_factor(self, index):\n",
    "\n",
    "        if self.n_branches_tree == 1:\n",
    "            return 1\n",
    "\n",
    "        value = 1 + ((self.n_branches_tree - 1) * index) / self.n_branches_tree\n",
    "        layer = math.floor(math.log(value, self.n_branches_tree)) + 1\n",
    "        reduction_factor = self.n_branches_tree ** layer\n",
    "        return reduction_factor\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, node_paths, branch_paths, expert_indices) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "            \"\"\"\n",
    "            A vectorized MoE forward pass.\n",
    "\n",
    "            Input:\n",
    "            hidden_state: Tensor of shape (B, N, D) representing token representations.\n",
    "            Other inputs: node_paths, branch_paths, expert_indices\n",
    "            Returns:\n",
    "            aggregated_out: Tensor of shape (B, N, d_out) – the combined output from the experts.\n",
    "            router_logits : Tensor of shape (BN, n_experts) – the raw gating scores.\n",
    "            token_expert_weights: Tensor of shape (BN, n_experts) – the weights of the experts for each token.\n",
    "            \"\"\"\n",
    "            B, N, D = hidden_state.shape\n",
    "            # Flatten B x N tokens into BN tokens.\n",
    "            hidden_state_flat = hidden_state.view(B*N, D)  # shape: (BN, D)\n",
    "\n",
    "            \n",
    "            # 1) Compute routing (gate) logits and probabilities.\n",
    "            router_logits = self.routers(hidden_state_flat).reshape(B*N,  self.n_routers, self.n_branches_tree) # (BN, self.n_routers, self.n_branches_tree)\n",
    "            routing_weights = F.softmax(router_logits, dim=-1) # (BN, self.n_routers, self.n_branches_tree)\n",
    "            routing_weights = routing_weights[:, node_paths, branch_paths] # (BN, self.n_all_paths, n_layers_tree)\n",
    "            routing_weights_finalprob = routing_weights.prod(dim=-1) # (BN, self.n_all_paths)\n",
    "            routing_weights_probs = routing_weights.cumprod(dim=-1) # (BN, self.n_all_paths, n_layers_tree)\n",
    "            _, topk_indices = torch.topk(routing_weights_finalprob, self.top_k, dim=-1)\n",
    "            selected_experts = expert_indices[topk_indices] # (BN, self.top_k, self.n_layers_tree)\n",
    "            routing_weights_probs = torch.gather(input=routing_weights_probs, dim=1, index=topk_indices.unsqueeze(-1).expand(-1, -1, self.n_layers_tree))\n",
    "            \n",
    "            one_hot = F.one_hot(selected_experts, num_classes=self.n_experts).to(hidden_state_flat.device)\n",
    "            weighted_one_hot = one_hot * routing_weights_probs.unsqueeze(-1) # rel_weights.unsqueeze(-1) has shape (BN, self.n_all_paths, self.n_layers_tree, 1)\n",
    "            token_expert_weights = weighted_one_hot.sum(dim=(1, 2)) # (BN, self.n_experts)\n",
    "            token_expert_weights = token_expert_weights.to(hidden_state_flat.dtype)\n",
    "            # 2) self.use_lflb and self.norm_topk_prob are not used in this implementation\n",
    "            # 3) ---------------- Dispatch tokens to experts and accumulate outputs ----------------\n",
    "    \n",
    "            mlp_output = torch.zeros((B * N, D), dtype=hidden_state.dtype, device=hidden_state.device)\n",
    "            \n",
    "            # For each expert, fetch all the tokens that are activated (i.e. weight > 0)\n",
    "            for expert_ix, expert in enumerate(self.experts):\n",
    "                # token_expert_weights[:, expert_ix] is (BN,)\n",
    "                token_mask = token_expert_weights[:, expert_ix] > 0\n",
    "                if token_mask.sum() == 0:\n",
    "                    continue  # No tokens routed to this expert.\n",
    "                token_indices = token_mask.nonzero(as_tuple=True)[0]  # indices of activated tokens.\n",
    "                \n",
    "                # Gather input tokens for this expert.\n",
    "                tokens_for_expert = hidden_state_flat[token_indices, :]  # (num_tokens, D)\n",
    "                \n",
    "                # Move tokens to the expert's device.\n",
    "                expert_device = next(expert.parameters()).device\n",
    "                tokens_for_expert = tokens_for_expert.to(expert_device)\n",
    "                \n",
    "                # Forward through expert.\n",
    "                expert_output = expert(tokens_for_expert)  # (num_tokens, D)\n",
    "                \n",
    "                # Multiply each expert output by its corresponding routing weight.\n",
    "                weights = token_expert_weights[token_indices, expert_ix].unsqueeze(1).to(expert_device)  # (num_tokens, 1)\n",
    "                expert_output = expert_output * weights\n",
    "                \n",
    "                # Move back to the original device and accumulate into mlp_output.\n",
    "                expert_output = expert_output.to(mlp_output.device)\n",
    "                mlp_output.index_add_(0, token_indices, expert_output.to(hidden_state.dtype))\n",
    "            \n",
    "            # Reshape aggregated output back to (B, N, D)\n",
    "            aggregated_out = mlp_output.view(B, N, D)\n",
    "            \n",
    "            # 4 ) Add the shared expert output\n",
    "            shared_output = self.shared_expert(hidden_state)\n",
    "            aggregated_out = aggregated_out + shared_output\n",
    "            \n",
    "            return aggregated_out, router_logits, token_expert_weights\n",
    "            \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated output shape: torch.Size([2, 10, 768])\n",
      "Router logits shape: torch.Size([20, 40, 3])\n",
      "Expert weights shape: torch.Size([20, 120])\n"
     ]
    }
   ],
   "source": [
    "hmoe = OlmoeMoe(conf)\n",
    "\n",
    "# Create a dummy hidden_state tensor with shape (B, N, D).\n",
    "B = 2    # batch size\n",
    "N = 10   # number of tokens per batch\n",
    "hidden_state = torch.randn(B, N, conf.D).to(main_device)\n",
    "\n",
    "# Run the forward pass.\n",
    "with torch.no_grad():\n",
    "    aggregated_out, router_logits, token_expert_weights = hmoe(hidden_state, node_paths, branch_paths, expert_indices)\n",
    "\n",
    "# Print out the results.\n",
    "print(\"Aggregated output shape:\", aggregated_out.shape)  # Expected: (B, N, D)\n",
    "print(\"Router logits shape:\", router_logits.shape)       # Expected: (B*N, n_routers, n_branches_tree)\n",
    "print(\"Expert weights shape:\", token_expert_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Now let's define the transformer block.\n",
    "- Most likely, there is nothing to change here, unless you need to change the input/outputs from the MoE layer.\n",
    "- Note that this forward pass is nested within a `custom_forward` call in order to support gradient checkpointing.\n",
    "\"\"\"\n",
    "\n",
    "class OlmoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer layer\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.D = conf.D\n",
    "        self.self_attn = OlmoeAttention(conf = conf)\n",
    "        # self.self_attn = torch.compile(self.self_attn) # attn layers can be compiled for speed \n",
    "        self.moe = OlmoeMoe(conf)\n",
    "        self.input_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "        self.post_attention_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor]\n",
    "        ):\n",
    "\n",
    "        def custom_forward(hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings):\n",
    "\n",
    "            ### Pre-SA Residual Stream + Norm ###\n",
    "            residual = hidden_state\n",
    "            hidden_state = self.input_layernorm(hidden_state)\n",
    "            \n",
    "            ### SA + Sum to Residual Stream ###\n",
    "            attn_output = self.self_attn(\n",
    "                hidden_state,\n",
    "                attention_mask = attention_mask,\n",
    "                position_ids = position_ids,\n",
    "                position_embeddings = position_embeddings\n",
    "            )\n",
    "            hidden_state = residual + attn_output\n",
    "\n",
    "            ### Pre-MLP Residual Stream + Norm ###\n",
    "            residual = hidden_state\n",
    "            hidden_state = self.post_attention_layernorm(hidden_state)\n",
    "            \n",
    "            ### MLP + Sum to Residual Stream###\n",
    "            mlp_output, router_logits, topk_experts = self.moe(hidden_state, node_paths, branch_paths, expert_indices)\n",
    "            hidden_state = residual + mlp_output\n",
    "            \n",
    "            return hidden_state, router_logits, topk_experts\n",
    "        \n",
    "            \n",
    "        hidden_state, router_logits, topk_experts = custom_forward(\n",
    "            hidden_state, attention_mask, position_ids, position_embeddings\n",
    "        )\n",
    "\n",
    "            \n",
    "        return hidden_state, router_logits, topk_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Now define the top-level model.\n",
    "- This class is initialized with the `ModelConf` config settings as well as a list of expert-device mappings (leave blank for single-GPU tests).\n",
    "- After initialization, it creates all child layers and moves the experts to their correct devices. \n",
    "  - All other parameters will continue to exist on the default device.\n",
    "- Modify `_init_weights` to change the weight initialization scheme.\n",
    "- The forward pass calls the children layers and also calculates the loss (standard cross-entropy + aux loss). \n",
    "\"\"\"\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "\n",
    "class OlmoeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The top level model object. Also handles weight initialization and loss calculations.\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf, primary_device: str, expert_device_map: None|list[str] = None):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            @conf: A configuration object of class ModelConf.\n",
    "            @primary_device: A device for which to store the dense layers and shared experts on.\n",
    "            @expert_device_map: A list of devices to store experts on. If `None`, stores them all on whatever the torch default device is.\n",
    "              For example, `expert_device_map = ['cuda:0', 'cuda:1', 'cuda:1', 'cuda:2']` means to store expert 0 on cuda:0, experts 1-2 on the device cuda:1, and expert 3 on cuda:2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "        \n",
    "        ### Layers ###\n",
    "        self.embed_tokens = nn.Embedding(self.conf.vocab_size, self.conf.D, self.conf.padding_idx)\n",
    "        self.rotary_emb = OlmoeRotaryEmbedding(conf = self.conf)\n",
    "        self.layers = nn.ModuleList([OlmoeBlock(self.conf, layer_idx) for layer_idx in range(self.conf.n_layers)])\n",
    "        self.norm = OlmoeRMSNorm(self.conf.D, eps = self.conf.rms_norm_eps)\n",
    "        self.lm_head = nn.Linear(self.conf.D, self.conf.vocab_size, bias = False)\n",
    "        \n",
    "        ### Initialize weights ###\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        ### Model ###\n",
    "        self.to(primary_device)\n",
    "\n",
    "        ### Experts ###\n",
    "        if expert_device_map is not None:\n",
    "            self._move_experts_to_devices(expert_device_map)\n",
    "\n",
    "    # OlMoE weight initiation - see https://github.com/huggingface/transformers/blob/8f1509a96c96747c893051ac947795cfb0750357/src/transformers/modeling_utils.py#L2500-L2515\n",
    "    # Normal distribution for linear layers + embeddings\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "            # In the vocab -> embedding layer, set all embeddings to 0 for the padding token (tokenizer.pad_token_id)\n",
    "            if module is self.embed_tokens:\n",
    "                self.embed_tokens.weight.data[self.conf.padding_idx].zero_() \n",
    "        # Seems to use default weight initialization for other layers\n",
    "            # Move all parameters and buffers to the specified dtype\n",
    "            \n",
    "    def _move_experts_to_devices(self, expert_device_map: list[str]):\n",
    "        \"\"\"\n",
    "        Move each expert in each layer's MoE to the specified device.\n",
    "        \"\"\"\n",
    "        # Require that the length of expert_device_map equal the length of conf.n_experts.\n",
    "        n_experts = int(self.conf.n_branches_tree*(self.conf.n_branches_tree ** self.conf.n_layers_tree -1) // (self.conf.n_branches_tree - 1))\n",
    "        if len(expert_device_map) != n_experts:\n",
    "            raise ValueError(f\"expert_device_map has length {len(expert_device_map)} but n_experts = {n_experts}.\")\n",
    "            \n",
    "        for _, layer in enumerate(self.layers):\n",
    "            moe_block = layer.moe \n",
    "            for ex_idx, expert in enumerate(moe_block.experts):\n",
    "                target_dev = expert_device_map[ex_idx]\n",
    "                expert.to(target_dev)\n",
    "            \n",
    "    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.Tensor, moe_method: str, use_lflb: bool = False, use_checkpointing : bool = False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            @input_ids: A tensor of input IDs of size B x N, where B is the batch size and N is the sequence length.\n",
    "            @attention_mask: An attention mask tensor of size B x N.\n",
    "            @moe_method: The method to use to calculate the MoE routing. See the `OlmoeMoe` class for details.\n",
    "            @use_lflb: Whether or not to use loss-free balancing.\n",
    "            @use_checkpointing: Whether to use gradient checkpointing. Only set `True` during training.\n",
    "        \"\"\"\n",
    "        hidden_state = self.embed_tokens(input_ids)\n",
    "        B, N, D = hidden_state.shape\n",
    "\n",
    "        ### Prep rotary embeddings + attention masks  ###\n",
    "        cache_position = torch.arange(0, N, device = hidden_state.device)\n",
    "        position_ids = cache_position.unsqueeze(0)\n",
    "        position_embeddings = self.rotary_emb(hidden_state, position_ids) # Position embeddings to be shared across transformer layers\n",
    "\n",
    "        # This is the upper-trangular matrix of infinities to mask future tokens in the attention softmax;\n",
    "        if self.conf.attn_method in ['normal', 'sdpa']:\n",
    "            causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(attention_mask, N, N, hidden_state.dtype, hidden_state.device, cache_position, B)\n",
    "        # The flash attention mask is simpler - takes only the original attention mask or None\n",
    "        elif self.conf.attn_method == 'fa2':\n",
    "            causal_mask  = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "        \n",
    "        ### Transformer layers ###\n",
    "        all_router_logits = () # Save router logits from each layer into this; will be needed for load balancing loss\n",
    "        all_topk_experts = () # Return topk experts\n",
    "        \n",
    "        for _, layer in enumerate(self.layers):\n",
    "            hidden_state, router_logits, topk_experts = layer(\n",
    "                hidden_state,\n",
    "                attention_mask = causal_mask,\n",
    "                position_ids = position_ids,\n",
    "                position_embeddings = position_embeddings\n",
    "            )\n",
    "            all_router_logits += (router_logits, )\n",
    "            all_topk_experts += (topk_experts,)  # Store the topk_experts for each layer\n",
    "\n",
    "        hidden_state = self.norm(hidden_state)\n",
    "        output_logits = self.lm_head(hidden_state)\n",
    "\n",
    "        ##### Calculate Loss #####\n",
    "        # The labels object should be a tensor of token IDs or -100 (for attention mask, since don't want to calculate loss for those)\n",
    "        label_ids = torch.where(input_ids == self.conf.padding_idx, torch.tensor(-100), input_ids)\n",
    "        # Get regular loss\n",
    "        base_loss = ForCausalLMLoss(output_logits, label_ids, self.conf.vocab_size)\n",
    "        # Get load balancing loss\n",
    "        aux_loss = torch.tensor(0.0).to(main_device)\n",
    "        # aux_loss = load_balancing_loss_func(gate_logits = all_router_logits, num_experts = self.conf.n_experts, top_k = self.conf.top_k, attention_mask = attention_mask)\n",
    "\n",
    "        return {\n",
    "            'all_router_logits': all_router_logits,\n",
    "            'all_topk_experts': all_topk_experts,\n",
    "            'logits': output_logits,\n",
    "            'aux_loss': aux_loss,\n",
    "            'base_loss': base_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 679,121,664\n",
      "Device 0: NVIDIA A100 80GB PCIe\n",
      "  Allocated: 1.38 GB\n",
      "  Reserved: 2.68 GB\n",
      "  Total: 79.25 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_LOGS\"] = \"recompiles\"\n",
    "\"\"\" \n",
    "Let's load the model\n",
    "- Set the default_device to specify where all the non-expert layers live (the experts are moved on model init)\n",
    "- Set the default_dtype to specify the model dtype, all params will be in this dtype except for this explicitly specified differently in class definition\n",
    "  - In the default OlMoE, RMSNorm is required to be f32 whereas all other params are bf16. \n",
    "\"\"\"\n",
    "# torch.set_default_device(main_device) # This is buggy, don't use\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "torch.set_float32_matmul_precision('medium') # See https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html \n",
    "torch.manual_seed(seed)\n",
    "\n",
    "n_experts = int(conf.n_branches_tree*(conf.n_branches_tree ** conf.n_layers_tree -1) // (conf.n_branches_tree - 1))\n",
    "\n",
    "model = OlmoeModel(\n",
    "    conf,\n",
    "    primary_device = main_device, # Where to store dense layers and shared experts\n",
    "    expert_device_map = ['cuda:0'] * n_experts # Here let's test them with all of them on cuda:0\n",
    ")\n",
    "model = torch.compile(model)\n",
    "# make the output in model.forward in moe.py to be a tensor of shape (BN, D)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0212 15:25:24.730000 35197 torch/_dynamo/convert_frame.py:844] [26/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W0212 15:25:24.730000 35197 torch/_dynamo/convert_frame.py:844] [26/8]    function: 'forward' (/tmp/ipykernel_35197/750249474.py:31)\n",
      "W0212 15:25:24.730000 35197 torch/_dynamo/convert_frame.py:844] [26/8]    last reason: 26/0: tensor 'L['x']' size mismatch at index 0. expected 252, actual 4\n",
      "W0212 15:25:24.730000 35197 torch/_dynamo/convert_frame.py:844] [26/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0212 15:25:24.730000 35197 torch/_dynamo/convert_frame.py:844] [26/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " audience\n",
      "�\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Let's load a forward pass with a batch size of 2, to make sure the model is able to run\n",
    "- If you have multiple working forward methods, this is a good chance to test them for equality\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False)\n",
    "prompt = ['I am a dog and I like to eat. My favorite food is', 'My cat is']\n",
    "inputs = tokenizer(prompt, truncation = True, max_length = 128, padding = 'max_length', return_tensors = 'pt').to(main_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # output = model(inputs['input_ids'], inputs['attention_mask'], moe_method = 'forward_slow', use_lflb = True, use_checkpointing = False)\n",
    "    # output = model(inputs['input_ids'], inputs['attention_mask'], moe_method = 'forward_fast', use_lflb = True, use_checkpointing = False)\n",
    "    output = model(inputs['input_ids'], inputs['attention_mask'], moe_method = 'forward_async', use_lflb = True, use_checkpointing = False)\n",
    "    \n",
    "\n",
    "output_ids = torch.argmax(output['logits'][:, :, :], dim = 2)\n",
    "for i in range(output_ids.size(0)):\n",
    "    idx = inputs[\"attention_mask\"].sum(dim = -1)[i].item() - 1 # get length of attention mask to find the last non-mask output token ix\n",
    "    print(tokenizer.decode(output_ids[i, idx], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runs': 10, 'average_time': '0.37910819s', 'average_peak_mem': '1532.0788MB', 'average_increase_mem_MB': '2.7723MB'}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(profile_memory(\n",
    "        model,\n",
    "        input_ids = inputs['input_ids'],\n",
    "        attention_mask = inputs['attention_mask'],\n",
    "        moe_method = 'forward_slow',\n",
    "        use_lflb = False,\n",
    "        use_checkpointing = False\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First, load some validation data as a dataloader. This will be needed for evaluation during model training later. \n",
    "- The function `load_shard_as_dataloader` loads JSON data shards, concatenates them with a seperator in between, then splits them by a given seq length.\n",
    "- We do not load the dataloader for training data yet; the total size is too large, so instead these will be loaded later as needed in the training loop.\n",
    "\"\"\" \n",
    "from helpers.dataset import load_shard_as_dataloader\n",
    "\n",
    "val_dl = load_shard_as_dataloader(\n",
    "    './../../data/val_shard.json',\n",
    "    tokenizer,\n",
    "    batch_size = 32,\n",
    "    seq_len = 2048,\n",
    "    eos_seperator_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 10.99523800611496, 'base_loss': 10.99523800611496, 'aux_loss': 0.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Now, define a function for calculating validation metrics. This will be later used in the training loop.\n",
    "- An untrained model should typically return validation loss of ~10 for the base cross-entropy loss.\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def get_val_stats(model, val_dl, router_aux_loss_coef):\n",
    "    \"\"\"\n",
    "    Get eval set metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    val_loss_sum = 0.0\n",
    "    val_base_sum = 0.0\n",
    "    val_aux_sum = 0.0\n",
    "    val_steps = 0\n",
    "\n",
    "    for val_batch in val_dl:\n",
    "        val_input_ids = val_batch['input_ids'].to(main_device)\n",
    "        val_attn_mask = val_batch['attention_mask'].to(main_device)\n",
    "        \n",
    "        test_outputs = model(val_input_ids, val_attn_mask, moe_method = 'forward_slow', use_checkpointing = False)\n",
    "\n",
    "        val_base_sum += test_outputs['base_loss'].detach().cpu().item()\n",
    "        val_aux_sum  += test_outputs['aux_loss'].detach().cpu().item()\n",
    "        val_loss_sum += (test_outputs['base_loss'] + router_aux_loss_coef * test_outputs['aux_loss']).detach().cpu().item()\n",
    "        \n",
    "        val_steps += 1\n",
    "\n",
    "    avg_test_loss = val_loss_sum / val_steps\n",
    "    avg_test_base = val_base_sum / val_steps\n",
    "    avg_test_aux  = val_aux_sum  / val_steps\n",
    "\n",
    "    model.train()\n",
    "    return {\n",
    "        \"loss\": avg_test_loss,\n",
    "        \"base_loss\": avg_test_base,\n",
    "        \"aux_loss\": avg_test_aux\n",
    "    }\n",
    "\n",
    "get_val_stats(model, val_dl, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Set training constants to be used for training later.\n",
    "- The batch size will be equal to micro_batch_size * accumulation_steps.\n",
    "\"\"\"\n",
    "@dataclass\n",
    "class TrainConf:\n",
    "    router_aux_loss_coef: float = 0.005  # Base OlMoE: 0.01 (relative weight of balancing loss)\n",
    "    use_lflb: bool = False # Use loss-free load balancing\n",
    "    bias_update_rate: float = .001 # Bias update rate for lflb\n",
    "    lr: float = 5e-4 * (64 * 8)/(256) * 1.2 # The starting LR (after warmup)\n",
    "    min_lr: float = 5e-5 # The minimum LR\n",
    "    warmup_steps: int = 500 # How long it takes to warmup to the starting LR\n",
    "    decay_steps: int = 19500 # How long it takes to decay from the starting LR to the minimum LR\n",
    "    max_grad_norm: float = 1.0 # Gradient clipping for non-expert grads\n",
    "    max_expert_grad_norm: float = 1.0 # Gradient clipping for expert grads\n",
    "    micro_batch_size: int = 16 # Size of a microbatch\n",
    "    accumulation_steps: int = 8 # Number of microbatches within a batch\n",
    "    seq_len: int = 2048 # The sequence length\n",
    "\n",
    "train_conf = TrainConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuanbo096\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/MoE/experiments/hierarchical-moe/wandb/run-20250212_152614-8j4atm9o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuanbo096/interpretable-moes/runs/8j4atm9o' target=\"_blank\">test-11 -single-gpu -experts-16 -topk-3(+1) -forward-slow -lfbl</a></strong> to <a href='https://wandb.ai/yuanbo096/interpretable-moes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yuanbo096/interpretable-moes' target=\"_blank\">https://wandb.ai/yuanbo096/interpretable-moes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yuanbo096/interpretable-moes/runs/8j4atm9o' target=\"_blank\">https://wandb.ai/yuanbo096/interpretable-moes/runs/8j4atm9o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup a Wandb run for logging. Choose a run name and notes for the run!\n",
    "\"\"\"\n",
    "RUN_NAME = 'test-11 -single-gpu -experts-16 -topk-3(+1) -forward-slow -lfbl'\n",
    "RUN_NOTES = 'Baseline test with 16 experts (3+1), and LFBL. Memory savings with compile + fused AdamW. Slight param change from test-10 (higher peak LR + lower lfbl coef)'\n",
    "\n",
    "load_dotenv('./../../secrets.env')\n",
    "wandb.login(key = os.getenv('WANDB_API_KEY'))\n",
    "run = wandb.init(\n",
    "    project = 'interpretable-moes', \n",
    "    name = RUN_NAME,\n",
    "    notes = RUN_NOTES,\n",
    "    config = {**asdict(conf), **asdict(train_conf)}\n",
    ")\n",
    "\n",
    "# (Optional) Also log various info as a wandb media object.\n",
    "additional_log_notes = {\n",
    "    'run_name': RUN_NAME,\n",
    "    'notes': RUN_NOTES,\n",
    "    'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_model_params': sum(p.numel() for p in model.parameters()),\n",
    "    'available_cuda_gpus': [torch.cuda.get_device_properties(i).name for i in range(torch.cuda.device_count())],\n",
    "    'model_conf': asdict(conf),\n",
    "    'train_conf': asdict(train_conf)\n",
    "}\n",
    "\n",
    "wandb.log({'conf': wandb.Html(f\"<pre style='font-size:12px;'>{json.dumps(additional_log_notes, indent = 2)}</pre>\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 shards.\n",
      "\n",
      "=== Loading shard ./../../data/train_shard_0.json (index 0) ===\n",
      "Step 0: avg_loss=10.9893 | fwd_time=19.66s | bwd_time=13.98s | batch_time = 34.36 | lr=2.4e-04\n",
      "Step 1: avg_loss=10.3227 | fwd_time=12.30s | bwd_time=13.57s | batch_time = 26.31 | lr=2.4e-04\n",
      "Step 2: avg_loss=9.7379 | fwd_time=10.50s | bwd_time=12.02s | batch_time = 22.94 | lr=2.5e-04\n",
      "Step 3: avg_loss=9.4710 | fwd_time=9.50s | bwd_time=10.71s | batch_time = 20.66 | lr=2.5e-04\n",
      "Step 4: avg_loss=9.2875 | fwd_time=9.35s | bwd_time=10.07s | batch_time = 19.82 | lr=2.5e-04\n",
      "Step 5: avg_loss=9.1368 | fwd_time=8.67s | bwd_time=9.70s | batch_time = 18.75 | lr=2.5e-04\n",
      "Step 6: avg_loss=8.9381 | fwd_time=8.44s | bwd_time=9.42s | batch_time = 18.24 | lr=2.5e-04\n",
      "Step 7: avg_loss=8.7441 | fwd_time=8.37s | bwd_time=9.33s | batch_time = 18.07 | lr=2.6e-04\n",
      "Step 8: avg_loss=8.6018 | fwd_time=8.11s | bwd_time=8.98s | batch_time = 17.44 | lr=2.6e-04\n",
      "Step 9: avg_loss=8.4448 | fwd_time=8.08s | bwd_time=8.97s | batch_time = 17.41 | lr=2.6e-04\n",
      "Step 10: avg_loss=8.2952 | fwd_time=7.96s | bwd_time=8.84s | batch_time = 17.16 | lr=2.6e-04\n",
      "Step 20: avg_loss=7.5725 | fwd_time=7.97s | bwd_time=8.96s | batch_time = 17.29 | lr=2.8e-04\n",
      "Step 30: avg_loss=7.3235 | fwd_time=8.51s | bwd_time=9.29s | batch_time = 18.16 | lr=3.0e-04\n",
      "Step 40: avg_loss=7.1470 | fwd_time=9.01s | bwd_time=10.08s | batch_time = 19.46 | lr=3.2e-04\n",
      "Step 50: avg_loss=6.9297 | fwd_time=9.79s | bwd_time=11.02s | batch_time = 21.20 | lr=3.4e-04\n",
      "Step 60: avg_loss=6.7014 | fwd_time=9.65s | bwd_time=11.29s | batch_time = 21.33 | lr=3.6e-04\n",
      "Step 70: avg_loss=6.5643 | fwd_time=9.96s | bwd_time=11.66s | batch_time = 22.01 | lr=3.8e-04\n",
      "Step 80: avg_loss=6.4854 | fwd_time=10.79s | bwd_time=12.25s | batch_time = 23.44 | lr=4.0e-04\n",
      "Step 90: avg_loss=6.4021 | fwd_time=10.62s | bwd_time=12.53s | batch_time = 23.55 | lr=4.1e-04\n",
      "Step 100: avg_loss=6.3489 | fwd_time=10.82s | bwd_time=12.77s | batch_time = 24.01 | lr=4.3e-04\n",
      "Skipping leftover batch, need at least 128\n",
      "\n",
      "=== Loading shard ./../../data/train_shard_1.json (index 1) ===\n",
      "Step 200: avg_loss=5.6382 | fwd_time=11.74s | bwd_time=13.84s | batch_time = 26.02 | lr=6.3e-04\n",
      "Step 300: avg_loss=5.0192 | fwd_time=12.41s | bwd_time=14.25s | batch_time = 27.10 | lr=8.2e-04\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's train the model.\n",
    "- The training loop will loop through training data shards. Each shard will be loaded and concatenated into chunks of size seq_len.\n",
    "- Things to consider implementing in the future: more aggressive router LR decay (to encourage router stability)\n",
    "\"\"\"\n",
    "# Initialize optimizer/scheduler. The scheduler combines a warmup + cosine annealing.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = train_conf.lr, fused = True)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers = [\n",
    "        torch.optim.lr_scheduler.LinearLR(optimizer, start_factor = 0.2, end_factor = 1.0, total_iters = train_conf.warmup_steps), # Warmup\n",
    "        torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_conf.decay_steps, eta_min = train_conf.min_lr, last_epoch = -1) # Cosine annealing\n",
    "    ],\n",
    "    milestones = [train_conf.warmup_steps]\n",
    ")\n",
    "\n",
    "# Look for all training data files\n",
    "shard_files = sorted(glob.glob(\"./../../data/train_shard_*.json\"))\n",
    "print(f\"Found {len(shard_files)} shards.\")\n",
    "\n",
    "# Initialize step count\n",
    "step = 0\n",
    "total_tokens_trained = 0\n",
    "model.train()\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for shard_idx, shard_path in enumerate(shard_files):\n",
    "\n",
    "    print(f\"\\n=== Loading shard {shard_path} (index {shard_idx}) ===\")\n",
    "    shard_dl = load_shard_as_dataloader(shard_path, tokenizer, batch_size = train_conf.micro_batch_size * train_conf.accumulation_steps, seq_len = train_conf.seq_len, eos_seperator_id = tokenizer.eos_token_id)\n",
    "\n",
    "    for batch_idx, batch in enumerate(shard_dl):\n",
    "\n",
    "        # ====================== SPLIT BATCH INTO MICRO-BATCHES ======================\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "\n",
    "        if input_ids.size(0) < (train_conf.accumulation_steps * train_conf.micro_batch_size):\n",
    "            print(f\"Skipping leftover batch, need at least {train_conf.accumulation_steps * train_conf.micro_batch_size}\")\n",
    "            continue\n",
    "\n",
    "        sub_input_ids = input_ids.split(train_conf.micro_batch_size, dim = 0) \n",
    "        sub_attn_mask = attention_mask.split(train_conf.micro_batch_size, dim = 0)\n",
    "\n",
    "        # ====================== ZERO GRAD ONCE PER \"BIG BATCH\" ======================\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # We'll track times and losses across micro-batches\n",
    "        total_fwd_time = 0.0\n",
    "        total_bwd_time = 0.0\n",
    "        total_loss_val = 0.0\n",
    "        start_batch = time.time()\n",
    "\n",
    "        # We'll keep a list of dictionaries, one per layer, each mapping expert_id -> usage_count\n",
    "        usage_accum = [defaultdict(int) for _ in range(model.conf.n_layers)]\n",
    "\n",
    "        # ====================== MICRO-BATCH LOOP ======================\n",
    "        for i in range(train_conf.accumulation_steps):\n",
    "\n",
    "            mb_input_ids = sub_input_ids[i]\n",
    "            mb_attn_mask = sub_attn_mask[i]\n",
    "\n",
    "            # ---------------------- Forward ----------------------\n",
    "            start_fwd = time.time()\n",
    "            outputs = model(mb_input_ids, mb_attn_mask, moe_method = 'forward_slow', use_lflb = train_conf.use_lflb, use_checkpointing = True)\n",
    "            loss = outputs['base_loss'] + train_conf.router_aux_loss_coef * outputs['aux_loss']\n",
    "            fwd_time = time.time() - start_fwd\n",
    "            total_fwd_time += fwd_time\n",
    "\n",
    "            # ---------------------- Collect Expert Usage for This Micro-Batch ----------------------\n",
    "            # with torch.no_grad():\n",
    "            #     all_topk_experts = outputs['all_topk_experts']\n",
    "            #     for layer_idx, topk_expert_tensor in enumerate(outputs[\"all_topk_experts\"]):\n",
    "            #         flat_experts = topk_expert_tensor.view(-1)  \n",
    "            #         unique_experts = flat_experts.unique()\n",
    "            #         for ex_id in unique_experts:\n",
    "            #             ex_count = (flat_experts == ex_id).sum().item()\n",
    "            #             usage_accum[layer_idx][int(ex_id)] += ex_count\n",
    "                        \n",
    "            # ---------------------- Backward ----------------------\n",
    "            # Divide by accumulation_steps so total gradient matches \"big batch\" size\n",
    "            scaled_loss = loss / train_conf.accumulation_steps\n",
    "            start_bwd = time.time()\n",
    "            scaled_loss.backward()\n",
    "            bwd_time = time.time() - start_bwd\n",
    "            total_bwd_time += bwd_time\n",
    "\n",
    "            total_loss_val += loss.item()\n",
    "\n",
    "        # ====================== GRAD CLIPPING & OPT STEP ======================\n",
    "        shared_params = [p for n,p in model.named_parameters() if 'expert' not in n]\n",
    "        expert_params = [p for n,p in model.named_parameters() if 'expert' in n]\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(shared_params, train_conf.max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(expert_params, train_conf.max_expert_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # ====================== LOSS-FREE BIAS UPDATE ======================\n",
    "        # We'll do sign-based bias updates after each \"big batch\"\n",
    "        if train_conf.use_lflb:\n",
    "            for layer_ix in range(model.conf.n_layers):\n",
    "                model.layers[layer_ix].moe.update_expert_biases(usage_accum[layer_ix], train_conf.bias_update_rate)\n",
    "\n",
    "        # ============== METRICS ==============\n",
    "        avg_loss = total_loss_val / train_conf.accumulation_steps # Take the average loss over micro-batches. total_loss_val is the sum of 'loss.item()'.\n",
    "        total_tokens_trained += attention_mask.sum().detach().cpu().item()\n",
    "        metrics = {\n",
    "            'step': step,\n",
    "            'shard_idx': shard_idx,\n",
    "            'batch_size': input_ids.shape[0],\n",
    "            'total_tokens_trained': total_tokens_trained,\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'aux_coef': train_conf.router_aux_loss_coef,\n",
    "            'train': {\n",
    "                'loss': avg_loss,\n",
    "                'base_loss': outputs['base_loss'].detach().cpu().item(), # From last microbatch only\n",
    "                'aux_loss':  outputs['aux_loss'].detach().cpu().item() # From last microbatch only\n",
    "            },\n",
    "            'fwd_time':  total_fwd_time,\n",
    "            'bwd_time':  total_bwd_time,\n",
    "            'batch_time':  time.time() - start_batch\n",
    "        }\n",
    "\n",
    "        # ============== EXPENSIVE METRICS (EVERY 10 STEPS) ==============\n",
    "        if step % 10 == 0:\n",
    "            \n",
    "            metrics['gradients'] = get_gradient_stats(model)\n",
    "\n",
    "            # Convert usage_accum (list of defaultdicts) into a more standard dict for logging\n",
    "            usage_dict_final = {}\n",
    "            for layer_idx, ex_dict in enumerate(usage_accum):\n",
    "                usage_dict_final[layer_idx] = dict(ex_dict)  # convert defaultdict -> normal dict\n",
    "            metrics['expert_usage'] = usage_dict_final\n",
    "\n",
    "        # ============== EXTRA EXPENSIVE METRICS (EVERY 500 STEPS) ==============\n",
    "        if step % 250 == 0:\n",
    "            metrics['val'] = get_val_stats(model, val_dl, train_conf.router_aux_loss_coef)\n",
    "\n",
    "        # ============== SAVE (EVERY 5000 STEPS) ==============\n",
    "        if step % 2500 == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model_state_dict': model.state_dict(), \n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'total_tokens_trained': total_tokens_trained,\n",
    "                    'step': step\n",
    "                },\n",
    "                f\"saves/checkpoint_{step:08d}.pt\"\n",
    "            )\n",
    "\n",
    "        # ============== LOG TO W&B ==============\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # ============== PRINT ==============\n",
    "        if step <= 10 or (step <= 100 and step % 10 == 0) or (step > 100 and step % 100 == 0):\n",
    "            print(f\"Step {step}: avg_loss={metrics['train']['loss']:.4f} \"\n",
    "                f\"| fwd_time={metrics['fwd_time']:.2f}s | bwd_time={metrics['bwd_time']:.2f}s | batch_time = {metrics['batch_time']:.2f} \"\n",
    "                f\"| lr={metrics['lr']:.1e}\"\n",
    "            )\n",
    "            \n",
    "        step += 1\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Final save\n",
    "\"\"\" \n",
    "from helpers.memory import clear_all_cuda_memory\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        'model_state_dict': model.state_dict(), \n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'total_tokens_trained': total_tokens_trained,\n",
    "        'step': step,\n",
    "    },\n",
    "    f\"saves/checkpoint_{step:08d}.pt\"\n",
    "    )\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qualitative test\n",
    "\"\"\"\n",
    "prompt = 'My dog likes to eat '\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to(main_device)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False)\n",
    "\n",
    "# Iteratively generate tokens\n",
    "with torch.no_grad():\n",
    "    for _ in range(255):\n",
    "        output = model(input_ids, attention_mask, moe_method = 'forward_slow', use_checkpointing = False)['logits']\n",
    "\n",
    "        next_token_id = torch.argmax(output[0, -1, :], dim = -1).unsqueeze(0)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim = 1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones((1, 1), dtype = torch.long, device = input_ids.device)], dim = 1)\n",
    "\n",
    "        if next_token_id.item() in [tokenizer.eos_token_id, tokenizer.encode('\\n')[0]]:\n",
    "            break\n",
    "\n",
    "# Decode final sequence\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens = False)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
