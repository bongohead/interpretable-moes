{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "import math\n",
    "from accelerate import Accelerator\n",
    "from helpers.memory import check_memory\n",
    "from dataclasses import dataclass\n",
    "\n",
    "load_dotenv('secrets.env')\n",
    "main_device = 'cuda:0'\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a model class that can be used for training.\n",
    "- The model class does not have to handle sharding/expert movements or anything like that, later a wrapper will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MoeConf:\n",
    "    \"\"\"\n",
    "    General config settings for this MoE\n",
    "    \"\"\"\n",
    "    vocab_size: int = 50304 # Base OlMoE: 50304 (vocab size)\n",
    "    D: int = 2048 # Base OlMoE: 2048 (hidden state dimension)\n",
    "    H: int = 16 # Base OlMoE: 16 (number of attention heads)\n",
    "    I: int = 1024 # Base OlMoE: 1024 (expert MLP dimension)\n",
    "    n_experts: int = 64 # Base OlMoE: 64\n",
    "    top_k: int = 8 # Base OlMoE: 8 \n",
    "    norm_topk_prob: bool = False # Base OlMoE: false (whether to normalize so that expert weights sum to 1 after topk)\n",
    "    padding_idx: int = 1 # Base OlMoE: 1 (index where padding gets mapped to)\n",
    "    n_layers: int = 16 # Base OlMoE: 16 (transformer layers)\n",
    "    rms_norm_eps: float = 1e-05 # Base OlMoE: 1e-05\n",
    "    rope_theta: float = 10000.0 # Base OlMoe: 10000.0 (this is something needed for ROPE)\n",
    "    max_position_embeddings: int = 4096 # Base OlMoE: 4096 (this is something needed for ROPE)\n",
    "    router_aux_loss_coef: float = 0.01  # Base OlMoE: 0.01 (relative weight of balancing loss)\n",
    "    attn_method: str = 'fa2' # In OlMoE this is chosen automatically, here we explicitly pass it - choose 'normal', 'sdpa', or 'fa2'\n",
    "    torch_dtype: torch.dtype = torch.bfloat16\n",
    "\n",
    "conf = MoeConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions that will be later needed by the class\n",
    "\n",
    "# Create the upper-trangular matrix of infinities to mask future tokens in the attention softmax;\n",
    "def _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "    attention_mask: torch.Tensor,\n",
    "    sequence_length: int,\n",
    "    target_length: int,\n",
    "    dtype: torch.dtype,\n",
    "    device: torch.device,\n",
    "    cache_position: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    if attention_mask is not None and attention_mask.dim() == 4:\n",
    "        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n",
    "        causal_mask = attention_mask\n",
    "    else:\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        causal_mask = torch.full(\n",
    "            (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n",
    "        )\n",
    "        if sequence_length != 1:\n",
    "            causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "            mask_length = attention_mask.shape[-1]\n",
    "            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "            padding_mask = padding_mask == 0\n",
    "            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                padding_mask, min_dtype\n",
    "            )\n",
    "\n",
    "    return causal_mask\n",
    "\n",
    "# Load balancing loss, copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "def load_balancing_loss_func(gate_logits, num_experts, top_k, attention_mask):\n",
    "    compute_device = gate_logits[0].device\n",
    "    concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim = 0)\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "    else:\n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (attention_mask[None, :, :, None, None].expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts)).reshape(-1, top_k, num_experts).to(compute_device))\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(expert_attention_mask, dim=0)\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (attention_mask[None, :, :, None].expand((num_hidden_layers, batch_size, sequence_length, num_experts)).reshape(-1, num_experts).to(compute_device))\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(router_per_expert_attention_mask, dim=0)\n",
    "    \n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from transformers.modeling_flash_attention_utils import _flash_attention_forward # Flash attention forward\n",
    "from transformers.activations import silu\n",
    "\n",
    "class OlmoeRMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply RMS Norm\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L137-L154\n",
    "    - This is the only norm used in OlMoE!\n",
    "      - It's used 4 times per layer (attention key norm, attention query norm, layer residual pre-attention norm, post-attention norm)\n",
    "      - Also one additional time before the final LM head \n",
    "    \"\"\"\n",
    "    def __init__(self, D, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(D))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim = True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class OlmoeRotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Get sin/cos ROPE embeddings\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L161-L219\n",
    "    - Code has been simplified heavily since we're not using dynamic ROPE scaling\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf):\n",
    "        super().__init__()\n",
    "        dim = int(conf.D/conf.H)\n",
    "        inv_freq = 1.0 / (conf.rope_theta ** (torch.arange(0, dim, 2, dtype = torch.int64).float()/dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent = False)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type = device_type, enabled = False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim = -1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype = x.dtype), sin.to(dtype = x.dtype)\n",
    "\n",
    "class OlmoeAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention implementation\n",
    "    - Modified from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L288-L391\n",
    "    - Simplfied to handle base attention/sdpa/flash attention within this one class\n",
    "    - Also doesn't support GQA (OlMoE doesn't use anyways)\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf):\n",
    "        super().__init__()\n",
    "        self.attn_method = conf.attn_method\n",
    "        self.D = conf.D # Hidden state dim\n",
    "        self.H = conf.H # Num of attention heads\n",
    "        self.Dh = int(conf.D/conf.H) # Dimensions per head\n",
    "        \n",
    "        # Initialize attention layers - no biases following OlMoE architecture\n",
    "        self.q_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.k_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.v_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.o_proj = nn.Linear(self.D, self.D, bias = False)\n",
    "        self.q_norm = OlmoeRMSNorm(self.D, eps = conf.rms_norm_eps)\n",
    "        self.k_norm = OlmoeRMSNorm(self.D, eps = conf.rms_norm_eps)\n",
    "\n",
    "    # Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L223-L255\n",
    "    def apply_rotary_pos_emb(self, q, k, cos, sin, unsqueeze_dim = 1):\n",
    "        \n",
    "        def rotate_half(x):\n",
    "            \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "            x1 = x[..., : x.shape[-1] // 2]\n",
    "            x2 = x[..., x.shape[-1] // 2 :]\n",
    "            return torch.cat((-x2, x1), dim=-1)\n",
    "            \n",
    "        cos = cos.unsqueeze(unsqueeze_dim)\n",
    "        sin = sin.unsqueeze(unsqueeze_dim)\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings: tuple[torch.Tensor, torch.Tensor]):\n",
    "        \n",
    "        B, N , D = hidden_state.shape\n",
    "\n",
    "        query_state = self.q_norm(self.q_proj(hidden_state)).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "        key_state = self.k_norm(self.k_proj(hidden_state)).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "        value_state = self.v_proj(hidden_state).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_state, key_state = self.apply_rotary_pos_emb(query_state, key_state, cos, sin)\n",
    "        \n",
    "        if self.attn_method == 'normal':\n",
    "            attn_weights = torch.matmul(query_state, key_state.transpose(2, 3))/math.sqrt(self.Dh)  # Should be shape B x H x N x N\n",
    "            attn_weights = attn_weights + attention_mask # Attention mask is upper triangular of negative infinity\n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(query_state.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_state) # B x H x N x D/H\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "            attn_output = attn_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "            \n",
    "        elif self.attn_method == 'sdpa':\n",
    "            attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "                query_state, key_state, value_state,\n",
    "                attention_mask, dropout_p = 0.0, is_causal = True\n",
    "            )\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            attn_output = attn_output.view(B, N, D)\n",
    "            \n",
    "        elif self.attn_method == 'fa2':\n",
    "            query_state = query_state.transpose(1, 2)\n",
    "            key_state = key_state.transpose(1, 2)\n",
    "            value_state = value_state.transpose(1, 2)\n",
    "            attn_output = _flash_attention_forward(\n",
    "                query_state, key_state, value_state,\n",
    "                attention_mask, N, dropout = 0.0, use_top_left_mask = False, is_causal = True\n",
    "            )\n",
    "            attn_output = attn_output.reshape(B, N, D).contiguous()\n",
    "            \n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output\n",
    "    \n",
    "class OlmoeMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Individual expert MLP\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L258-L272\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf):\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "        self.D = conf.D\n",
    "        self.I = conf.I\n",
    "        self.gate_proj = nn.Linear(self.D, self.I, bias = False)\n",
    "        self.up_proj = nn.Linear(self.D, self.I, bias = False)\n",
    "        self.down_proj = nn.Linear(self.I, self.D, bias = False)\n",
    "        self.act_fn = silu\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "\n",
    "class OlmoeMoe(nn.Module):\n",
    "    \"\"\"\n",
    "    Entire MLP layer including router\n",
    "    - Modified from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L604-L649\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf):\n",
    "        super().__init__()\n",
    "        self.n_experts = conf.n_experts\n",
    "        self.top_k = conf.top_k\n",
    "        self.norm_topk_prob = conf.norm_topk_prob\n",
    "        self.gate = nn.Linear(conf.D, self.n_experts, bias = False) # Router\n",
    "        self.experts = nn.ModuleList([OlmoeMLP(conf) for _ in range(self.n_experts)]) # Create experts using OlmoeMLP\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, N, D = hidden_state.shape\n",
    "        \n",
    "        hidden_state = hidden_state.view(B * N, D) # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        router_logits = self.gate(hidden_state) # Output BN x N_EXPERTS (routing probability for each token)\n",
    "        routing_weights = F.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "\n",
    "        # Below both routing_weights and selected_experts are of size BN x TOP_K (for each token, the selected TOP_K experts and corresponding weights)\n",
    "        # Weights do NOT sum to 1 since we only top_k'd after the softmax\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim = -1) \n",
    "        if self.norm_topk_prob:\n",
    "            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        routing_weights = routing_weights.to(hidden_state.dtype)\n",
    "\n",
    "        # One hot encode - for each expert, which topk x token is active - e.g. expert_assignment_mask[0, :] will be 0s if the first expert is never chosen\n",
    "        expert_assignment_mask = F.one_hot(selected_experts, num_classes = self.n_experts).permute(2, 1, 0) # Creates (N_EXPERTS, TOP_K, BN)\n",
    "\n",
    "        mlp_output = torch.zeros((B * N, D), dtype = hidden_state.dtype, device = hidden_state.device) # Initialize MLP output - later iterate through experts and sum onto this object\n",
    "        \n",
    "        # Iterate through all the experts, apply each expert to the tokens where the expert are relevant, multiple output by the weights for the topk/token for that expert, then sum onto the mlp_output obj\n",
    "        for expert_ix, expert in enumerate(self.experts):\n",
    "            # For this expert, gives the (topk, token) coordinates which uses the expert\n",
    "            topk_slot, token_indices = torch.where(expert_assignment_mask[expert_ix, :])\n",
    "            \n",
    "            if token_indices.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # Get hidden states for tokens that use this expert - shape of num_assigned_tokens x D\n",
    "            tokens_for_expert = hidden_state[token_indices, :]\n",
    "\n",
    "            # Move input to expert device and get expert output\n",
    "            expert_output = expert(tokens_for_expert)\n",
    "            # For each num_assigned_tokens, multiples it by the corresponding weight in topk_slot fort that token_index\n",
    "            expert_output = expert_output * routing_weights[token_indices, topk_slot].unsqueeze(1) \n",
    "\n",
    "            mlp_output.index_add_(0, token_indices, expert_output.to(hidden_state.dtype))\n",
    "\n",
    "        mlp_output = mlp_output.reshape(B, N, D) # Convert back from BN x D -> B x N x D\n",
    "        \n",
    "        return mlp_output, router_logits\n",
    "\n",
    "class OlmoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer layer\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.D = conf.D\n",
    "        self.self_attn = OlmoeAttention(conf = conf)\n",
    "        self.moe = OlmoeMoe(conf)\n",
    "        self.input_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "        self.post_attention_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "            \n",
    "        ### Pre-SA Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.input_layernorm(hidden_state)\n",
    "        \n",
    "        ### SA + Sum to Residual Stream ###\n",
    "        attn_output = self.self_attn(\n",
    "            hidden_state,\n",
    "            attention_mask = attention_mask,\n",
    "            position_ids = position_ids,\n",
    "            position_embeddings = position_embeddings\n",
    "        )\n",
    "        hidden_state = residual + attn_output\n",
    "\n",
    "        ### Pre-MLP Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.post_attention_layernorm(hidden_state)\n",
    "        \n",
    "        ### MLP + Sum to Residual Stream###\n",
    "        mlp_output, router_logits = self.moe(hidden_state)\n",
    "        hidden_state = residual + mlp_output\n",
    "        \n",
    "        return hidden_state, router_logits\n",
    "\n",
    "class OlmoeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The top level model object. Also handles weight initialization and loss calculations.\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            @conf: A configuration object of class MoeConf\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "        \n",
    "        ### Layers ###\n",
    "        self.embed_tokens = nn.Embedding(self.conf.vocab_size, self.conf.D, self.conf.padding_idx)\n",
    "        self.rotary_emb = OlmoeRotaryEmbedding(conf = self.conf)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [OlmoeBlock(self.conf, layer_idx) for layer_idx in range(self.conf.n_layers)]\n",
    "        )\n",
    "        self.norm = OlmoeRMSNorm(self.conf.D, eps = self.conf.rms_norm_eps)\n",
    "        self.lm_head = nn.Linear(self.conf.D, self.conf.vocab_size, bias = False)\n",
    "        \n",
    "        ### Initialize weights ###\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # OlMoE weight initiation - see https://github.com/huggingface/transformers/blob/8f1509a96c96747c893051ac947795cfb0750357/src/transformers/modeling_utils.py#L2500-L2515\n",
    "    # Normal distribution for linear layers + embeddings\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "            # In the vocab -> embedding layer, set all embeddings to 0 for the padding token (tokenizer.pad_token_id)\n",
    "            if module is self.embed_tokens:\n",
    "                self.embed_tokens.weight.data[self.conf.padding_idx].zero_() \n",
    "        # Seems to use default weight initialization for other layers\n",
    "            # Move all parameters and buffers to the specified dtype\n",
    "    \n",
    "    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.Tensor):\n",
    "\n",
    "        hidden_state = self.embed_tokens(input_ids)\n",
    "        B, N, D = hidden_state.shape\n",
    "\n",
    "        ### Prep rotary embeddings + attention masks  ###\n",
    "        cache_position = torch.arange(0, N, device = hidden_state.device)\n",
    "        position_ids = cache_position.unsqueeze(0)\n",
    "        position_embeddings = self.rotary_emb(hidden_state, position_ids) # Position embeddings to be shared across the decoder layers\n",
    "\n",
    "        # This is the upper-trangular matrix of infinities to mask future tokens in the attention softmax;\n",
    "        if self.conf.attn_method in ['normal', 'sdpa']:\n",
    "            causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(attention_mask, N, N, hidden_state.dtype, hidden_state.device, cache_position, B)\n",
    "        # The flash attention mask is simpler - takes only the original attention mask or None\n",
    "        elif self.conf.attn_method == 'fa2':\n",
    "            causal_mask  = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "        \n",
    "        ### Transformer layers ###\n",
    "        all_router_logits = () # Save router logits from each layer into this; will be needed for balancing loss\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            hidden_state, router_logits = layer(\n",
    "                hidden_state,\n",
    "                causal_mask,\n",
    "                position_ids,\n",
    "                position_embeddings\n",
    "            )\n",
    "            all_router_logits += (router_logits, )\n",
    "\n",
    "        hidden_state = self.norm(hidden_state)\n",
    "        output_logits = self.lm_head(hidden_state)\n",
    "\n",
    "        ##### Calculate Loss #####\n",
    "        # The labels object should be a tensor of token IDs or -100 (for attention mask, since don't want to calculate loss for those)\n",
    "        label_ids = torch.where(input_ids == self.conf.padding_idx, torch.tensor(-100), input_ids)\n",
    "        # Get regular loss\n",
    "        base_loss = ForCausalLMLoss(output_logits, label_ids, self.conf.vocab_size)\n",
    "        # Get load balancing loss\n",
    "        aux_loss = load_balancing_loss_func(gate_logits = all_router_logits, num_experts = self.conf.n_experts, top_k = self.conf.top_k, attention_mask = attention_mask)\n",
    "        # Get total loss = regular loss + .01 * load bal loss\n",
    "        loss = base_loss + self.conf.router_aux_loss_coef * aux_loss \n",
    "\n",
    "        return output_logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test a forward pass\n",
    "\n",
    "# Load the model - everything on the main device with bf16\n",
    "torch.set_default_device(main_device)\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "model = OlmoeModel(conf)\n",
    "check_memory()\n",
    "\n",
    "# Test a forward pass\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "\n",
    "prompt = 'I am a dog and I like to eat. My favorite food is'\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to(main_device)\n",
    "with torch.no_grad():\n",
    "    output = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    \n",
    "output_ids = torch.argmax(output[0][0, :, :], dim = 1)\n",
    "print(tokenizer.decode(output_ids[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now wrap it in a parent class to handle device movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceWrappedExpert(nn.Module):\n",
    "    def __init__(self, expert: OlmoeMLP, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.expert = expert.to(device)\n",
    "        self.stream = torch.cuda.Stream(device = device)  # Dedicated stream per expert\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        orig_device = x.device\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            x = x.to(self.device, non_blocking = True)\n",
    "            out = self.expert(x)\n",
    "            out = out.to(orig_device, non_blocking = True)\n",
    "        return out\n",
    "\n",
    "class DistributedOlmoeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper class for OlmoeModel that handles multi-device expert distribution.\n",
    "    Keeps the original implementation intact while managing device placement.\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: MoeConf, main_device: str | torch.device, expert_device_map: dict[int, str | torch.device] | None = None, expert_device_list: list[str | torch.device] | None = None):\n",
    "        \"\"\"\n",
    "        This is a wrapper around the main model class that just handles device movement. Either set `expert_device_map` to explicit map experts to devices, or `expert_device_list` to equally partition experts among devices. If both are None, all experts are placed on the `main_device`.\n",
    "\n",
    "        Params:\n",
    "            @conf: A configuration object of class MoeConf\n",
    "            @main_device: A torch.device object where all dense layers will be stored, such as \"cuda:0\"\n",
    "            @expert_device_map: A mapping of experts to devices, e.g. `{0: \"cuda:1\", 1: \"cuda:2\", 2: \"cuda:1\", ...}`.\n",
    "            @expert_device_list: A list of devices to equally allocate the experts, e.g. `[\"cuda:1\", \"cuda:2\"]`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Validate device configurations\n",
    "        if expert_device_map and expert_device_list:\n",
    "            raise ValueError(\"Cannot specify both expert_device_map and expert_device_list\")\n",
    "        \n",
    "        # Convert main_device to torch.device if it's a string\n",
    "        self.main_device = torch.device(main_device)\n",
    "\n",
    "        # Auto-generate expert devices if list provided\n",
    "        if expert_device_map is None and expert_device_list is not None:\n",
    "            expert_device_map = self._create_block_expert_devices(conf.n_experts, [torch.device(d) for d in expert_device_list])\n",
    "        \n",
    "        # Default expert devices if not provided\n",
    "        self.expert_device_map = expert_device_map or {}\n",
    "        \n",
    "        # Initialize base model on main device\n",
    "        self.model = OlmoeModel(conf).to(self.main_device)\n",
    "\n",
    "        # Distribute experts\n",
    "        self._distribute_experts()\n",
    "\n",
    "    def _create_block_expert_devices(self, n_experts: int, device_list: list[str | torch.device]) -> dict[int, str | torch.device]:\n",
    "        \"\"\"\n",
    "        Distribute experts in contiguous blocks across devices.\n",
    "        \n",
    "        Example: 8 experts, 2 devices → [0-3] on device 0, [4-7] on device 1\n",
    "        \"\"\"\n",
    "        num_devices = len(device_list)\n",
    "        experts_per_device = n_experts // num_devices\n",
    "        remainder = n_experts % num_devices\n",
    "        \n",
    "        expert_map = {}\n",
    "        expert_idx = 0\n",
    "        \n",
    "        for dev_idx, device in enumerate(device_list):\n",
    "            # Calculate how many experts this device gets\n",
    "            count = experts_per_device + (1 if dev_idx < remainder else 0)\n",
    "            \n",
    "            # Assign contiguous block\n",
    "            for _ in range(count):\n",
    "                expert_map[expert_idx] = device\n",
    "                expert_idx += 1\n",
    "                \n",
    "        return expert_map\n",
    "    \n",
    "    def _distribute_experts(self):\n",
    "        \"\"\"\n",
    "        Wrap experts in device-aware modules\n",
    "        \"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if not hasattr(layer, 'moe'):\n",
    "                continue  # Skip non-MoE layers\n",
    "\n",
    "            moe_layer = layer.moe\n",
    "            new_experts = nn.ModuleList()\n",
    "            \n",
    "            for expert_idx, expert in enumerate(moe_layer.experts):\n",
    "                # Get device for this expert index\n",
    "                device = self.expert_device_map.get(expert_idx, self.main_device)\n",
    "                \n",
    "                # Wrap expert with device handler\n",
    "                wrapped_expert = DeviceWrappedExpert(expert, torch.device(device))\n",
    "                new_experts.append(wrapped_expert)\n",
    "                \n",
    "            # Replace original experts with wrapped versions\n",
    "            moe_layer.experts = new_experts\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.Tensor):\n",
    "        # Ensure base inputs stay on main device\n",
    "        return self.model(\n",
    "            input_ids.to(self.main_device), \n",
    "            attention_mask.to(self.main_device)\n",
    "        )\n",
    "        \n",
    "    def state_dict(self, *args, **kwargs):\n",
    "        \"\"\"Return the state dict of the underlying model\"\"\"\n",
    "        return self.model.state_dict(*args, **kwargs)\n",
    "    \n",
    "    def load_state_dict(self, state_dict, *args, **kwargs):\n",
    "        \"\"\"Load state dict into the underlying model\"\"\"\n",
    "        return self.model.load_state_dict(state_dict, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test this\n",
    " # Make sure everything is empty\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "check_memory()\n",
    "\n",
    "# Load the model - everything on the main device with bf16\n",
    "torch.set_default_device(main_device)\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "model = DistributedOlmoeModel(conf, main_device = main_device, expert_device_list = ['cuda:1', 'cuda:2'])\n",
    "print('Model loaded')\n",
    "check_memory()\n",
    "\n",
    "# Test a forward pass\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "\n",
    "prompt = 'I am a dog and I like to eat. My favorite food is'\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to(main_device)\n",
    "with torch.no_grad():\n",
    "    output = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    \n",
    "output_ids = torch.argmax(output[0][0, :, :], dim = 1)\n",
    "print(tokenizer.decode(output_ids[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Let's use some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from itertools import islice\n",
    "\n",
    "def get_fineweb_edu_ds(n_samples: int = 1000):\n",
    "    dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", name = \"default\", split = 'train', streaming = True)\n",
    "    dataset = dataset.filter(lambda x: x.get('language') == 'en' and x.get('score') >= 4)\n",
    "    dataset_pulled = list(islice(dataset, n_samples))  # Convert to a list of the first 1,000 samples\n",
    "    dataset_pulled = [x['text'] for x in dataset_pulled]    \n",
    "    return dataset_pulled\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, tokenizer_output):\n",
    "        self.input_ids = tokenizer_output['input_ids']\n",
    "        self.attention_mask = tokenizer_output['attention_mask']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "    \n",
    "fw_data = get_fineweb_edu_ds(100)\n",
    "fw_tokenized = tokenizer(fw_data, truncation = True, max_length = 4096, padding = 'max_length', return_tensors = 'pt')\n",
    "fw_ds = TestDataset(fw_tokenized)\n",
    "fw_dl = DataLoader(fw_ds, batch_size = 2, shuffle = True, generator = torch.Generator(device = main_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training code\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set different LRs for experts/non-experts\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': [p for n,p in model.named_parameters() if 'expert' in n], 'lr': 1e-4},\n",
    "    {'params': [p for n,p in model.named_parameters() if 'expert' not in n], 'lr': 3e-5}\n",
    "], weight_decay = 0.01)\n",
    "\n",
    "max_grad_norm = 1.0  # Set the value for gradient clipping\n",
    "step = 0\n",
    "\n",
    "\n",
    "for batch in fw_dl:\n",
    "    model.train()\n",
    "\n",
    "    # Device-aware batch preparation\n",
    "    input_ids = batch['input_ids'].to(model.main_device)\n",
    "    attention_mask = batch['attention_mask'].to(model.main_device)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Forward pass with timing\n",
    "    optimizer.zero_grad()\n",
    "    start_time = time.time()\n",
    "    logits, loss = model(input_ids, attention_mask)\n",
    "    fwd_time = time.time() - start_time\n",
    "\n",
    "    # Backward pass\n",
    "    start_bwd = time.time()\n",
    "    loss.backward()\n",
    "    bwd_time = time.time() - start_bwd\n",
    "\n",
    "    # MoE-specific gradient clipping\n",
    "    expert_params = [p for n,p in model.named_parameters() if 'expert' in n]\n",
    "    shared_params = [p for n,p in model.named_parameters() if 'expert' not in n]\n",
    "\n",
    "    # Different clipping for experts vs shared params\n",
    "    torch.nn.utils.clip_grad_norm_(expert_params, max_grad_norm * 2)  # Looser for experts\n",
    "    torch.nn.utils.clip_grad_norm_(shared_params, max_grad_norm)\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    # MoE metrics collection\n",
    "    metrics = {\n",
    "        'loss': loss.item(),\n",
    "        'fwd_time': fwd_time,\n",
    "        'bwd_time': bwd_time,\n",
    "        'expert_usage': defaultdict(int)\n",
    "    }\n",
    "\n",
    "    # Calculate expert utilization\n",
    "    with torch.no_grad():\n",
    "        for layer_idx, layer in enumerate(model.model.layers):\n",
    "            if hasattr(layer.moe, 'last_router_logits'):\n",
    "                router_logits = layer.moe.last_router_logits\n",
    "                _, selected_experts = torch.topk(router_logits, model.conf.top_k, dim=-1)\n",
    "                for expert_idx in selected_experts.flatten().unique():\n",
    "                    metrics['expert_usage'][f'layer_{layer_idx}/expert_{expert_idx.item()}'] += 1\n",
    "\n",
    "    print(f\"Step {step}:\")\n",
    "    print(f\"  Loss: {metrics['loss']:.3f}\")\n",
    "    print(f\"  Fwd/Bwd Time: {metrics['fwd_time']:.2f}s/{metrics['bwd_time']:.2f}s\")\n",
    "    print(\"  Expert Usage:\")\n",
    "    for expert, count in metrics['expert_usage'].items():\n",
    "        print(f\"    {expert}: {count}\")\n",
    "\n",
    "    step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: Implement streaming to avoid sequential looping over experts, concurrent launch should get ~3-5x speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# from transformers.modeling_rope_utils import _compute_default_rope_parameters\n",
    "# print(inspect.getsource(_compute_default_rope_parameters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
