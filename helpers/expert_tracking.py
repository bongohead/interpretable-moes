import os
import torch
import pandas as pd


@torch.no_grad()
def save_expert_usage(model, dl, out_pt_path: str, main_device: str, max_batches: int = 2):
    """
    Use this to save expert selections. This runs through the validation set, retrieves model outputs (including top-k experts), and saves them to a pt file for later analysis.
     Note that the dataloader should be unshuffled if you want to identify/compare expert selections of the same tokens across runs.

    Params:
        @model (nn.Module): The model object. The forward pass must return a dict with key `all_topk_experts`, a tuple of length n_layers each entry is shape [B*N, top_k] containing expert indices.
        @dl (DataLoader): A dataloader, yielding dicts with "input_ids", "attention_mask", etc.
        @out_pt_path (str): Where to write the .pt file of usage data.
        @main_device (str): The device the model lives on.

    Returns:
        The saved format is a Python dict with lists of equal length, containing batch_ix, seq_ix, token_ix, token_id, layer_ix, topk_slot, and expert_id. This 
         identifies, for a unique batch x minibatch x token position, at each layer ix, the expert_id used for each topk slot.
    """

    # Store usage info in lists into saving as a pt file later
    all_records = {"batch_ix": [], "seq_ix": [], "token_ix": [], "token_id": [], "layer_ix": [], "topk_slot": [], "expert_id": []}

    for batch_ix, val_batch in enumerate(dl):
        val_input_ids = val_batch["input_ids"].to(main_device)
        val_attn_mask = val_batch["attention_mask"].to(main_device)
        outputs = model(input_ids = val_input_ids, attention_mask = val_attn_mask, moe_method = 'forward_slow', use_checkpointing = False)

        all_topk_experts = outputs["all_topk_experts"]
        n_layers = len(all_topk_experts)

        B, N = val_input_ids.shape

        for b in range(B):
            for n_idx in range(N):
                # Skip attention mask
                if val_attn_mask[b, n_idx].item() == 0:
                    continue

                token_id = val_input_ids[b, n_idx].item()
                offset = b * N + n_idx

                # For each layer, retrieve the top-k experts
                for layer_idx in range(n_layers):
                    topk_exps_tensor = all_topk_experts[layer_idx][offset]  # shape [top_k]
                    
                    # For each top-k slot, record a row in "long" format
                    for slot_idx, ex_id in enumerate(topk_exps_tensor):
                        all_records["batch_ix"].append(batch_ix)
                        all_records['seq_ix'].append(b)
                        all_records["token_ix"].append(n_idx)
                        all_records["token_id"].append(token_id)
                        all_records["layer_ix"].append(layer_idx)
                        all_records["topk_slot"].append(slot_idx)
                        all_records["expert_id"].append(ex_id.item())

        if batch_ix >= max_batches - 1:
            break

    torch.save(all_records, out_pt_path)
    
    print(f"Saved records: {len(all_records['batch_ix'])}")


def convert_folder_of_pts(pt_dir: str):
    """
    Converts all pt files in a directory saved from `save_expert_usage_stats` into CSVs for easier processing.

    Params:
        @pt_dir: The directory containing pt files generated by `save_expert_usage_stats`

    Example:
        convert_folder_of_pts('logs')
    """
    def convert_pt_to_csv(in_pt_path: str, out_csv_path: str):
        data = torch.load(in_pt_path)

        for key, val in data.items():
            if isinstance(val, torch.Tensor):
                data[key] = val.tolist()
        
        df = pd.DataFrame(data)
        
        df.to_csv(out_csv_path, index=False)
        print(f"Converted {in_pt_path} to {out_csv_path} (rows={len(df)})")

    for fname in os.listdir(pt_dir):
        if fname.endswith(".pt"):
            pt_path = os.path.join(pt_dir, fname)
            base_name = os.path.splitext(fname)[0]  # e.g. "example"
            csv_path = os.path.join(pt_dir, base_name + ".csv")
            if os.path.isfile(csv_path):
                print(f"Skipping {pt_path} because CSV already exists at {csv_path}.")
                continue

            convert_pt_to_csv(pt_path, csv_path)